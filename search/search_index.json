{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Numerical Linear Algebra Notes","text":"<p>Author: Shuojiang Liu</p> <p>This site documents the detailed notes on my study of Numerical Linear Algebra, the content of which is derived from synthesizing and contemplating the content of the Georgia Tech MATH 6643 course I have taken.</p> <p>Contents:</p> <ol> <li>Introduction to Numerical Linear Algebra</li> <li>Direct Methods</li> <li>Condition and Stability</li> <li>Eigenvalues Solving</li> <li>Iterative Methods</li> <li>Miscellaneous Topics</li> </ol>"},{"location":"Basics/","title":"Introduction to Numerical Linear Algebra","text":"<p>Basic Introduction</p> <p>Review of Linear Algrbra</p>"},{"location":"Basics/Introduction/","title":"Getting Started","text":""},{"location":"Basics/Introduction/#general-viewpoint","title":"General Viewpoint","text":"<p><code>Overall Task</code>: Solve linear system of equations</p> \\[ \\boldsymbol{Ax}=\\boldsymbol{b} \\] <p>In the discussion afterwards, we assume that \\(\\boldsymbol{A}\\) is invertible, \\(\\boldsymbol{A}\\in \\mathbb{R} ^{n\\times n}\\) but \\(n\\) is very large.</p> <p>In scientific computing, there are three important issues:</p> <ul> <li>Efficiency</li> <li>Accuracy</li> <li>Stability: a concept measuring how sensitive the algorithm to perturbation</li> </ul>"},{"location":"Basics/Introduction/#introductory-examples","title":"Introductory Examples","text":"<p>Here are two examples for linear systems in scientific computing.</p>"},{"location":"Basics/Introduction/#ode","title":"ODE","text":"<p>Consider:</p> \\[ \\begin{cases}     -u''\\left( x \\right) =f\\left( x \\right)\\\\     u\\left( 0 \\right) =1, u\\left( 1 \\right) =2\\\\ \\end{cases}, x\\in \\left( 0,1 \\right)  \\] <p>where \\(f(x)\\) is given. We want to know \\(u\\) using the computer.</p> <p>The important step is to do the discretization.</p> <p>We split the interval \\([0,1]\\) into: \\(0=x_0&lt;x_1&lt;\\cdots &lt;x_{i-1}&lt;x_i&lt;x_{i+1}&lt;\\cdots &lt;x_N=1\\), where \\(x_{i+1}-x_i\\triangleq h=\\frac{1}{N}, \\left( i=0,1,\\cdots ,N-1 \\right)\\). We just need to know the solutions on these points for \\(u(x_i)\\).</p> <p>Let \\(u\\left( x_i \\right) \\triangleq u_i\\).</p> <p>Using the finite difference method, we know that:</p> \\[ u\\prime\\left( x \\right) =\\lim_{\\varepsilon \\rightarrow 0} \\frac{u\\left( x+\\varepsilon \\right) -u\\left( x \\right)}{\\varepsilon}\\approx \\frac{u\\left( x+\\varepsilon \\right) -u\\left( x \\right)}{\\varepsilon} \\] <p>if \\(\\varepsilon\\) is small. We take \\(\\varepsilon=h\\), then \\(u\\prime\\left( x \\right) \\approx \\frac{u\\left( x+h \\right) -u\\left( x \\right)}{h}\\). We can get \\(\\frac{u\\left( x_i+h \\right) -u\\left( x_i \\right)}{h}=\\frac{u_{i+1}-u_i}{h}\\).</p> <p>Similarly, we can get \\(u''\\left( x \\right) \\approx \\frac{u_{i+1}-2u_i+u_{i-1}}{h^2}\\). Also \\(u''\\left( x \\right) \\propto O\\left( h^2 \\right)\\).</p> <p>Therefore, \\(-\\frac{u_{i+1}-2u_i+u_{i-1}}{h^2}=f\\left( x_i \\right)\\)</p> <p>To sum up, \\(\\boldsymbol{u}=\\left[ u_1,u_2,\\cdots ,u_{N-1} \\right]^T\\) is a linear system that satisfies:</p> \\[ \\boldsymbol{A\\cdot u}=\\boldsymbol{f} \\] <p>We can get:</p> \\[ \\boldsymbol{u}=\\left[ \\begin{array}{c}     u_1\\\\     \\vdots\\\\     u_{N-1}\\\\ \\end{array} \\right] , \\boldsymbol{A}=\\left[ \\begin{matrix}     2&amp;      -1&amp;     0&amp;      \\cdots&amp;     0\\\\     -1&amp;     2&amp;      -1&amp;     \\cdots&amp;     0\\\\     0&amp;      -1&amp;     \\ddots&amp;     \\ddots&amp;     \\vdots\\\\     \\vdots&amp;     \\vdots&amp;     \\ddots&amp;     2&amp;      -1\\\\     0&amp;      0&amp;      \\cdots&amp;     -1&amp;     2\\\\ \\end{matrix} \\right] , \\boldsymbol{f}=\\left[ \\begin{array}{c}     h^2f\\left( x_1 \\right) +?\\\\     h^2f\\left( x_2 \\right)\\\\     \\vdots\\\\     h^2f\\left( x_N \\right) +?\\\\ \\end{array} \\right]  \\]"},{"location":"Basics/Introduction/#de-blurring-problem-in-image-processing","title":"De-blurring Problem in Image Processing","text":"<p>The degrading image model is:</p> \\[ \\underset{\\mathrm{observed} \\ \\mathrm{image}}{\\underbrace{g\\left( x \\right) }}=\\underset{\\mathrm{true} \\ \\mathrm{image}}{\\underbrace{u\\left( x \\right) }}+\\int_{\\varOmega}{\\underset{\\mathrm{blurring} \\ \\mathrm{kernel}}{\\underbrace{k\\left( x-y \\right) }}u\\left( y \\right) \\mathrm{d}y}+\\underset{\\mathrm{noise}}{\\underbrace{\\eta \\left( x \\right) }} \\] <p>Sometimes \\(k\\left( x \\right) =\\frac{1}{c}e^{-\\frac{x^2}{2}}\\) which is Gaussian blur. Usually:</p> \\[ k\\left( x-y \\right) =\\begin{cases}     c, if\\,\\,\\left| x-y \\right|&lt;\\varepsilon\\\\     0, \\mathrm{otherwise}\\\\ \\end{cases} \\] <p>To simplify, we ignore the noise: let \\(\\eta \\left( x \\right) =0\\).</p> <p>Given \\(g(x)\\), we want to find \\(u(x)\\). Assume that \\(k(x)\\) is given and the image/signal is 1D. We can model the linear system as:</p> \\[ \\boldsymbol{u}=\\left[ \\begin{array}{c}     u_1\\\\     u_2\\\\     \\vdots\\\\     u_n\\\\ \\end{array} \\right] , \\boldsymbol{g}=\\left[ \\begin{array}{c}     g_1\\\\     g_2\\\\     \\vdots\\\\     g_n\\\\ \\end{array} \\right] , \\boldsymbol{A}=\\left[ \\begin{matrix}     1&amp;      0&amp;      \\cdots&amp;     0\\\\     0&amp;      1&amp;      \\cdots&amp;     0\\\\     \\vdots&amp;     \\vdots&amp;     \\ddots&amp;     \\vdots\\\\     0&amp;      0&amp;      \\cdots&amp;     1\\\\ \\end{matrix} \\right] +\\left[ \\begin{matrix}     k_0&amp;        k_1&amp;        k_2&amp;        \\cdots&amp;     k_{n-1}\\\\     k_1&amp;        k_0&amp;        k_1&amp;        \\cdots&amp;     k_{n-2}\\\\     k_2&amp;        k_1&amp;        k_0&amp;        \\cdots&amp;     k_{n-3}\\\\     \\vdots&amp;     \\ddots&amp;     \\ddots&amp;     \\ddots&amp;     \\vdots\\\\     k_{n-1}&amp;        k_{n-2}&amp;        k_{n-3}&amp;        \\cdots&amp;     k_0\\\\ \\end{matrix} \\right] , \\\\ \\boldsymbol{Au}=\\boldsymbol{g} \\] <p>\\(\\boldsymbol{A}\\) is called Toeplitz matrix.</p>"},{"location":"Basics/Linear_Algebra/","title":"Linear Algebra Review","text":""},{"location":"Basics/Linear_Algebra/#vector-norm","title":"Vector Norm","text":"<p>The range of vector norm is \\(\\boldsymbol{x}\\in \\mathbb{R} ^n, \\left\\| \\boldsymbol{x} \\right\\| \\in \\mathbb{R} ^+\\cup \\left\\{ \\mathbf{0} \\right\\}\\).</p> <p>Some examples:</p> <p>\\(L^2\\)-Norm:</p> \\[ \\left\\| \\boldsymbol{x} \\right\\| _2=\\sqrt{\\boldsymbol{x}^T\\boldsymbol{x}}=\\left&lt; \\boldsymbol{x},\\boldsymbol{x} \\right&gt; ^{\\frac{1}{2}} \\] <p>\\(L^1\\)-Norm:</p> \\[ \\left\\| \\boldsymbol{x} \\right\\| _1=\\sum_{i=1}^n{\\left| x_i \\right|} \\] <p>\\(L^\\infty\\)-Norm:</p> \\[ \\left\\| \\boldsymbol{x} \\right\\| _{\\infty}=\\max_{1\\le i\\le n} \\left\\| x_i \\right\\|  \\] <p>\\(L^p\\)-Norm:</p> \\[ \\left\\| \\boldsymbol{x} \\right\\| _p=\\left( \\sum_{i=1}^n{\\left| x_i \\right|^p} \\right) ^{\\frac{1}{p}}, p\\in \\left[ 1,\\infty \\right)  \\] <p>Weighted Norm: ( \\(\\boldsymbol{W}\\) is a given matrix)</p> \\[ \\boldsymbol{W}=\\left[ \\begin{matrix}     w_1&amp;        \\cdots&amp;     O\\\\     \\vdots&amp;     \\ddots&amp;     \\vdots\\\\     O&amp;      \\cdots&amp;     w_n\\\\ \\end{matrix} \\right] , \\\\ \\left\\| \\boldsymbol{x} \\right\\| _{\\boldsymbol{w}}=\\left\\| \\boldsymbol{Wx} \\right\\| _2=\\left( \\sum_{i=1}^n{\\left| w_ix_i \\right|^2} \\right) ^{\\frac{1}{2}} \\] <p>Geometric Intuition:</p> <p>Unit disk with different norms: \\(\\left\\{ \\boldsymbol{x}\\in \\mathbb{R} ^n|\\left\\| \\boldsymbol{x} \\right\\| \\le 1 \\right\\}\\)</p> <p>\\(L^2\\) is like a circle. \\(L^1\\) is like a diamond. \\(L^\\infty\\) is like a square. \\(L^p (1&lt;p&lt;2)\\) is like the shape between circle and diamond. \\(L^p (p&gt;2)\\) is like the shape between circle and square.</p>"},{"location":"Basics/Linear_Algebra/#matrix-norm","title":"Matrix Norm","text":"<p>Two different ways to define the matrix norm for \\(\\boldsymbol{A}\\in \\mathbb{R} ^{m\\times n}\\):</p> <p>Way 1: View matrix as a vector (reshape):</p> \\[ \\left\\| \\boldsymbol{A} \\right\\| _F=\\left( \\sum_{i=1}^m{\\sum_{j=1}^n{\\left| a_{ij} \\right|^2}} \\right) ^{\\frac{1}{2}} \\] <p>Way 2: Induced Matrix Norm (preferred way to define the matrix norm)</p> \\[ \\boldsymbol{A}\\in \\mathbb{R} ^{m\\times n}, \\boldsymbol{A}:\\mathbb{R} ^n\\longmapsto \\mathbb{R} ^m\\,\\,\\left( \\forall \\boldsymbol{x}\\in \\mathbb{R} ^n, \\boldsymbol{Ax}\\in \\mathbb{R} ^m \\right)  \\] \\[ \\left\\| \\boldsymbol{A} \\right\\| _{\\left( m,n \\right)}=\\mathop {\\mathrm{sup}} \\limits_{\\left\\| \\boldsymbol{x} \\right\\| \\ne 0}\\frac{\\left\\| \\boldsymbol{Ax} \\right\\| _v}{\\left\\| \\boldsymbol{x} \\right\\| _v} \\] <p>Also:</p> \\[ \\left\\| \\boldsymbol{A} \\right\\| _{\\left( m,n \\right)}=\\mathop {\\mathrm{sup}} \\limits_{\\begin{array}{c}     \\boldsymbol{x}\\in \\mathbb{R} ^n,\\\\     \\left\\| \\boldsymbol{x} \\right\\| =1\\\\ \\end{array}}\\frac{\\left\\| \\boldsymbol{Ax} \\right\\|}{\\left\\| \\boldsymbol{x} \\right\\|}=\\mathop {\\mathrm{sup}} \\limits_{\\begin{array}{c}     \\boldsymbol{x}\\in \\mathbb{R} ^n,\\\\     \\left\\| \\boldsymbol{x} \\right\\| =1\\\\ \\end{array}}\\left\\| \\boldsymbol{Ax} \\right\\|  \\] <p><code>Conclusion</code>:</p> <p>If \\(\\left\\| \\boldsymbol{x} \\right\\| _1\\) is taken:</p> \\[ \\boldsymbol{A}=\\left[ \\begin{matrix}     \\boldsymbol{a}_1&amp;       \\boldsymbol{a}_2&amp;       \\cdots&amp;     \\boldsymbol{a}_n\\\\ \\end{matrix} \\right] , \\left\\| \\boldsymbol{A} \\right\\| _1=\\max_{1\\le j\\le n} \\left\\| \\boldsymbol{a}_j \\right\\| _1 \\] <p>If \\(\\left\\| \\boldsymbol{x} \\right\\| _{\\infty}\\) is taken:</p> \\[ \\boldsymbol{A}=\\left[ \\begin{array}{c}     {\\boldsymbol{a}_1}^*\\\\     {\\boldsymbol{a}_2}^*\\\\     \\vdots\\\\     {\\boldsymbol{a}_m}^*\\\\ \\end{array} \\right] , \\left\\| \\boldsymbol{A} \\right\\| _{\\infty}=\\max_{1\\le i\\le m} \\left\\| {\\boldsymbol{a}_i}^* \\right\\| _1 \\] <p>What about induced 2-norms? It turns out that we cannot find a formula for it!</p> <p>The 2-norm(spectral norm) of a matrix \\(\\boldsymbol{A}\\) is the largest singular value of \\(\\boldsymbol{A}\\) (i.e., the square root of the largest eigenvalue of the matrix \\(\\boldsymbol{A}^{*}\\boldsymbol{A}\\), where \\(\\boldsymbol{A}^{*}\\) denotes the conjugate transpose of \\(\\boldsymbol{A}\\) ):</p> \\[ \\left\\| \\boldsymbol{A} \\right\\| _2=\\sqrt{\\lambda _{\\max}\\left( \\boldsymbol{A}^*\\boldsymbol{A} \\right)}=\\sigma _{\\max}\\left( \\boldsymbol{A} \\right)  \\]"},{"location":"Basics/Linear_Algebra/#spectral-radius","title":"Spectral Radius","text":"<p>The Spectral Radius of a square matrix is the maximum of the absolute values of its eigenvalues.</p> <p>Let \\(\\lambda _1,\\lambda _2,\\cdots ,\\lambda _n\\) be the eigenvalues of a matrix \\(\\boldsymbol{A}\\in \\mathbb{C} ^{n\\times n}\\). Then the spectral radius of \\(\\boldsymbol{A}\\) is defined as:</p> \\[ \\rho \\left( \\boldsymbol{A} \\right) =\\max \\left( \\left| \\lambda _1 \\right|,\\left| \\lambda _2 \\right|,\\cdots ,\\left| \\lambda _n \\right| \\right)  \\] <p>If \\(\\boldsymbol{A}\\) satisfies \\(\\boldsymbol{AA}^*=\\boldsymbol{A}^*\\boldsymbol{A}\\) (Normal Matrix), then:</p> \\[ \\rho \\left( \\boldsymbol{A} \\right) =\\left\\| \\boldsymbol{A} \\right\\| _2 \\]"},{"location":"Condition%20and%20Stability/","title":"Conditioning and Stability","text":"<p>Describing the small perturbation properties</p> <p>Conditioning: for the problem</p> <p>Stability: for the algorithm</p>"},{"location":"Condition%20and%20Stability/Conditioning/","title":"Conditioning","text":""},{"location":"Condition%20and%20Stability/Conditioning/#introduction","title":"Introduction","text":"<p>Let's start with an example:</p> <p>Given \\(x\\in \\mathbb{R} ^+\\) , compute \\(\\sqrt{x}\\) . We are considering the problem not the algorithm here.</p> <p>We denote the problem as:</p> \\[ \\delta f=f\\left( x+\\delta x \\right) -f\\left( x \\right) :\\begin{cases}     x\\longmapsto \\sqrt{x}=f\\left( x \\right)\\\\     \\Downarrow\\\\     x+\\delta x\\longmapsto \\sqrt{x+\\delta x}=f\\left( x+\\delta x \\right)\\\\ \\end{cases} \\] <p>Rather than considering \\(\\frac{\\left\\| \\delta f \\right\\|}{\\left\\| fx \\right\\|}\\) , we can consider the relative ratio:</p> \\[ \\frac{\\frac{\\left\\| \\delta f \\right\\|}{\\left\\| f \\right\\|}}{\\frac{\\left\\| \\delta x \\right\\|}{\\left\\| x \\right\\|}} \\] <p>For \\(f\\left( x \\right) =\\sqrt{x}\\) :</p> \\[ \\frac{\\frac{\\left\\| \\delta f \\right\\|}{\\left\\| f \\right\\|}}{\\frac{\\left\\| \\delta x \\right\\|}{\\left\\| x \\right\\|}}=\\frac{\\frac{\\left| \\sqrt{x+\\delta x}-\\sqrt{x} \\right|}{\\sqrt{x}}}{\\frac{\\left| \\delta x \\right|}{\\left| x \\right|}}=\\frac{\\frac{\\left| x+\\delta x-x \\right|}{\\sqrt{x}\\left( \\sqrt{x+\\delta x}+\\sqrt{x} \\right)}}{\\frac{\\left| \\delta x \\right|}{\\left| x \\right|}} \\\\ =\\frac{\\left| x \\right|}{\\sqrt{x}\\left( \\sqrt{x+\\delta x}+\\sqrt{x} \\right)}\\approx \\frac{\\left| x \\right|}{\\sqrt{x}\\left( 2\\sqrt{x} \\right)}=\\frac{1}{2} \\] <p>It means that the output does not amplify the permutation/mistake of the input.</p>"},{"location":"Condition%20and%20Stability/Conditioning/#definition","title":"Definition","text":"<p>Assume \\(\\delta x\\) is small, we define the condition number as:</p> \\[ \\lim_{\\alpha \\rightarrow 0} \\underset{\\left\\| \\delta x \\right\\| \\leqslant \\alpha}{\\mathrm{sup}}\\frac{\\frac{\\left\\| \\delta f \\right\\|}{\\left\\| f \\right\\|}}{\\frac{\\left\\| \\delta x \\right\\|}{\\left\\| x \\right\\|}} \\] <p>Here \\(f\\) is any problem.</p>"},{"location":"Condition%20and%20Stability/Conditioning/#matrix-conditioning-number","title":"Matrix Conditioning Number","text":""},{"location":"Condition%20and%20Stability/Conditioning/#formula-of-matrix-conditioning-number","title":"Formula of Matrix Conditioning Number","text":"<p>If \\(\\boldsymbol{A}\\in \\mathbb{R} ^{n\\times n}\\) , then matrix condition number can be defined as:</p> \\[ \\kappa \\left( \\boldsymbol{A} \\right) =\\left\\| \\boldsymbol{A} \\right\\| \\cdot \\left\\| \\boldsymbol{A} \\right\\| ^{-1} \\] <p>Why is this definition important?</p>"},{"location":"Condition%20and%20Stability/Conditioning/#proof-of-the-formula","title":"Proof of the Formula","text":"<p>Consider \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\), perturbations may occur to \\(\\boldsymbol{A}, \\boldsymbol{x}, \\boldsymbol{b}\\) .</p>"},{"location":"Condition%20and%20Stability/Conditioning/#case-1","title":"Case 1","text":"<p>\\(\\boldsymbol{x}\\rightarrow \\boldsymbol{x}+\\delta \\boldsymbol{x}\\) , \\(\\boldsymbol{A}\\) is given.</p> \\[ \\boldsymbol{Ax}=\\boldsymbol{b}, \\\\ \\boldsymbol{A}\\left( \\boldsymbol{x}+\\delta \\boldsymbol{x} \\right) =\\boldsymbol{b}+\\delta \\boldsymbol{b}, \\delta \\boldsymbol{b}=\\boldsymbol{A}\\delta \\boldsymbol{x}, \\] \\[ \\underset{\\left\\| \\delta \\boldsymbol{x} \\right\\| \\leqslant \\alpha}{\\mathrm{sup}}\\frac{\\frac{\\left\\| \\boldsymbol{A}\\left( \\boldsymbol{x}+\\delta \\boldsymbol{x} \\right) -\\boldsymbol{Ax} \\right\\|}{\\left\\| \\boldsymbol{Ax} \\right\\|}}{\\frac{\\left\\| \\delta \\boldsymbol{x} \\right\\|}{\\left\\| \\boldsymbol{x} \\right\\|}}=\\underset{\\left\\| \\delta \\boldsymbol{x} \\right\\| \\leqslant \\alpha}{\\mathrm{sup}}\\frac{\\frac{\\left\\| \\boldsymbol{A}\\delta \\boldsymbol{x} \\right\\|}{\\left\\| \\boldsymbol{Ax} \\right\\|}}{\\frac{\\left\\| \\delta \\boldsymbol{x} \\right\\|}{\\left\\| \\boldsymbol{x} \\right\\|}}=\\underset{\\left\\| \\delta \\boldsymbol{x} \\right\\| \\leqslant \\alpha}{\\mathrm{sup}}\\frac{\\left\\| \\boldsymbol{A}\\delta \\boldsymbol{x} \\right\\|}{\\left\\| \\delta \\boldsymbol{x} \\right\\|}\\cdot \\frac{\\left\\| \\boldsymbol{x} \\right\\|}{\\left\\| \\boldsymbol{Ax} \\right\\|} \\\\ =\\frac{\\left\\| \\boldsymbol{x} \\right\\|}{\\left\\| \\boldsymbol{Ax} \\right\\|}\\cdot \\underset{\\left\\| \\delta \\boldsymbol{x} \\right\\| \\leqslant \\alpha}{\\mathrm{sup}}\\frac{\\left\\| \\boldsymbol{A}\\delta \\boldsymbol{x} \\right\\|}{\\left\\| \\delta \\boldsymbol{x} \\right\\|}=\\frac{\\left\\| \\boldsymbol{x} \\right\\|}{\\left\\| \\boldsymbol{Ax} \\right\\|}\\cdot \\left\\| \\boldsymbol{A} \\right\\|  \\] <p>Introduce \\(\\boldsymbol{y}=\\boldsymbol{Ax}\\) , \\(\\boldsymbol{A}\\) is invertible, then:</p> \\[ \\boldsymbol{x}=\\boldsymbol{A}^{-1}\\boldsymbol{y}, \\] \\[ \\frac{\\left\\| \\boldsymbol{x} \\right\\|}{\\left\\| \\boldsymbol{Ax} \\right\\|}\\left\\| \\boldsymbol{A} \\right\\| =\\frac{\\left\\| \\boldsymbol{A}^{-1}\\boldsymbol{y} \\right\\|}{\\left\\| \\boldsymbol{y} \\right\\|}\\left\\| \\boldsymbol{A} \\right\\| \\underset{\\mathrm{achievable}}{\\underbrace{\\leqslant }}\\left\\| \\boldsymbol{A}^{-1} \\right\\| \\cdot \\left\\| \\boldsymbol{A} \\right\\|  \\]"},{"location":"Condition%20and%20Stability/Conditioning/#case-2","title":"Case 2","text":"<p>Perturbation happens to \\(\\boldsymbol{b}\\) :</p> \\[ \\boldsymbol{Ax}=\\boldsymbol{b}, \\boldsymbol{x}=\\boldsymbol{A}^{-1}\\boldsymbol{b}, \\\\ \\boldsymbol{b}\\rightarrow \\boldsymbol{b}+\\delta \\boldsymbol{b} \\] <p>Using case 1, we can easily have the condition number:</p> \\[ \\left\\| \\left( \\boldsymbol{A}^{-1} \\right) ^{-1} \\right\\| \\cdot \\left\\| \\boldsymbol{A}^{-1} \\right\\| =\\left\\| \\boldsymbol{A} \\right\\| \\cdot \\left\\| \\boldsymbol{A}^{-1} \\right\\|  \\]"},{"location":"Condition%20and%20Stability/Conditioning/#case-3","title":"Case 3","text":"<p>Given \\(\\boldsymbol{A}, \\boldsymbol{b}\\) , and let \\(\\boldsymbol{A}\\rightarrow \\boldsymbol{A}+\\delta \\boldsymbol{A}\\) . Then it incurs \\(\\boldsymbol{x}\\rightarrow \\boldsymbol{x}+\\delta \\boldsymbol{x}\\) . We can get:</p> \\[ \\left( \\boldsymbol{A}+\\delta \\boldsymbol{A} \\right) \\left( \\boldsymbol{x}+\\delta \\boldsymbol{x} \\right) =\\boldsymbol{b}, \\\\ \\Longrightarrow \\boldsymbol{Ax}+\\boldsymbol{A}\\cdot \\delta \\boldsymbol{x}+\\delta \\boldsymbol{A}\\cdot \\boldsymbol{x}+\\delta \\boldsymbol{A}\\cdot \\delta \\boldsymbol{x}=\\boldsymbol{b} \\] <p>Usually \\(\\delta \\boldsymbol{A}\\cdot \\delta \\boldsymbol{x}\\) is smaller than \\(\\delta \\boldsymbol{x}\\) or \\(\\delta \\boldsymbol{A}\\), then we can also get:</p> \\[ \\boldsymbol{A}\\cdot \\delta \\boldsymbol{x}+\\delta \\boldsymbol{A}\\cdot \\boldsymbol{x}=0, \\\\ \\boldsymbol{A}\\cdot \\delta \\boldsymbol{x}=-\\delta \\boldsymbol{A}\\cdot \\boldsymbol{x}, \\\\ \\delta \\boldsymbol{x}=\\boldsymbol{A}^{-1}\\left( -\\delta \\boldsymbol{A} \\right) \\boldsymbol{x}, \\\\ \\left\\| \\delta \\boldsymbol{x} \\right\\| \\leqslant \\left\\| \\boldsymbol{A}^{-1} \\right\\| \\left\\| \\delta \\boldsymbol{A} \\right\\| \\left\\| \\boldsymbol{x} \\right\\| , \\] \\[ \\Longrightarrow \\underset{\\left\\| \\delta \\boldsymbol{A} \\right\\| \\le \\alpha}{\\mathrm{sup}}\\frac{\\frac{\\left\\| \\delta \\boldsymbol{x} \\right\\|}{\\left\\| \\boldsymbol{x} \\right\\|}}{\\frac{\\left\\| \\delta \\boldsymbol{A} \\right\\|}{\\left\\| \\boldsymbol{A} \\right\\|}}\\le \\underset{\\left\\| \\delta \\boldsymbol{A} \\right\\| \\le \\alpha}{\\mathrm{sup}}\\frac{\\left\\| \\boldsymbol{A} \\right\\| \\left\\| \\boldsymbol{A}^{-1} \\right\\| \\left\\| \\delta \\boldsymbol{A} \\right\\| \\left\\| \\boldsymbol{x} \\right\\|}{\\left\\| \\delta \\boldsymbol{A} \\right\\| \\left\\| \\boldsymbol{x} \\right\\|} \\\\ =\\underset{\\left\\| \\delta \\boldsymbol{A} \\right\\| \\le \\alpha}{\\mathrm{sup}}\\left\\| \\boldsymbol{A} \\right\\| \\left\\| \\boldsymbol{A}^{-1} \\right\\| =\\left\\| \\boldsymbol{A} \\right\\| \\left\\| \\boldsymbol{A}^{-1} \\right\\|  \\] <p>Based on the three conditions above, we can derive the conclusion:</p> \\[ \\left\\| \\boldsymbol{A} \\right\\| \\left\\| \\boldsymbol{A}^{-1} \\right\\| =\\kappa \\left( \\boldsymbol{A} \\right)  \\]"},{"location":"Condition%20and%20Stability/Conditioning/#extensions","title":"Extensions","text":""},{"location":"Condition%20and%20Stability/Conditioning/#more-examples","title":"More Examples","text":"<p>Here are a few more examples:</p> <p>If:</p> \\[ \\boldsymbol{A}=\\left[ \\begin{matrix}     \\lambda _1&amp;     &amp;       0\\\\     &amp;       \\ddots&amp;     \\\\     0&amp;      &amp;       \\lambda _n\\\\ \\end{matrix} \\right] , \\lambda _i\\ne 0,\\lambda _1\\geqslant \\lambda _2\\geqslant \\cdots \\geqslant \\lambda _n \\] <p>then \\(\\kappa \\left( \\boldsymbol{A} \\right) =\\frac{\\lambda _1}{\\lambda _n}\\)</p> <p>Let's look at another example. If:</p> \\[ \\boldsymbol{A}=\\boldsymbol{U}\\mathbf{\\Sigma }\\boldsymbol{V}^*, \\mathbf{\\Sigma }=\\left[ \\begin{matrix}     \\sigma _1&amp;      &amp;       0\\\\     &amp;       \\ddots&amp;     \\\\     0&amp;      &amp;       \\sigma _n\\\\ \\end{matrix} \\right] , \\sigma _1\\geqslant \\sigma _2\\geqslant \\cdots \\geqslant \\sigma _n&gt;0 \\] <p>then:</p> \\[ \\left\\| \\boldsymbol{A} \\right\\| _2=\\sigma _1, \\\\ \\left\\| \\boldsymbol{A}^{-1} \\right\\| _2=\\frac{1}{\\sigma _n}, \\left( \\boldsymbol{A}^{-1}=\\boldsymbol{V}\\mathbf{\\Sigma }^{-1}\\boldsymbol{U}^* \\right)  \\\\ \\Longrightarrow \\kappa \\left( \\boldsymbol{A} \\right) =\\left\\| \\boldsymbol{A} \\right\\| _2\\cdot \\left\\| \\boldsymbol{A}^{-1} \\right\\| _2=\\frac{\\sigma _1}{\\sigma _n} \\]"},{"location":"Condition%20and%20Stability/Conditioning/#useful-properties","title":"Useful Properties","text":"<p>The \\(\\kappa \\left( \\boldsymbol{A} \\right)\\) satisfies \\(\\kappa \\left( \\boldsymbol{A} \\right) \\geqslant 1\\) .</p> <p>If \\(\\kappa \\left( \\boldsymbol{A} \\right)\\) is large, the question is called ill-conditioned. If \\(\\kappa \\left( \\boldsymbol{A} \\right)\\) is small, it is called well-conditioned.</p>"},{"location":"Condition%20and%20Stability/Stability/","title":"Stability","text":"<p>Stability is for the algorithms.</p> <p>Two concepts:</p> <ul> <li>Backward Stability</li> <li>General Stability</li> </ul> <p><code>Problem</code>: \\(f: X\\mapsto Y\\)</p> <p><code>Algorithm</code>: \\(\\tilde{f}: X\\mapsto Y\\)</p> <p>The algorithm \\(\\tilde{f}\\) is used to solve the problem \\(f\\).</p>"},{"location":"Condition%20and%20Stability/Stability/#backward-stability","title":"Backward Stability","text":""},{"location":"Condition%20and%20Stability/Stability/#definition-of-backward-stability","title":"Definition of Backward Stability","text":"<p><code>Definition</code> (Backward Stability): An algorithm \\(\\tilde{f}\\) is called backward stable if for each \\(x\\in X\\) :</p> \\[ \\tilde{f}\\left( x \\right) =f\\left( \\tilde{x} \\right)  \\] <p>for some \\(\\tilde{x}\\) with \\(\\frac{\\left\\| x-\\tilde{x} \\right\\|}{\\left\\| x \\right\\|}\\leqslant O\\left( \\varepsilon _{\\mathrm{machine}} \\right)\\).</p> <p>In words, a backward stable algorithm gives exactly the right solution to a nearly right problem.</p> <p><code>Theorem</code>: If \\(\\tilde{f}\\) is a backward stable algorithm for a problem \\(f: X\\mapsto Y\\) with condition number \\(\\kappa \\left( f \\right)\\) , then the relative error satisfies:</p> \\[ \\frac{\\left\\| \\tilde{f}\\left( x \\right) -f\\left( x \\right) \\right\\|}{\\left\\| f\\left( x \\right) \\right\\|}=O\\left( \\kappa \\left( f \\right) \\cdot \\varepsilon _{\\mathrm{machine}} \\right)  \\] <p><code>Proof</code>: \\(\\tilde{f}\\) is backward stable: \\(\\forall x\\in X\\) , \\(\\exists \\tilde{x}\\in X\\) such that \\(\\tilde{f}\\left( x \\right) =f\\left( \\tilde{x} \\right) , \\frac{\\left\\| x-\\tilde{x} \\right\\|}{\\left\\| x \\right\\|}\\leqslant O\\left( \\varepsilon _{\\mathrm{machine}} \\right)\\) . Then we can get:</p> \\[ \\frac{\\left\\| \\tilde{f}\\left( x \\right) -f\\left( x \\right) \\right\\|}{\\left\\| f\\left( x \\right) \\right\\|}=\\frac{\\left\\| f\\left( \\tilde{x} \\right) -f\\left( x \\right) \\right\\|}{\\left\\| f\\left( x \\right) \\right\\|}\\leqslant \\kappa \\left( f \\right) \\cdot \\frac{\\left\\| \\tilde{x}-x \\right\\|}{\\left\\| x \\right\\|}\\leqslant O\\left( \\kappa \\left( f \\right) \\cdot \\varepsilon _{\\mathrm{machine}} \\right)  \\] <p><code>Remark</code>: If \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) is ill-conditioned, one must expect to lose \\(\\log _{10}\\kappa \\left( \\boldsymbol{A} \\right)\\) digits in computing the solution with a backward stable method except under very special cases.</p>"},{"location":"Condition%20and%20Stability/Stability/#example-of-backward-stability","title":"Example of Backward Stability","text":"<p><code>Question</code>: How can we know an algorithm is backward stable or not? (tough and tedious)</p> <p><code>Example</code>: Forward substitution is backward stable.</p> <p>Consider \\(\\boldsymbol{Rx}=\\boldsymbol{b}\\) , where \\(\\boldsymbol{R}\\in \\mathbb{R} ^{m\\times m}\\) is a lower triangular matrix. The forward substitution steps are:</p> \\[ x_j=\\small{\\frac{b_j-\\sum_{k=1}^{j-1}{x_kr_{kj}}}{r_{jj}}} \\] <p>where \\(r_{ij}\\) is the element of \\(\\boldsymbol{R}\\) .</p> <p>Denote \\(+,-,\\times ,\\div\\) as exact operations and \\(\\oplus ,\\ominus ,\\otimes ,\\oslash\\) as operations with error.</p>"},{"location":"Condition%20and%20Stability/Stability/#case-1","title":"Case 1","text":"<p>Case \\(m=1\\) : Exact version:</p> \\[ r_{11}x_1=b_1\\Leftrightarrow x_1=\\frac{b_1}{r_{11}}=b_1\\div r_{11} \\] <p>Computed version:</p> \\[ \\tilde{x}_1=b_1\\oslash r_{11}=\\frac{b_1}{r_{11}}\\left( 1+\\varepsilon _1 \\right) , \\\\ \\varepsilon _1\\sim O\\left( \\varepsilon _{\\mathrm{machine}} \\right)  \\] <p>Introduce \\(\\left( 1+\\varepsilon _1 \\right) \\left( 1+\\varepsilon _1\\prime \\right) =1\\) . We can rewrite as:</p> \\[ \\tilde{x}_1=\\frac{b_1}{r_{11}\\left( 1+\\varepsilon _1\\prime \\right)}\\Longleftrightarrow r_{11}\\left( 1+\\varepsilon _1\\prime \\right) \\tilde{x}_1=b_1 \\] <p>The outcome can be viewed as an exact solution for a slightly perturbed problem.</p>"},{"location":"Condition%20and%20Stability/Stability/#case-2","title":"Case 2","text":"<p>Case \\(m=2\\) : </p> <p>Step 1: </p> \\[ \\tilde{x}_1=b_1\\oslash r_{11}=\\frac{b_1}{r_{11}\\left( 1+\\varepsilon _1\\prime \\right)} \\] <p>Step 2:</p> \\[ \\tilde{x}_2=\\left[ b_2\\ominus \\left( \\tilde{x}_1\\otimes r_{21} \\right) \\right] \\oslash r_{22}=\\left[ b_2\\ominus \\left( \\tilde{x}_1\\cdot r_{21} \\right) \\cdot \\left( 1+\\varepsilon _2 \\right) \\right] \\oslash r_{22} \\] \\[ =\\left[ b_2-\\left( \\tilde{x}_1\\cdot r_{21} \\right) \\left( 1+\\varepsilon _2 \\right) \\right] \\left( 1+\\varepsilon _3 \\right) \\oslash r_{22} \\] \\[ =\\left[ b_2-\\left( \\tilde{x}_1\\cdot r_{21} \\right) \\left( 1+\\varepsilon _2 \\right) \\right] \\left( 1+\\varepsilon _3 \\right) \\div r_{22}\\left( 1+\\varepsilon _4 \\right)  \\] \\[ =\\frac{b_2-\\left( \\tilde{x}_1\\cdot r_{21} \\right) \\left( 1+\\varepsilon _2 \\right)}{r_{22}}\\left( 1+\\varepsilon _3 \\right) \\left( 1+\\varepsilon _4 \\right)  \\] <p>Introduce \\(\\left( 1+\\varepsilon _i \\right) \\left( 1+\\varepsilon _i\\prime \\right) =1\\) , then:</p> \\[ \\tilde{x}_2=\\frac{b_2-\\left( \\tilde{x}_1\\cdot r_{21} \\right) \\left( 1+\\varepsilon _2 \\right)}{r_{22}\\left( 1+\\varepsilon _3\\prime \\right) \\left( 1+\\varepsilon _4\\prime \\right)} \\\\ =\\frac{b_2-\\tilde{x}_1\\cdot \\left( r_{21}\\left( 1+\\varepsilon _2 \\right) \\right)}{r_{22}\\left( 1+2\\varepsilon _5\\prime \\right)} \\] <p>Then:</p> \\[ \\left( r_{21}\\left( 1+\\varepsilon _2 \\right) \\right) \\cdot \\tilde{x}_1+r_{22}\\left( 1+2\\varepsilon _5\\prime \\right) \\cdot \\tilde{x}_2=b_2 \\] <p>We can claim that solving \\(\\boldsymbol{Rx}=\\boldsymbol{b}\\) by a computer with a numerical solution \\(\\hat{\\boldsymbol{x}}\\) can be viewed as \\(\\left( \\boldsymbol{R}+\\delta \\boldsymbol{R} \\right) \\hat{\\boldsymbol{x}}=\\boldsymbol{b}\\) exactly. Also:</p> \\[ \\delta \\boldsymbol{R}=\\left[ \\begin{matrix}     \\varepsilon _1\\prime r_{11}&amp;        0\\\\     \\varepsilon _2r_{21}&amp;       2\\varepsilon _5\\prime r_{22}\\\\ \\end{matrix} \\right] \\]"},{"location":"Condition%20and%20Stability/Stability/#general-case","title":"General Case","text":"<p>For general \\(m\\) , \\(\\hat{\\boldsymbol{x}}\\) satisfies:</p> \\[ \\frac{\\left\\| \\delta \\boldsymbol{R} \\right\\|}{\\left\\| \\boldsymbol{R} \\right\\|}\\leqslant \\left\\| \\left[ \\begin{matrix}     1&amp;      &amp;       &amp;       O\\\\     \\vdots&amp;     2&amp;      &amp;       \\\\     \\vdots&amp;     \\vdots&amp;     \\ddots&amp;     \\\\     m-1&amp;        m-1&amp;        \\cdots&amp;     m\\\\ \\end{matrix} \\right] \\right\\| \\cdot \\varepsilon _{\\mathrm{machine}} \\] <p>Finally, we can claim that forward substitution algorithm computes the solution \\(\\hat{\\boldsymbol{x}}\\) of \\(\\boldsymbol{Rx}=\\boldsymbol{b}\\) satisfying </p> \\[ \\left( \\boldsymbol{R}+\\delta \\boldsymbol{R} \\right) \\hat{\\boldsymbol{x}}=\\boldsymbol{b} \\] <p>for some \\(\\delta \\boldsymbol{R}\\) satisfying</p> \\[ \\frac{\\left\\| \\delta \\boldsymbol{R} \\right\\|}{\\left\\| \\boldsymbol{R} \\right\\|}=O\\left( \\varepsilon _{\\mathrm{machine}} \\right)  \\] <p>Also, forward and backward substitutions are all backward stable.</p>"},{"location":"Condition%20and%20Stability/Stability/#general-stability","title":"General Stability","text":""},{"location":"Condition%20and%20Stability/Stability/#definition-of-general-stability","title":"Definition of General Stability","text":"<p><code>Problem</code>: \\(f: X\\mapsto Y\\)</p> <p><code>Algorithm</code>: \\(\\tilde{f}: X\\mapsto Y\\)</p> <p><code>Definition</code>: \\(\\tilde{f}\\) is called stable if for \\(\\forall x\\in X\\) ,</p> \\[ \\frac{\\left\\| \\tilde{f}\\left( x \\right) -f\\left( \\tilde{x} \\right) \\right\\|}{\\left\\| f\\left( \\tilde{x} \\right) \\right\\|}=O\\left( \\kappa \\left( f \\right) \\cdot \\varepsilon _{\\mathrm{machine}} \\right)  \\] <p>for some \\(\\tilde{x}\\) satisfying \\(\\frac{\\left\\| \\tilde{x}-x \\right\\|}{\\left\\| x \\right\\|}=O\\left( \\varepsilon _{\\mathrm{machine}} \\right)\\).</p> <p><code>Conclusion</code>: If \\(\\tilde{f}\\) is backward stable, then it is also stable.</p> <p>In words, a stable algorithm gives nearly the right solution to nearly the right algorithm.</p>"},{"location":"Condition%20and%20Stability/Stability/#conclusions-on-gaussian-elimination","title":"Conclusions on Gaussian Elimination","text":"<p><code>Theorem</code>: Let \\(\\boldsymbol{A}=\\boldsymbol{LU}\\) of a nonsingular matrix \\(\\boldsymbol{A}\\in \\mathbb{R} ^{m\\times m}\\) be computed by Gaussian Elimination without pivoting, then:</p> \\[ \\tilde{\\boldsymbol{L}}\\tilde{\\boldsymbol{U}}=\\boldsymbol{A}+\\delta \\boldsymbol{A} \\] <p>for some \\(\\delta \\boldsymbol{A}\\in \\mathbb{R} ^{m\\times m}\\) satisfying \\(\\frac{\\left\\| \\delta \\boldsymbol{A} \\right\\|}{\\left\\| \\boldsymbol{L} \\right\\| \\left\\| \\boldsymbol{U} \\right\\|}=O\\left( \\varepsilon _{\\mathrm{machine}} \\right)\\).</p> <p>If \\(\\left\\| \\boldsymbol{L} \\right\\| \\left\\| \\boldsymbol{U} \\right\\| =O\\left( \\left\\| \\boldsymbol{A} \\right\\| \\right)\\) , then Gaussian Elimination without pivoting is stable. Otherwise, the method is not stable.</p> <p><code>Theorem</code>: Gaussian Elimination with partial pivoting is backward stable:</p> \\[ \\tilde{\\boldsymbol{L}}\\tilde{\\boldsymbol{U}}=\\tilde{\\boldsymbol{P}}\\left( \\boldsymbol{A}+\\delta \\boldsymbol{A} \\right)  \\] <p>for some \\(\\delta \\boldsymbol{A}\\in \\mathbb{R} ^{m\\times m}\\) satisfying \\(\\frac{\\left\\| \\delta \\boldsymbol{A} \\right\\|}{\\left\\| \\boldsymbol{A} \\right\\|}=O\\left( \\rho \\cdot \\varepsilon _{\\mathrm{machine}} \\right)\\) where</p> \\[ \\rho =\\frac{\\max_{i,j} \\left| \\tilde{u}_{ij} \\right|}{\\max_{i,j} \\left| a_{ij} \\right|} \\] <p>is called growth factor.</p> <p><code>Theorem</code>: If \\(\\boldsymbol{A}\\) is symmetric positive definite (SPD), then Cholesky Factorization is backward stable:</p> \\[ \\tilde{\\boldsymbol{R}}^T\\tilde{\\boldsymbol{R}}=\\boldsymbol{A}+\\delta \\boldsymbol{A} \\] <p>for some \\(\\delta \\boldsymbol{A}\\) satisfying \\(\\frac{\\left\\| \\delta \\boldsymbol{A} \\right\\|}{\\left\\| \\boldsymbol{A} \\right\\|}=O\\left( \\varepsilon _{\\mathrm{machine}} \\right)\\).</p>"},{"location":"Eigenvalue/","title":"Eigenvalue Problems","text":"<p>Introduction to Eigenvalue Problems</p> <p>QR Factorization</p> <p>Classical Eigenvalue Algorithms</p> <p>QR Algorithm</p> <p>Divide and Conquer</p> <p>SVD</p> <p><code>Quick review</code>: Gram-Schmidt Process</p> <p>If a vector group \\(\\boldsymbol{\\alpha }_1,\\boldsymbol{\\alpha }_2,\\cdots ,\\boldsymbol{\\alpha }_k\\) is linearly independent, let:</p> \\[ \\boldsymbol{\\beta }_1=\\boldsymbol{\\alpha }_1, \\] \\[ \\boldsymbol{\\beta }_2=\\boldsymbol{\\alpha }_2-\\frac{\\left( \\boldsymbol{\\alpha }_2,\\boldsymbol{\\beta }_1 \\right)}{\\left( \\boldsymbol{\\beta }_1,\\boldsymbol{\\beta }_1 \\right)}\\boldsymbol{\\beta }_1, \\] \\[ \\boldsymbol{\\beta }_3=\\boldsymbol{\\alpha }_3-\\frac{\\left( \\boldsymbol{\\alpha }_3,\\boldsymbol{\\beta }_1 \\right)}{\\left( \\boldsymbol{\\beta }_1,\\boldsymbol{\\beta }_1 \\right)}\\boldsymbol{\\beta }_1-\\frac{\\left( \\boldsymbol{\\alpha }_2,\\boldsymbol{\\beta }_2 \\right)}{\\left( \\boldsymbol{\\beta }_2,\\boldsymbol{\\beta }_2 \\right)}\\boldsymbol{\\beta }_2, \\] \\[ \\vdots  \\] \\[ \\boldsymbol{\\beta }_k=\\boldsymbol{\\alpha }_k-\\frac{\\left( \\boldsymbol{\\alpha }_k,\\boldsymbol{\\beta }_1 \\right)}{\\left( \\boldsymbol{\\beta }_1,\\boldsymbol{\\beta }_1 \\right)}\\boldsymbol{\\beta }_1-\\frac{\\left( \\boldsymbol{\\alpha }_k,\\boldsymbol{\\beta }_2 \\right)}{\\left( \\boldsymbol{\\beta }_2,\\boldsymbol{\\beta }_2 \\right)}\\boldsymbol{\\beta }_2-\\cdots -\\frac{\\left( \\boldsymbol{\\alpha }_k-\\boldsymbol{\\beta }_{k-1} \\right)}{\\left( \\boldsymbol{\\beta }_{k-1},\\boldsymbol{\\beta }_{k-1} \\right)}\\boldsymbol{\\beta }_{k-1} \\] <p>Then \\(\\boldsymbol{\\beta }_1,\\boldsymbol{\\beta }_2,\\cdots ,\\boldsymbol{\\beta }_k\\) are orthogonal to each other. Unitize them, we get:</p> \\[ \\boldsymbol{\\gamma }_1=\\frac{\\boldsymbol{\\beta }_1}{\\left\\| \\boldsymbol{\\beta }_1 \\right\\|},\\boldsymbol{\\gamma }_2=\\frac{\\boldsymbol{\\beta }_2}{\\left\\| \\boldsymbol{\\beta }_2 \\right\\|},\\cdots ,\\boldsymbol{\\gamma }_k=\\frac{\\boldsymbol{\\beta }_k}{\\left\\| \\boldsymbol{\\beta }_k \\right\\|} \\] <p>The process from \\(\\boldsymbol{\\alpha }_1,\\boldsymbol{\\alpha }_2,\\cdots ,\\boldsymbol{\\alpha }_k\\) to \\(\\boldsymbol{\\gamma }_1,\\boldsymbol{\\gamma }_2,\\cdots ,\\boldsymbol{\\gamma }_k\\) is called Gram-Schmidt Orthogonalization. We can also get:</p> \\[ \\boldsymbol{A}=\\left[ \\begin{matrix}     \\boldsymbol{\\alpha }_1&amp;     \\boldsymbol{\\alpha }_2&amp;     \\cdots&amp;     \\boldsymbol{\\alpha }_k\\\\ \\end{matrix} \\right]  \\] \\[ =\\left[ \\begin{matrix}     \\boldsymbol{\\gamma }_1&amp;     \\boldsymbol{\\gamma }_2&amp;     \\cdots&amp;     \\boldsymbol{\\gamma }_k\\\\ \\end{matrix} \\right]\\cdot \\left[ \\begin{matrix}     \\left\\| \\boldsymbol{\\beta }_1 \\right\\|&amp;     \\left( \\boldsymbol{\\alpha }_2,\\boldsymbol{\\gamma }_1 \\right)&amp;       \\cdots&amp;     \\left( \\boldsymbol{\\alpha }_k,\\boldsymbol{\\gamma }_1 \\right)\\\\     0&amp;      \\left\\| \\boldsymbol{\\beta }_2 \\right\\|&amp;     \\cdots&amp;     \\left( \\boldsymbol{\\alpha }_k,\\boldsymbol{\\gamma }_2 \\right)\\\\     \\vdots&amp;     \\ddots&amp;     \\ddots&amp;     \\vdots\\\\     0&amp;      \\cdots&amp;     0&amp;      \\left\\| \\boldsymbol{\\beta }_k \\right\\|\\\\ \\end{matrix} \\right]  \\]"},{"location":"Eigenvalue/Classical_Eigenvalue_Algorithms/","title":"Classical Eigenvalue Algorithms","text":""},{"location":"Eigenvalue/Classical_Eigenvalue_Algorithms/#preprocessing-phase-one","title":"Preprocessing: Phase One","text":""},{"location":"Eigenvalue/Classical_Eigenvalue_Algorithms/#general-steps-for-phase-one","title":"General Steps for Phase One","text":"<p>Eigenvalue problem phase 1: \\(\\boldsymbol{A}\\rightarrow \\boldsymbol{H}\\,\\,\\left( \\boldsymbol{A}\\sim \\boldsymbol{H} \\right)\\), \\(\\boldsymbol{H}\\) is a upper Hessenberg matrix. We get \\(\\boldsymbol{A}=\\boldsymbol{XHX}^{-1}\\).</p> <p>For example, let \\(\\boldsymbol{A}\\in \\mathbb{R} ^{n\\times n}\\), the steps for phase 1 can be shown as:</p> \\[ \\boldsymbol{A}=\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\ \\end{matrix} \\right] \\xrightarrow{{\\boldsymbol{Q}_1}^*\\left( \\mathrm{left} \\right)}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\ \\end{matrix} \\right] \\xrightarrow[\\boldsymbol{Q}_1\\left( \\mathrm{right} \\right)]{}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\ \\end{matrix} \\right]  \\] \\[ \\xrightarrow{{\\boldsymbol{Q}_2}^*\\left( \\mathrm{left} \\right)}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\ \\end{matrix} \\right] \\xrightarrow[\\boldsymbol{Q}_2\\left( \\mathrm{right} \\right)]{}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\ \\end{matrix} \\right] =\\boldsymbol{H} \\]"},{"location":"Eigenvalue/Classical_Eigenvalue_Algorithms/#householder-reduction","title":"Householder Reduction","text":"<p>Algorithm (Householder Reduction to upper Hessenberg form):</p> <p>For \\(k=1:m-2\\):</p> <ul> <li>\\(\\boldsymbol{x}=\\boldsymbol{A}_{k+1:m,k}\\);</li> <li>\\(\\boldsymbol{v}_k=\\mathrm{sign}\\left( x_1 \\right) \\left\\| \\boldsymbol{x} \\right\\| \\boldsymbol{e}_1+\\boldsymbol{x}\\);</li> <li>\\(\\boldsymbol{v}_k=\\frac{\\boldsymbol{v}_k}{\\left\\| \\boldsymbol{v}_k \\right\\|}\\);</li> <li>\\(\\boldsymbol{A}_{k+1:m,k:m}=\\boldsymbol{A}_{k+1:m,k:m}-2\\boldsymbol{v}_k\\left( {\\boldsymbol{v}_k}^*\\cdot \\boldsymbol{A}_{k+1:m,k:m} \\right)\\);</li> <li>\\(\\boldsymbol{A}_{1:m,k+1:m}=\\boldsymbol{A}_{1:m,k+1:m}-2\\left( \\boldsymbol{A}_{1:m,k+1:m}\\cdot \\boldsymbol{v}_k \\right) {\\boldsymbol{v}_k}^*\\);</li> </ul> <p>End</p> <p>The cost of the algorithm is \\(O\\left( \\frac{10}{3}m^3 \\right)\\).</p> <p>Why does symmetric positive definite (SPD) matrix is easier to compute? Most stable situation in physics.</p> <p>If \\(\\boldsymbol{A}\\) is symmetric, the cost is reduced to \\(O\\left( \\frac{4}{3}m^3 \\right)\\), and \\(\\boldsymbol{H}\\) is tridiagonal and also symmetric.</p> <p><code>Theorem</code> (backward stability): Let the Hessenberg reduction \\(\\boldsymbol{A}=\\boldsymbol{QHQ}^*\\) be computed by the householder algorithm, then we have \\(\\tilde{\\boldsymbol{Q}}\\tilde{\\boldsymbol{H}}\\tilde{\\boldsymbol{Q}}^*=\\boldsymbol{A}+\\delta \\boldsymbol{A}\\) where for some \\(\\delta \\boldsymbol{A}\\) satisfying \\(\\frac{\\left\\| \\delta \\boldsymbol{A} \\right\\|}{\\left\\| \\boldsymbol{A} \\right\\|}=O\\left( \\varepsilon _{\\mathrm{machine}} \\right)\\).</p> <p>We discuss some classical eigenvalue solvers below. Assume \\(\\boldsymbol{A}\\) is SPD, and eigenvalues \\(\\lambda _1,\\lambda _2,\\cdots \\lambda\\) satisfy \\(\\lambda _1\\geqslant \\lambda _2\\geqslant \\cdots \\geqslant \\lambda _m\\). Their corresponding eigenvectors are \\(\\boldsymbol{q}_1,\\boldsymbol{q}_2,\\cdots \\boldsymbol{q}_m\\).</p>"},{"location":"Eigenvalue/Classical_Eigenvalue_Algorithms/#power-iteration","title":"Power Iteration","text":"<p>Algorithm (Power Iteration):</p> <p>Pick \\(\\boldsymbol{v}^{\\left( 0 \\right)}\\) as some vector with \\(\\left\\| \\boldsymbol{v}^{\\left( 0 \\right)} \\right\\| =1\\);</p> <p>For \\(k=1,2,\\cdots\\):</p> <ul> <li>\\(\\boldsymbol{w}=\\boldsymbol{Av}^{\\left( k-1 \\right)}\\);</li> <li>\\(\\boldsymbol{v}^{\\left( k \\right)}=\\frac{\\boldsymbol{w}}{\\left\\| \\boldsymbol{w} \\right\\|}\\);</li> <li>\\(\\lambda ^{\\left( k \\right)}=\\left( \\boldsymbol{v}^{\\left( k \\right)} \\right) ^T\\boldsymbol{Av}^{\\left( k \\right)}\\);</li> </ul> <p>End</p> <p><code>Claim of Convergence</code>: \\(\\lambda ^{\\left( k \\right)}\\rightarrow \\lambda _1,\\boldsymbol{v}^{\\left( k \\right)}\\rightarrow \\boldsymbol{q}_1\\) as \\(k\\rightarrow +\\infty\\).</p> <p><code>Proof</code>: We can write \\(\\boldsymbol{v}^{\\left( k \\right)}=c_k\\boldsymbol{A}^k\\boldsymbol{v}^{\\left( 0 \\right)}\\). We can also get \\(\\boldsymbol{v}^{\\left( 0 \\right)}=a_1\\boldsymbol{q}_1+a_2\\boldsymbol{q}_2+\\cdots +a_m\\boldsymbol{q}_m\\). Therefore, </p> \\[ \\boldsymbol{v}^{\\left( k \\right)}=c_k\\boldsymbol{A}^k\\boldsymbol{v}^{\\left( 0 \\right)}=c_k\\boldsymbol{A}^k\\left( a_1\\boldsymbol{q}_1+a_2\\boldsymbol{q}_2+\\cdots +a_m\\boldsymbol{q}_m \\right)  \\] \\[ =c_k\\left( a_1\\boldsymbol{A}^k\\boldsymbol{q}_1+a_2\\boldsymbol{A}^k\\boldsymbol{q}_2+\\cdots +a_m\\boldsymbol{A}^k\\boldsymbol{q}_m \\right)  \\] \\[ =c_k\\left( a_1{\\lambda _1}^k\\boldsymbol{q}_1+a_2{\\lambda _2}^k\\boldsymbol{q}_2+\\cdots +a_m{\\lambda _m}^k\\boldsymbol{q}_m \\right)  \\] \\[ =c_k{\\lambda _1}^k\\left( a_1\\boldsymbol{q}_1+a_2\\frac{{\\lambda _2}^k}{{\\lambda _1}^k}\\boldsymbol{q}_2+\\cdots +a_m\\frac{{\\lambda _m}^k}{{\\lambda _1}^k}\\boldsymbol{q}_m \\right)  \\] <p>If \\({\\lambda}_1 &gt; {\\lambda}_2\\), we know that \\(\\left| \\frac{\\lambda _i}{\\lambda _1} \\right|&lt;1, i\\geqslant 2\\). Therefore, \\(\\left( \\frac{{\\lambda _i}^k}{{\\lambda _1}^k} \\right) \\rightarrow 0\\) as \\(k\\rightarrow +\\infty\\). We can get the result.</p> <p>In conclusion, \\(\\boldsymbol{v}^{\\left( k \\right)}\\rightarrow \\pm \\boldsymbol{q}_1; \\lambda ^{\\left( k \\right)}=\\left( \\boldsymbol{v}^{\\left( k \\right)} \\right) ^T\\boldsymbol{Av}^{\\left( k \\right)}\\rightarrow {\\boldsymbol{q}_1}^T\\boldsymbol{Aq}_1=\\lambda _1\\).</p> <p><code>Theorem</code>: Suppose \\(\\left| \\lambda _1 \\right|&gt;\\left| \\lambda _2 \\right|\\geqslant \\cdots \\geqslant \\left| \\lambda _m \\right|&gt;0\\), and \\({\\boldsymbol{q}_1}^T\\boldsymbol{v}^{\\left( 0 \\right)}\\ne 0 \\left( a_1\\ne 0 \\right)\\), then the Power Iteration has:</p> \\[ \\left\\| \\boldsymbol{v}^{\\left( k \\right)}-\\left( \\pm \\boldsymbol{q}_1 \\right) \\right\\| =O\\left( \\left| \\frac{\\lambda _2}{\\lambda _1} \\right|^k \\right) ; \\] \\[ \\left\\| \\lambda ^{\\left( k \\right)}-\\lambda _1 \\right\\| =O\\left( \\left| \\frac{\\lambda _2}{\\lambda _1} \\right|^{2k} \\right)  \\] <p>as \\(k\\rightarrow +\\infty\\). \\(\\pm 1\\) sign means one of them is correct.</p> <p>Comments:</p> <ol> <li>\\({\\boldsymbol{q}_1}^T\\boldsymbol{v}^{\\left( 0 \\right)}\\ne 0\\) is only required for the theory purpose. In practice, this is rarely needed because the computer may perturb.</li> <li>The eigenvalue convergence is quadratic, while the eigenvector convergence is linear.</li> <li>Power Iteration only gives \\(\\lambda _1\\) and \\(\\boldsymbol{q}_1\\).</li> <li>The rate of convergence is determined by \\(\\left| \\frac{\\lambda _2}{\\lambda _1} \\right|\\) (\"Spectral Gap\"). If \\(\\left| \\frac{\\lambda _2}{\\lambda _1} \\right|\\sim 1\\), the convergence is slow.</li> </ol> <p>To capture the smallest (in absolute value sense) eigenvalue, we can use Power Iteration to \\(\\boldsymbol{A}^{-1}\\): \\(\\boldsymbol{A}^{-1}\\boldsymbol{v}^{\\left( 0 \\right)}=\\boldsymbol{w}, \\boldsymbol{v}^{\\left( 1 \\right)}=\\frac{\\boldsymbol{w}}{\\left\\| \\boldsymbol{w} \\right\\|}\\). This gives us \\(\\frac{1}{\\lambda _m}, \\boldsymbol{q}_m\\). In practice, we should solve linear systems instead of getting the inverse directly: \\(\\boldsymbol{A}^{-1}\\boldsymbol{v}^{\\left( 0 \\right)}=\\boldsymbol{w}\\Longleftrightarrow \\boldsymbol{Aw}=\\boldsymbol{v}^{\\left( 0 \\right)}\\). Do \\(\\boldsymbol{A}=\\boldsymbol{LU}\\) first, then solve \\(\\boldsymbol{LUw}=\\boldsymbol{v}^{\\left( 0 \\right)}\\) or generally \\(\\boldsymbol{LUw}=\\boldsymbol{v}^{\\left( k \\right)}\\).</p>"},{"location":"Eigenvalue/Classical_Eigenvalue_Algorithms/#inverse-iteration-with-shift","title":"Inverse Iteration with Shift","text":"<p><code>Question</code>: How can we use Power Iteration to find the eigenvalue/eigenvector close to a number \\(\\mu\\) ?</p> <p>Inverse Iteration with Shift can solve this problem.</p> <p><code>Claim</code>: If \\(\\mu \\in \\mathbb{R}\\) is not an eigenvalue of \\(\\boldsymbol{A}\\), then \\(\\left( \\boldsymbol{A}-\\mu \\mathbf{I} \\right) ^{-1}\\) and \\(\\boldsymbol{A}\\) has the same eigenvectors, and their corresponding eigenvalues are \\(\\left( \\lambda _i-\\mu \\right) ^{-1}\\) and \\(\\lambda _i\\) respectively.</p> <p><code>Proof</code>: \\(\\left( \\boldsymbol{A}-\\mu \\mathbf{I} \\right) ^{-1}\\) exists. If \\(\\boldsymbol{q}_i\\) is an eigenvector of \\(\\boldsymbol{A}\\), then \\(\\boldsymbol{Aq}_i=\\lambda _i\\boldsymbol{q}_i\\). We can get \\(\\boldsymbol{Aq}_i-\\mu \\boldsymbol{q}_i=\\left( \\lambda _i-\\mu \\right) \\boldsymbol{q}_i\\Longleftrightarrow \\left( \\boldsymbol{A}-\\mu \\mathbf{I} \\right) \\boldsymbol{q}_i=\\left( \\lambda _i-\\mu \\right) \\boldsymbol{q}_i\\), and \\(\\frac{1}{\\lambda _i-\\mu}\\boldsymbol{q}_i=\\left( \\boldsymbol{A}-\\mu \\mathbf{I} \\right) ^{-1}\\boldsymbol{q}_i\\). Therefore, \\(\\boldsymbol{q}_i\\) is an eigenvector of \\(\\left( \\boldsymbol{A}-\\mu \\mathbf{I} \\right) ^{-1}\\).</p> <p>If \\(\\mu\\) is the closest to \\(\\lambda _J\\), then \\(\\left( \\lambda _J-\\mu \\right) ^{-1}\\) is the largest eigenvalue for \\(\\left( \\boldsymbol{A}-\\mu \\mathbf{I} \\right) ^{-1}\\).</p> <p>Apply Power Iteration to \\(\\left( \\boldsymbol{A}-\\mu \\mathbf{I} \\right) ^{-1}\\), we can obtain \\(\\boldsymbol{q}_J\\) and \\(\\left( \\lambda _J-\\mu \\right) ^{-1}\\).</p> <p>Algorithm (Inverse Iteration with Shift):</p> <p>\\(\\boldsymbol{v}^{\\left( 0 \\right)}\\) is selected with \\(\\left\\| \\boldsymbol{v}^{\\left( 0 \\right)} \\right\\| =1\\); Also solve \\(\\boldsymbol{LU}=\\boldsymbol{A}-\\mu \\mathbf{I}\\);</p> <p>For \\(k=1,2,\\cdots\\):</p> <ul> <li>Solve \\(\\boldsymbol{LUw}=\\boldsymbol{v}^{\\left( k-1 \\right)}\\) for \\(\\boldsymbol{w}\\);</li> <li>\\(\\boldsymbol{v}^{\\left( k \\right)}=\\frac{\\boldsymbol{w}}{\\left\\| \\boldsymbol{w} \\right\\|}\\);</li> <li>\\(\\lambda ^{\\left( k \\right)}=\\left( \\boldsymbol{v}^{\\left( k \\right)} \\right) ^T\\boldsymbol{Av}^{\\left( k \\right)}\\);</li> </ul> <p>End</p> <p><code>Theorem</code>: Assume \\(\\lambda _J\\) is closest to \\(\\mu\\), and \\(\\left| \\mu -\\lambda _J \\right|&lt;\\left| \\mu -\\lambda _K \\right|\\leqslant \\left| \\mu -\\lambda _i \\right|, \\left( i\\ne J \\right)\\). We can get:</p> \\[ \\left\\| \\boldsymbol{v}^{\\left( k \\right)}-\\left| \\pm \\boldsymbol{q}_j \\right| \\right\\| =O\\left( \\left| \\frac{\\mu -\\lambda _J}{\\mu -\\lambda _K} \\right|^k \\right) ; \\] \\[ \\left\\| \\lambda _J-\\lambda ^{\\left( k \\right)} \\right\\| =O\\left( \\left| \\frac{\\mu -\\lambda _J}{\\mu -\\lambda _K} \\right|^{2k} \\right)  \\] <p>as \\(k\\rightarrow +\\infty\\).</p>"},{"location":"Eigenvalue/Classical_Eigenvalue_Algorithms/#rayleigh-quotient-iteration","title":"Rayleigh Quotient Iteration","text":""},{"location":"Eigenvalue/Classical_Eigenvalue_Algorithms/#rayleigh-quotient","title":"Rayleigh Quotient","text":"<p>Given matrix \\(\\boldsymbol{A}\\), the Rayleigh Quotient of a vector \\(\\boldsymbol{x}\\ne 0\\) is the ratio:</p> \\[ r\\left( \\boldsymbol{x} \\right) =\\frac{\\boldsymbol{x}^T\\boldsymbol{Ax}}{\\boldsymbol{x}^T\\boldsymbol{x}} \\] <p>If \\(\\boldsymbol{x}\\) is an eigenvector, then \\(r\\left( \\boldsymbol{x} \\right)\\) is its corresponding eigenvalue: \\(r\\left( \\boldsymbol{x} \\right) =\\frac{\\boldsymbol{x}^T\\boldsymbol{Ax}}{\\boldsymbol{x}^T\\boldsymbol{x}}=\\frac{\\boldsymbol{x}^T\\lambda \\boldsymbol{x}}{\\boldsymbol{x}^T\\boldsymbol{x}}=\\lambda\\).</p> <p><code>Claim</code>: If \\(\\boldsymbol{x}\\) is close to an eigenvector, then \\(r\\left( \\boldsymbol{x} \\right)\\) is also close to the corresponding eigenvalue (due to continuity).</p> <p>Consider \\(\\frac{\\partial r\\left( \\boldsymbol{x} \\right)}{\\partial x_j}=\\frac{2}{\\boldsymbol{x}^T\\boldsymbol{x}}\\cdot \\left( \\boldsymbol{Ax}-r\\left( \\boldsymbol{x} \\right) \\cdot \\boldsymbol{x} \\right) _j, j=1,2,\\cdots ,m\\) (How to get this result?). We can get:</p> \\[ \\nabla r\\left( \\boldsymbol{x} \\right) =\\left[ \\begin{array}{c}     \\frac{\\partial r\\left( \\boldsymbol{x} \\right)}{\\partial x_1}\\\\     \\vdots\\\\     \\frac{\\partial r\\left( \\boldsymbol{x} \\right)}{\\partial x_m}\\\\ \\end{array} \\right] =\\frac{2}{\\boldsymbol{x}^T\\boldsymbol{x}}\\cdot \\left( \\boldsymbol{Ax}-r\\left( \\boldsymbol{x} \\right) \\cdot \\boldsymbol{x} \\right)  \\] <p>If \\(\\boldsymbol{x}\\) is an eigenvector of \\(\\boldsymbol{A}\\), then \\(\\nabla r\\left( \\boldsymbol{x} \\right) =\\mathbf{0}\\). If \\(\\boldsymbol{x}\\ne 0\\) satisfies \\(\\nabla r\\left( \\boldsymbol{x} \\right) =\\mathbf{0}\\), then \\(\\boldsymbol{x}\\) is an eigenvector of \\(\\boldsymbol{A}\\).</p> <p>Take the Taylor Expansion of \\(r\\left( \\boldsymbol{x} \\right)\\) at an eigenvector \\(\\boldsymbol{q}_J\\). We know:</p> \\[ r\\left( \\boldsymbol{x} \\right) =r\\left( \\boldsymbol{q}_J \\right) +\\left( \\boldsymbol{x}-\\boldsymbol{q}_J \\right) \\nabla r\\left( \\boldsymbol{q}_J \\right) +\\frac{1}{2}\\left\\| \\boldsymbol{x}-\\boldsymbol{q}_J \\right\\| ^2\\nabla ^2r\\left( \\boldsymbol{q}_J \\right) +\\cdots  \\] <p>If \\(\\boldsymbol{x}\\) is in a small neighborhood of \\(\\boldsymbol{q}_J\\), we can get:</p> \\[ r\\left( \\boldsymbol{x} \\right) =r\\left( \\boldsymbol{q}_J \\right) +\\left( \\boldsymbol{x}-\\boldsymbol{q}_J \\right) \\underset{0}{\\underbrace{\\nabla r\\left( \\boldsymbol{q}_J \\right) }}+O\\left( \\left\\| \\boldsymbol{x}-\\boldsymbol{q}_J \\right\\| ^2 \\right)  \\] \\[ =r\\left( \\boldsymbol{q}_J \\right) +O\\left( \\left\\| \\boldsymbol{x}-\\boldsymbol{q}_J \\right\\| ^2 \\right) =\\lambda _J+O\\left( \\left\\| \\boldsymbol{x}-\\boldsymbol{q}_J \\right\\| ^2 \\right)  \\]"},{"location":"Eigenvalue/Classical_Eigenvalue_Algorithms/#rayleigh-quotient-iteration-algorithm","title":"Rayleigh Quotient Iteration Algorithm","text":"<p>Algorithm (Rayleigh Quotient Iteration):</p> <p>Pick vector \\(\\boldsymbol{v}^{\\left( 0 \\right)}\\) as \\(\\left\\| \\boldsymbol{v}^{\\left( 0 \\right)} \\right\\| =1\\); Also \\(\\lambda ^{\\left( 0 \\right)}=\\boldsymbol{v}^{\\left( 0 \\right)}\\boldsymbol{Av}^{\\left( 0 \\right)}\\);</p> <p>For \\(k=1,2,\\cdots\\):</p> <ul> <li>Solve \\(\\left( \\boldsymbol{A}-\\lambda ^{\\left( k-1 \\right)}\\mathbf{I} \\right) \\boldsymbol{w}=\\boldsymbol{v}^{\\left( k-1 \\right)}\\) for \\(\\boldsymbol{w}\\);</li> <li>\\(\\boldsymbol{v}^{\\left( k \\right)}=\\frac{\\boldsymbol{w}}{\\left\\| \\boldsymbol{w} \\right\\|}\\);</li> <li>\\(\\lambda ^{\\left( k \\right)}=\\left( \\boldsymbol{v}^{\\left( k \\right)} \\right) ^T\\boldsymbol{Av}^{\\left( k \\right)}\\);</li> </ul> <p>End</p> <p><code>Theorem</code>: Rayleigh Quotient Iteration converges to an eigenvalue/eigenvector pair for all except a set of measure zero starting vectors, i.e., \\(\\left( \\boldsymbol{v}^{\\left( 0 \\right)} \\right) ^T\\boldsymbol{q}_J\\ne 0\\). When it converges, the ultimate rate of convergence is cubic:</p> \\[ \\left\\| \\boldsymbol{v}^{\\left( k \\right)}-\\left( \\pm \\boldsymbol{q}_J \\right) \\right\\| =O\\left( \\left\\| \\boldsymbol{v}^{\\left( k-1 \\right)}-\\left( \\pm \\boldsymbol{q}_J \\right) \\right\\| ^3 \\right) ; \\] \\[ \\left\\| \\lambda ^{\\left( k \\right)}-\\lambda _J \\right\\| =O\\left( \\left| \\lambda ^{\\left( k-1 \\right)}-\\lambda _J \\right|^3 \\right)  \\] <p>Remark:</p> <ol> <li>The cubic convergence is super fast. E.g., if \\(\\left\\| \\boldsymbol{v}^{\\left( 0 \\right)}-\\left( \\pm \\boldsymbol{q}_J \\right) \\right\\| =0.1\\), then \\(\\left\\| \\boldsymbol{v}^{\\left( 1 \\right)}-\\left( \\pm \\boldsymbol{q}_J \\right) \\right\\| =O\\left( 0.1^3 \\right) =10^{-3}\\), \\(\\left\\| \\boldsymbol{v}^{\\left( 2 \\right)}-\\left( \\pm \\boldsymbol{q}_J \\right) \\right\\| =10^{-9}\\), \\(\\left\\| \\boldsymbol{v}^{\\left( 3 \\right)}-\\left( \\pm \\boldsymbol{q}_J \\right) \\right\\| =10^{-27}\\).</li> <li>It reduces the number of iterations, but since we have to solve the linear system for each iteration, each iteration has much higher cost. Rayleigh Quotient Iteration is less frequently used by Inverse Iteration in practice.</li> </ol>"},{"location":"Eigenvalue/Divide_and_Conquer/","title":"Divide and Conquer","text":"<p>Assume \\(\\boldsymbol{A}\\in \\mathbb{R} ^{m\\times m}\\) is SPD. After phase 1, \\(\\boldsymbol{T}=\\left( \\hat{\\boldsymbol{Q}}^{\\left( 0 \\right)} \\right) ^T\\boldsymbol{A}\\hat{\\boldsymbol{Q}}^{\\left( 0 \\right)}\\) where \\(\\boldsymbol{T}\\) is tridiagonal ( \\(\\boldsymbol{T}\\) is also SPD ).</p> <p><code>Main Idea</code>: Split the matrix into smaller matrices in size, and find the eigenvalues in a recursive manner (surgical strategy).</p> <p>Let:</p> \\[ \\boldsymbol{T}=\\left[ \\begin{matrix}     \\boldsymbol{T}_1&amp;       &amp;       &amp;       \\\\     &amp;       &amp;       \\beta&amp;      \\\\     &amp;       \\beta&amp;      &amp;       \\\\     &amp;       &amp;       &amp;       \\boldsymbol{T}_2\\\\ \\end{matrix} \\right]  \\] <p>where \\(\\boldsymbol{T}_1\\in \\mathbb{R} ^{n\\times n},\\boldsymbol{T}_2\\in \\mathbb{R} ^{\\left( m-n \\right) \\times \\left( m-n \\right)}\\). Usually select \\(n\\sim \\frac{m}{2}\\). We either take \\(n=\\lfloor \\frac{m}{2} \\rfloor\\) or \\(n=\\lceil \\frac{m}{2} \\rceil\\).</p> <p>Let \\(\\beta =t_{n+1,n}=t_{n,n+1}\\). We can rewrite \\(\\boldsymbol{T}\\) as:</p> \\[ \\boldsymbol{T}=\\left[ \\begin{matrix}     \\hat{\\boldsymbol{T}}_1&amp;     \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\hat{\\boldsymbol{T}}_2\\\\ \\end{matrix} \\right] +\\left[ \\begin{matrix}     \\boldsymbol{O}&amp;     &amp;       &amp;       \\boldsymbol{O}\\\\     &amp;       \\beta&amp;      \\beta&amp;      \\\\     &amp;       \\beta&amp;      \\beta&amp;      \\\\     \\boldsymbol{O}&amp;     &amp;       &amp;       \\boldsymbol{O}\\\\ \\end{matrix} \\right]  \\] \\[ \\left( \\hat{\\boldsymbol{T}}_1 \\right) _{nn}=\\left( \\boldsymbol{T}_1 \\right) _{nn}-\\beta ; \\\\ \\left( \\hat{\\boldsymbol{T}}_2 \\right) _{11}=\\left( \\boldsymbol{T}_2 \\right) _{11}-\\beta  \\] <p>We can get:</p> \\[ \\boldsymbol{T}=\\hat{\\boldsymbol{T}}+\\left[ \\begin{array}{c}     0\\\\     \\vdots\\\\     0\\\\     \\sqrt{\\beta}\\\\     \\sqrt{\\beta}\\\\     0\\\\     \\vdots\\\\     0\\\\ \\end{array} \\right] \\cdot \\left[ \\begin{matrix}     0&amp;      \\cdots&amp;     0&amp;      \\sqrt{\\beta}&amp;       \\sqrt{\\beta}&amp;       0&amp;      \\cdots&amp;     0\\\\ \\end{matrix} \\right] \\triangleq \\hat{\\boldsymbol{T}}+\\boldsymbol{z}\\cdot \\boldsymbol{z}^T \\] <p>Therefore: \\(\\boldsymbol{T}=\\hat{\\boldsymbol{T}}+\\boldsymbol{z}\\cdot \\boldsymbol{z}^T\\).</p> <p>Question: Assume that we know the eigenvalues of \\(\\hat{\\boldsymbol{T}}_1\\) and \\(\\hat{\\boldsymbol{T}}_2\\), how to find the eigenvalues of \\(\\boldsymbol{T}\\) ?</p> <p>Denote \\(\\hat{\\boldsymbol{T}}_1=\\boldsymbol{Q}_1\\boldsymbol{D}_1{\\boldsymbol{Q}_1}^T\\) as the eigenvalue decomposition for \\(\\hat{\\boldsymbol{T}}_1\\). \\(\\boldsymbol{Q}_1\\) contains the eigenvectors of \\(\\hat{\\boldsymbol{T}}_1\\). \\(\\boldsymbol{D}_1\\) is a diagonal matrix whose diagonal elements are the eigenvalues. Also \\(\\hat{\\boldsymbol{T}}_2=\\boldsymbol{Q}_2\\boldsymbol{D}_2{\\boldsymbol{Q}_2}^T\\).</p> <p>Let us denote the last row of \\(\\boldsymbol{Q}_1\\) as \\({\\boldsymbol{q}_1}^T\\), and the first row of \\(\\boldsymbol{Q}_2\\) as \\({\\boldsymbol{q}_2}^T\\). Also denote \\(\\boldsymbol{v}=\\left[ {\\boldsymbol{q}_1}^T,{\\boldsymbol{q}_2}^T \\right] ^T\\).</p> <p><code>Theorem</code>: \\(\\boldsymbol{T}\\) is similar to \\(\\left[ \\begin{matrix}     \\boldsymbol{D}_1&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\boldsymbol{D}_2\\\\ \\end{matrix} \\right] +\\beta \\boldsymbol{vv}^T\\).</p> <p><code>Proof</code>: Define \\(\\boldsymbol{Q}=\\left[ \\begin{matrix}     \\boldsymbol{Q}_1&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\boldsymbol{Q}_2\\\\ \\end{matrix} \\right]\\)</p> \\[ \\boldsymbol{Q}\\left( \\left[ \\begin{matrix}     \\boldsymbol{D}_1&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\boldsymbol{D}_2\\\\ \\end{matrix} \\right] +\\beta \\boldsymbol{vv}^T \\right) \\boldsymbol{Q}^T \\] \\[ =\\left[ \\begin{matrix}     \\boldsymbol{Q}_1\\boldsymbol{D}_1{\\boldsymbol{Q}_1}^T&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\boldsymbol{Q}_2\\boldsymbol{D}_2{\\boldsymbol{Q}_2}^T\\\\ \\end{matrix} \\right] +\\beta \\left[ \\begin{matrix}     \\boldsymbol{Q}_1&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\boldsymbol{Q}_2\\\\ \\end{matrix} \\right] \\left[ \\begin{array}{c}     \\boldsymbol{q}_1\\\\     \\boldsymbol{q}_2\\\\ \\end{array} \\right] \\left[ \\begin{matrix}     {\\boldsymbol{q}_1}^T&amp;       {\\boldsymbol{q}_2}^T\\\\ \\end{matrix} \\right] \\left[ \\begin{matrix}     {\\boldsymbol{Q}_1}^T&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     {\\boldsymbol{Q}_2}^T\\\\ \\end{matrix} \\right]  \\] \\[ =\\left[ \\begin{matrix}     \\hat{\\boldsymbol{T}}_1&amp;     \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\hat{\\boldsymbol{T}}_2\\\\ \\end{matrix} \\right] +\\beta \\left[ \\begin{array}{c}     \\boldsymbol{Q}_1\\boldsymbol{q}_1\\\\     \\boldsymbol{Q}_2\\boldsymbol{q}_2\\\\ \\end{array} \\right] \\left[ \\begin{matrix}     {\\boldsymbol{q}_1}^T{\\boldsymbol{Q}_1}^T&amp;       {\\boldsymbol{q}_2}^T{\\boldsymbol{Q}_2}^T\\\\ \\end{matrix} \\right]  \\] \\[ =\\left[ \\begin{matrix}     \\hat{\\boldsymbol{T}}_1&amp;     \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\hat{\\boldsymbol{T}}_2\\\\ \\end{matrix} \\right] +\\beta \\left[ \\begin{array}{c}     0\\\\     \\vdots\\\\     0\\\\     1\\\\     1\\\\     0\\\\     \\vdots\\\\     0\\\\ \\end{array} \\right] \\cdot \\left[ \\begin{matrix}     0&amp;      \\cdots&amp;     0&amp;      1&amp;      1&amp;      0&amp;      \\cdots&amp;     0\\\\ \\end{matrix} \\right] =\\left[ \\begin{matrix}     \\hat{\\boldsymbol{T}}_1&amp;     \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\hat{\\boldsymbol{T}}_2\\\\ \\end{matrix} \\right] +\\boldsymbol{zz}^T \\] <p>which is \\(\\boldsymbol{T}\\).</p> <p>Question: What are the eigenvalues for \\(\\left[ \\begin{matrix}     \\boldsymbol{D}_1&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\boldsymbol{D}_2\\\\ \\end{matrix} \\right] +\\beta \\boldsymbol{vv}^T\\) ?</p> <p>Define \\(\\boldsymbol{w}=\\sqrt{\\beta}\\boldsymbol{v}\\), then \\(\\beta \\boldsymbol{vv}^T=\\boldsymbol{ww}^T\\). What are the eigenvalues for \\(\\left[ \\begin{matrix}     \\boldsymbol{D}_1&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\boldsymbol{D}_2\\\\ \\end{matrix} \\right] +\\boldsymbol{ww}^T\\) ?</p> <p><code>Claim</code>: \\(\\boldsymbol{w}\\ne 0\\). This is because \\(\\boldsymbol{v}=\\left[ \\begin{array}{c}     \\boldsymbol{q}_1\\\\     \\boldsymbol{q}_2\\\\ \\end{array} \\right]\\) where \\(\\boldsymbol{q}_1\\) and \\(\\boldsymbol{q}_2\\) are eigenvectors corresponding to \\(\\hat{\\boldsymbol{T}}_1,\\hat{\\boldsymbol{T}}_2\\).</p> <p>We can further assume that \\(w_i\\ne 0,\\left( \\forall i \\right)\\). If not, \\(\\boldsymbol{w}=\\left[ \\begin{array}{c}     w_1\\\\     \\vdots\\\\     w_{i-1}\\\\     0\\\\     w_{i+1}\\\\     \\vdots\\\\     w_m\\\\ \\end{array} \\right]\\) and we can break the problem into two smaller problems (in size) and continue the procedure.</p> <p>Let \\(\\left[ \\begin{matrix}     \\boldsymbol{D}_1&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\boldsymbol{D}_2\\\\ \\end{matrix} \\right] \\triangleq \\boldsymbol{D}\\) for simplicity.</p> <p><code>Theorem</code> (important): The eigenvalues of \\(\\boldsymbol{D}+\\boldsymbol{ww}^T\\) are the roots of the following rational function:</p> \\[ f\\left( \\lambda \\right) =1+\\boldsymbol{w}^T\\left( \\boldsymbol{D}-\\lambda \\mathbf{I} \\right) ^{-1}\\boldsymbol{w}, \\\\ \\boldsymbol{D}=\\mathrm{diag}\\left( d_1,d_2,\\cdots ,d_m \\right) ; \\] \\[ f\\left( \\lambda \\right) =1+\\sum_{j=1}^m{\\frac{{w_j}^2}{d_j-\\lambda}} \\] <p><code>Proof</code>: If \\(\\left( \\boldsymbol{D}+\\boldsymbol{ww}^T \\right) \\boldsymbol{q}=\\lambda \\boldsymbol{q}\\) for \\(\\boldsymbol{q}\\ne 0\\), we want to show that \\(f(\\lambda )=0\\).</p> <p>Rewrite the equation as \\(\\left( \\boldsymbol{D}-\\lambda \\mathbf{I} \\right) \\boldsymbol{q}+\\boldsymbol{w}\\left( \\boldsymbol{w}^T\\boldsymbol{q} \\right) =0\\). Assume \\(\\boldsymbol{D}-\\lambda \\mathbf{I}\\) is invertible (Why can we assume it is invertible?), then:</p> \\[ \\boldsymbol{q}+\\left( \\boldsymbol{D}-\\lambda \\mathbf{I} \\right) ^{-1}\\boldsymbol{w}\\left( \\boldsymbol{w}^T\\boldsymbol{q} \\right) =0; \\] \\[ \\boldsymbol{w}^T\\boldsymbol{q}+\\boldsymbol{w}^T\\left( \\boldsymbol{D}-\\lambda \\mathbf{I} \\right) ^{-1}\\boldsymbol{w}\\left( \\boldsymbol{w}^T\\boldsymbol{q} \\right) =0 \\] <p>Since \\(\\boldsymbol{w}^T\\boldsymbol{q}\\) is a scalar, then:</p> \\[ \\left( 1+\\boldsymbol{w}^T\\left( \\boldsymbol{D}-\\lambda \\mathbf{I} \\right) ^{-1}\\boldsymbol{w} \\right) \\cdot \\left( \\boldsymbol{w}^T\\boldsymbol{q} \\right) =0; \\\\ \\Longrightarrow f\\left( \\lambda \\right) \\cdot \\left( \\boldsymbol{w}^T\\boldsymbol{q} \\right) =0 \\] <p>If \\(\\boldsymbol{w}^T\\boldsymbol{q} \\ne 0\\), then we must have \\(f(\\lambda )=0\\); If \\(\\boldsymbol{w}^T\\boldsymbol{q}=0\\), then:</p> \\[ \\lambda \\boldsymbol{q}=\\left( \\boldsymbol{D}+\\boldsymbol{ww}^T \\right) \\boldsymbol{q}=\\boldsymbol{Dq}+\\boldsymbol{w}\\left( \\boldsymbol{w}^T\\boldsymbol{q} \\right) =\\boldsymbol{Dq} \\] <p>This means that \\(\\boldsymbol{q}\\) is an eigenvector of \\(\\boldsymbol{D}\\). \\(\\boldsymbol{D}\\) is diagonal, then \\(\\boldsymbol{q}\\) must be \\(\\boldsymbol{e}_i,1\\leqslant i\\leqslant m\\). \\(0=\\boldsymbol{w}^T\\boldsymbol{q}=\\boldsymbol{w}^T\\boldsymbol{e}_i=w_i\\) and we get the contradiction. This shows that \\(\\boldsymbol{w}^T\\boldsymbol{q}\\ne 0\\Longrightarrow f\\left( \\lambda \\right) =0\\).</p> <p>The converse is also true: If \\(\\lambda\\) is a root of \\(f(\\lambda )=0\\), then \\(\\lambda\\) is an eigenvalue of \\(\\boldsymbol{D}+\\boldsymbol{ww}^T\\).</p> <p>The properties of \\(f(\\lambda )\\): The roots of \\(f(\\lambda )=0\\) can be computed easily by many iterative methods, such as Newton's method because they are well separated and have no flat region.</p> <p></p> <p>We always have root \\({\\lambda}_i\\) in \\((d_i ,d_{i+1})\\):</p> \\[ {\\lambda _i}^{\\left( 0 \\right)}=\\frac{1}{2}\\left( d_i+d_{i+1} \\right) ; \\\\ {\\lambda _i}^{\\left( k+1 \\right)}={\\lambda _i}^{\\left( k \\right)}-\\frac{f\\left( {\\lambda _i}^{\\left( k \\right)} \\right)}{f\\prime\\left( {\\lambda _i}^{\\left( k \\right)} \\right)}; \\] \\[ k\\rightarrow +\\infty : {\\lambda _i}^{\\left( k \\right)}\\rightarrow \\lambda _i \\]"},{"location":"Eigenvalue/Introduction/","title":"Introduction to Eigenvalue Problem","text":"<p><code>Overall Task</code>: Given</p> \\[ \\boldsymbol{Ax}=\\lambda \\boldsymbol{x} \\] <p>We need to find \\(\\lambda\\) and \\(\\boldsymbol{x}\\).</p> <p>What are the methods to compute eigenvalues and eigenvectors?</p>"},{"location":"Eigenvalue/Introduction/#traditional-methods","title":"Traditional Methods","text":""},{"location":"Eigenvalue/Introduction/#traditional-methods-introduction","title":"Traditional Methods Introduction","text":"<p>Traditional Method:</p> <ul> <li>Step 1: Calculate the character polynomial \\(p_{\\boldsymbol{A}}\\left( z \\right) =\\det \\left( z\\boldsymbol{I}-\\boldsymbol{A} \\right)\\);</li> <li>Step 2: Find the roots of \\(p_{\\boldsymbol{A}}\\left( z \\right)\\): \\(\\lambda _i\\). They are eigenvalues;</li> <li>Step 3: Solve \\(\\boldsymbol{Ax}=\\lambda _i\\boldsymbol{x}\\) for \\(\\boldsymbol{x}\\). \\(\\boldsymbol{x}\\) is the eigenvalue corresponding to \\(\\lambda _i\\).</li> </ul>"},{"location":"Eigenvalue/Introduction/#problem-of-traditional-methods","title":"Problem of Traditional Methods","text":"<p><code>Example</code>: Let</p> \\[ \\boldsymbol{A}=\\left[ \\begin{matrix}     1&amp;      1\\\\     0&amp;      1\\\\ \\end{matrix} \\right]  \\] <p>Then \\(\\lambda _{1,2}=1\\), \\(p_{\\boldsymbol{A}}\\left( z \\right) =\\left( z-1 \\right) ^2=z^2-2z+1\\). The coefficients encodes in the computer are \\(\\left[ \\begin{matrix}     1&amp;      -2&amp;     1\\\\ \\end{matrix} \\right]\\). Assume that the input is \\(\\left[ \\begin{matrix}     1&amp;      -2&amp;     1-10^{-16}\\\\ \\end{matrix} \\right]\\). Then the polynomial becomes \\(z^2-2z+\\left( 1-10^{-16} \\right) =\\left( z-\\left( 1-10^{-8} \\right) \\right) \\cdot \\left( z-\\left( 1+10^{-8} \\right) \\right) =0\\). We get \\(\\lambda _1=1-10^{-8}, \\lambda _2=1+10^{-8}\\). Therefore, when the input error is \\(10^{-16}\\), the output error is \\(O(10^{-8})\\). The root finding procedure amplifies the error!</p> <p>Root finding is not stable, and algorithms using root finding procedures are not a stable algorithms.</p>"},{"location":"Eigenvalue/Introduction/#root-finding-vs-eigenvalue-problems","title":"Root Finding V.S. Eigenvalue Problems","text":""},{"location":"Eigenvalue/Introduction/#comparison","title":"Comparison","text":"<p>Actually, root finding in computers is implemented as eigenvalue solving algorithms.</p> <p><code>Claim</code>: Eigenvalue problem is equivalent to root finding for polynomials.</p> <p>Given \\(p\\left( z \\right) =z^m+a_{m-1}z^{m-1}+\\cdots +a_1z+a_0\\). Introduce a matrix:</p> \\[ \\boldsymbol{A}=\\left[ \\begin{matrix}     0&amp;      &amp;       &amp;       O&amp;      -a_0\\\\     1&amp;      \\ddots&amp;     &amp;       &amp;       -a_1\\\\     &amp;       \\ddots&amp;     \\ddots&amp;     &amp;       \\\\     &amp;       &amp;       \\ddots&amp;     0&amp;      -a_{m-2}\\\\     O&amp;      &amp;       &amp;       1&amp;      -a_{m-1}\\\\ \\end{matrix} \\right]  \\] <p>\\(\\boldsymbol{A}\\in \\mathbb{R} ^{m\\times m}\\) is called the companion matrix for \\(p(z)\\). We can prove that:</p> \\[ p_{\\boldsymbol{A}}\\left( z \\right) =\\det \\left( z\\boldsymbol{I}-\\boldsymbol{A} \\right) =p\\left( z \\right)  \\] <p>Then getting the roots for the original polynomial is euqivalent to getting the eigenvalues for the corresponding companion matrix.</p>"},{"location":"Eigenvalue/Introduction/#difficulty","title":"Difficulty","text":"<p>There is fundamental difficulty in the eigenvalue computation: no explicit formula for a general matrix \\(\\boldsymbol{A}_{m\\times m}\\) when \\(m\\geqslant 5\\).</p> <p><code>Theorem</code>: For any \\(m\\geqslant 5\\), there is a polynomial \\(p(z)\\) of degree \\(m\\) with rational coefficients that has a real root \\(p(r)=0\\) with the property that \\(r\\) can NOT be written using any expression involving rational number additions, subtractions, multiplications, divisions or \\(k\\) -th roots.</p> <p>All the methods for eigenvalues must be iterative!</p> <p>Even if working with exact arithmetic, there could be no computing program that would produce the exact roots of an arbitrary polynomial of degree \\(m\\geqslant 5\\) in a finite number of steps. All eigenvalue methods must be iterative.</p>"},{"location":"Eigenvalue/Introduction/#numerical-methods","title":"Numerical Methods","text":"<p>We want to convert the original matrix to a matrix that is easy to recognize the eigenvalues (upper/lower triangular or diagonal), and the transformation should not change the eigenvalues of matrices (this is the concept of similar).</p> <p>Commonly used numerical methods for eigenvalue problem have two phases:</p>"},{"location":"Eigenvalue/Introduction/#phase-one","title":"Phase One","text":"<p>Phase 1: A direct method to produce an upper Hessenberg matrix \\(\\boldsymbol{H}\\) (an upper Hessenberg matrix has zero entries below the first subdiagonal, and a lower Hessenberg matrix has zero entries above the first superdiagonal):</p> \\[ \\boldsymbol{H}=\\left[ \\begin{matrix}     a_{11}&amp;     a_{12}&amp;     a_{13}&amp;     a_{14}&amp;     \\cdots&amp;     \\cdots&amp;     a_{1\\left( n-1 \\right)}&amp;        a_{1n}\\\\     a_{21}&amp;     a_{22}&amp;     a_{23}&amp;     a_{24}&amp;     \\ddots&amp;     \\cdots&amp;     a_{2\\left( n-1 \\right)}&amp;        a_{2n}\\\\     0&amp;      a_{32}&amp;     a_{33}&amp;     a_{34}&amp;     \\ddots&amp;     \\cdots&amp;     a_{3\\left( n-1 \\right)}&amp;        a_{3n}\\\\     0&amp;      0&amp;      a_{43}&amp;     a_{44}&amp;     \\ddots&amp;     \\cdots&amp;     a_{4\\left( n-1 \\right)}&amp;        a_{4n}\\\\     0&amp;      0&amp;      0&amp;      a_{54}&amp;     \\ddots&amp;     \\ddots&amp;     a_{5\\left( n-1 \\right)}&amp;        a_{5n}\\\\     \\vdots&amp;     \\vdots&amp;     \\vdots&amp;     0&amp;      \\ddots&amp;     \\vdots&amp;     \\vdots&amp;     \\vdots\\\\     0&amp;      0&amp;      0&amp;      0&amp;      \\ddots&amp;     a_{\\left( n-1 \\right) \\left( n-2 \\right)}&amp;      a_{\\left( n-1 \\right) \\left( n-1 \\right)}&amp;      a_{\\left( n-1 \\right) n}\\\\     0&amp;      0&amp;      0&amp;      0&amp;      \\cdots&amp;     0&amp;      a_{n\\left( n-1 \\right)}&amp;        a_{nn}\\\\ \\end{matrix} \\right]  \\] <p>The procedure can end within finite number of steps. This is done by similar transforms. In linear algebra, \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{B}\\) are similar ( \\(\\boldsymbol{A}\\sim \\boldsymbol{B}\\) ), if there exists \\(\\boldsymbol{X}\\in \\mathbb{R} ^{m\\times m}\\) invertible such that \\(\\boldsymbol{XAX}^{-1}=\\boldsymbol{B}\\). We can let:</p> \\[ 0=\\det \\left( \\lambda \\boldsymbol{I}-\\boldsymbol{B} \\right) =\\det \\left( \\lambda \\boldsymbol{I}-\\boldsymbol{XAX}^{-1} \\right) =\\det \\left( \\boldsymbol{X}\\left( \\lambda \\boldsymbol{I}-\\boldsymbol{A} \\right) \\boldsymbol{X}^{-1} \\right) =\\det \\left( \\boldsymbol{X} \\right) \\cdot \\det \\left( \\lambda \\boldsymbol{I}-\\boldsymbol{A} \\right) \\cdot \\det \\left( \\boldsymbol{X}^{-1} \\right) \\] <p>Because \\(\\det \\left( \\boldsymbol{X} \\right) \\ne 0, \\det \\left( \\boldsymbol{X}^{-1} \\right) \\ne 0\\), we know that the characteristic polynomials of \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{B}\\) are the same, and \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{B}\\) have the same set of eigenvalues.</p>"},{"location":"Eigenvalue/Introduction/#phase-two","title":"Phase Two","text":"<p>Phase 2: An iterative procedure that is also based on similarity transforms to produce a formally infinite sequence of upper Hessenberg matrices that converges to a triangular form.</p> <p>Why we divide the algorithms into two phases? The cost concern is the reason for the two-phase strategy. Phase 1 has \\(O(m^3)\\) flops, while in Phase 2, it normally converges to \\(O\\left( \\varepsilon _{machine} \\right)\\) within \\(O(m)\\) steps, each step requiring at most \\(O(m^2)\\) steps to finish.</p> <p>Building blocks: QR Factorization.</p>"},{"location":"Eigenvalue/QR_Algorithm/","title":"QR Algorithm","text":"<p>QR Algorithm is the recommended algorithm for eigenvalue and eigenvector computation.</p> <p>The matrix \\(\\boldsymbol{A}\\) below has been computed after phase 1.</p>"},{"location":"Eigenvalue/QR_Algorithm/#pure-qr-algorithm","title":"\"Pure\" QR Algorithm","text":"<p>Pure QR Algorithm:</p> <p>Initialize: \\(\\boldsymbol{A}^{\\left( 0 \\right)}=\\boldsymbol{A}\\);</p> <p>For \\(k=1,2,\\cdots\\):</p> <ul> <li>Do QR factorization: \\(\\boldsymbol{Q}^{\\left( k \\right)}\\cdot \\boldsymbol{R}^{\\left( k \\right)}=\\boldsymbol{A}^{\\left( k-1 \\right)}\\);</li> <li>\\(\\boldsymbol{A}^{\\left( k \\right)}=\\boldsymbol{R}^{\\left( k \\right)}\\cdot \\boldsymbol{Q}^{\\left( k \\right)}\\);</li> </ul> <p>End</p>"},{"location":"Eigenvalue/QR_Algorithm/#similarity-transformation","title":"Similarity Transformation","text":"<p>Task 1: Verify that QR algorithm performs similarity transformation, that is, \\(\\boldsymbol{A}^{\\left( k \\right)}\\sim \\boldsymbol{A}^{\\left( k-1 \\right)}\\).</p> <ul> <li>Step 1: \\(\\boldsymbol{A}^{\\left( k-1 \\right)}=\\boldsymbol{Q}^{\\left( k \\right)}\\cdot \\boldsymbol{R}^{\\left( k \\right)}\\Leftrightarrow \\left( \\boldsymbol{Q}^{\\left( k \\right)} \\right) ^T\\boldsymbol{A}^{\\left( k-1 \\right)}=\\boldsymbol{R}^{\\left( k \\right)}\\)</li> <li>Step 2: \\(\\boldsymbol{A}^{\\left( k \\right)}=\\boldsymbol{R}^{\\left( k \\right)}\\cdot \\boldsymbol{Q}^{\\left( k \\right)}=\\left( \\boldsymbol{Q}^{\\left( k \\right)} \\right) ^T\\boldsymbol{A}^{\\left( k-1 \\right)}\\boldsymbol{Q}^{\\left( k \\right)}\\Rightarrow \\boldsymbol{A}^{\\left( k \\right)}\\sim \\boldsymbol{A}^{\\left( k-1 \\right)}\\). Then we can get:</li> </ul> \\[ \\boldsymbol{A}^{\\left( k \\right)}=\\left( \\boldsymbol{Q}^{\\left( k \\right)} \\right) ^T\\boldsymbol{A}^{\\left( k-1 \\right)}\\boldsymbol{Q}^{\\left( k \\right)}=\\left( \\boldsymbol{Q}^{\\left( k \\right)} \\right) ^T\\left( \\boldsymbol{Q}^{\\left( k-1 \\right)} \\right) ^T\\boldsymbol{A}^{\\left( k-2 \\right)}\\boldsymbol{Q}^{\\left( k \\right)}=\\cdots  \\] \\[ =\\underset{\\left( \\bar{\\boldsymbol{Q}}^{\\left( k \\right)} \\right) ^T}{\\underbrace{\\left( \\boldsymbol{Q}^{\\left( k \\right)} \\right) ^T\\left( \\boldsymbol{Q}^{\\left( k-1 \\right)} \\right) ^T\\cdots \\left( \\boldsymbol{Q}^{\\left( 1 \\right)} \\right) ^T}}\\cdot \\boldsymbol{A}^{\\left( 0 \\right)}\\cdot \\underset{\\bar{\\boldsymbol{Q}}^{\\left( k \\right)}}{\\underbrace{\\boldsymbol{Q}^{\\left( 1 \\right)}\\cdots \\boldsymbol{Q}^{\\left( k-1 \\right)}\\boldsymbol{Q}^{\\left( k \\right)}}} \\\\ =\\left( \\bar{\\boldsymbol{Q}}^{\\left( k \\right)} \\right) ^T\\boldsymbol{A}\\bar{\\boldsymbol{Q}}^{\\left( k \\right)} \\] <p>If \\(\\boldsymbol{A}^{\\left( k \\right)}\\) converges to \\(\\mathrm{diag}\\left( \\lambda _1,\\cdots ,\\lambda _n \\right)\\) as \\(k\\rightarrow +\\infty\\), then \\(\\bar{\\boldsymbol{Q}}^{\\left( k \\right)}\\rightarrow \\boldsymbol{Q}\\) which contains the eigenvectors of \\(\\boldsymbol{A}\\).</p>"},{"location":"Eigenvalue/QR_Algorithm/#about-phase-one","title":"About Phase One","text":"<p>Task 2: Why do we need phase 1 before the algorithm performs?</p> <p>Let us assume \\(\\boldsymbol{A}\\) is symmetric for simplicity, then \\(\\boldsymbol{H}=\\boldsymbol{QAQ}^T\\) is tridiagonal. Starting from \\(\\boldsymbol{H}\\), the cost for each iteration is of \\(O(m)\\) (using Given Rotation, for example).</p> <p>If we don't perform phase 1, starting from \\(\\boldsymbol{A}\\), the cost for each iteration is \\(O(m^3)\\) (using Householder QR factorization, for example).</p> <p>Question: What is the cost per iteration if \\(\\boldsymbol{H}\\) is and upper Hessenberg matrix? (HW Practice)</p>"},{"location":"Eigenvalue/QR_Algorithm/#convergence","title":"Convergence","text":"<p>Task 3: QR algorithm is convergent.</p> <p><code>Theorem</code>: Let the pure QR algorithm be applied to a real symmetric matrix \\(\\boldsymbol{A}\\) whose eigenvalues satisfy \\(\\left| \\lambda _1 \\right|&gt;\\left| \\lambda _2 \\right|&gt;\\cdots &gt;\\left| \\lambda _m \\right|\\), and the corresponding eigenmatrix \\(\\boldsymbol{Q}\\) has all nonsingular leading principal submatrices. Then as \\(k\\rightarrow +\\infty\\), \\(\\boldsymbol{A}^{\\left( k \\right)}\\) converges linearly with a constant rate given by \\(\\max_j \\frac{\\left| \\lambda _{j+1} \\right|}{\\left| \\lambda _j \\right|}\\) to diagonal matrix \\(\\mathrm{diag}\\left( \\lambda _1,\\lambda _2,\\cdots ,\\lambda _n \\right)\\), and \\(\\bar{\\boldsymbol{Q}}^{\\left( k \\right)}\\) (with signs of its columns adjusted as necessary) converges to \\(\\boldsymbol{Q}\\) at the same rate.</p> <p>In the theorem above, \\(\\boldsymbol{A}=\\boldsymbol{Q}^T\\mathbf{\\Lambda }\\boldsymbol{Q}\\) where \\(\\boldsymbol{Q}\\in \\mathbb{C} ^{m\\times m}\\) is unitary.</p> <p>The sketch of the proof (in three steps):</p>"},{"location":"Eigenvalue/QR_Algorithm/#step-one","title":"Step One","text":"<p>Step 1: QR algorithm is essentially equivalent to the so-called simultaneous iteration.</p> <p>Simultaneous Iteration:</p> <p>Pick \\(\\hat{\\boldsymbol{Q}}^{\\left( 0 \\right)}\\in \\mathbb{R} ^{m\\times n}\\) with orthonormal columns;</p> <p>For \\(k=1,2,\\cdots\\):</p> <ul> <li>\\(\\boldsymbol{Z}^{\\left( k \\right)}=\\boldsymbol{A}\\hat{\\boldsymbol{Q}}^{\\left( k-1 \\right)}\\);</li> <li>Do QR factorization: \\(\\hat{\\boldsymbol{Q}}^{\\left( k \\right)}\\hat{\\boldsymbol{R}}^{\\left( k \\right)}=\\boldsymbol{Z}^{\\left( k \\right)}\\);</li> </ul> <p>End</p> <p>Claim: If we pick \\(\\hat{\\boldsymbol{Q}}^{\\left( 0 \\right)}=\\mathbf{I}\\), then the simultaneous iteration becomes the QR algorithm. (How to verify?)</p>"},{"location":"Eigenvalue/QR_Algorithm/#step-two","title":"Step Two","text":"<p>Step 2: Simultaneous iteration is a block Power Iteration.</p> <ul> <li>Power Iteration: \\(\\boldsymbol{Z}^{\\left( k \\right)}=\\boldsymbol{A}\\hat{\\boldsymbol{Q}}^{\\left( k-1 \\right)}\\).</li> <li>Normalization and estimating eigenvalues: \\(\\hat{\\boldsymbol{Q}}^{\\left( k \\right)}\\hat{\\boldsymbol{R}}^{\\left( k \\right)}=\\boldsymbol{Z}^{\\left( k \\right)}\\).</li> </ul>"},{"location":"Eigenvalue/QR_Algorithm/#step-three","title":"Step Three","text":"<p>Step 3: The block Power Iteration is convergent with the rate given by</p> \\[ c=\\max_j \\frac{\\left| \\lambda _{j+1} \\right|}{\\left| \\lambda _j \\right|} \\] <p>if \\(\\left| \\lambda _1 \\right|&gt;\\left| \\lambda _2 \\right|&gt;\\cdots &gt;\\left| \\lambda _j \\right|&gt;\\left| \\lambda _{j+1} \\right|&gt;\\cdots &gt;\\left| \\lambda _m \\right|\\geqslant 0\\). Then we get \\(0&lt;c&lt;1\\).</p> <p>A special case for block Power Iteration to show the convergence (this is the essential idea):</p> <p>Consider \\(\\left[ \\begin{matrix}     {\\boldsymbol{v}_1}^{\\left( 0 \\right)}&amp;      {\\boldsymbol{v}_{\\left( 2 \\right)}}^{\\left( 0 \\right)}\\\\ \\end{matrix} \\right]\\) where \\({\\boldsymbol{v}_1}^{\\left( 0 \\right)}\\) and \\({\\boldsymbol{v}_{\\left( 2 \\right)}}^{\\left( 0 \\right)}\\) are unit vectors and orthogonal to each other. Then:</p> \\[ \\boldsymbol{A}\\left[ \\begin{matrix}     {\\boldsymbol{v}_1}^{\\left( 0 \\right)}&amp;      {\\boldsymbol{v}_{\\left( 2 \\right)}}^{\\left( 0 \\right)}\\\\ \\end{matrix} \\right] =\\left[ \\begin{matrix}     {\\boldsymbol{Av}_1}^{\\left( 0 \\right)}&amp;     {\\boldsymbol{Av}_{\\left( 2 \\right)}}^{\\left( 0 \\right)}\\\\ \\end{matrix} \\right] =\\left[ \\begin{matrix}     \\boldsymbol{w}_1&amp;       \\boldsymbol{w}_2\\\\ \\end{matrix} \\right]  \\] <p>Also:</p> \\[ \\boldsymbol{QR}=\\left[ \\begin{matrix}     \\boldsymbol{w}_1&amp;       \\boldsymbol{w}_2\\\\ \\end{matrix} \\right] ; \\\\ \\boldsymbol{Q}=\\left[ \\begin{matrix}     {\\boldsymbol{v}_1}^{\\left( 1 \\right)}&amp;      {\\boldsymbol{v}_{\\left( 2 \\right)}}^{\\left( 1 \\right)}\\\\ \\end{matrix} \\right] ;\\cdots  \\] <p>\\(\\boldsymbol{Q}\\) spans the space generated by the eigenvectors \\(\\boldsymbol{q}_1, \\boldsymbol{q}_2\\). We can further prove that \\({\\boldsymbol{v}_1}^{\\left( k \\right)}\\rightarrow \\boldsymbol{q}_1, {\\boldsymbol{v}_2}^{\\left( k \\right)}\\rightarrow \\boldsymbol{q}_2\\) as \\(k\\rightarrow +\\infty\\).</p>"},{"location":"Eigenvalue/QR_Algorithm/#practical-qr-algorithm","title":"\"Practical\" QR Algorithm","text":""},{"location":"Eigenvalue/QR_Algorithm/#algorithm-steps","title":"Algorithm Steps","text":"<p>We assume \\(\\boldsymbol{A}\\) is SPD below.</p> <p>Phase 1 is the same as before: \\(\\left( \\boldsymbol{Q}^{\\left( 0 \\right)} \\right) ^T\\boldsymbol{A}^{\\left( 0 \\right)}\\boldsymbol{Q}^{\\left( 0 \\right)}=\\boldsymbol{A}\\) where \\(\\boldsymbol{A}^{\\left( 0 \\right)}\\) is a tridiagonal matrix;</p> <p>For \\(k=1,2,\\cdots\\):</p> <ul> <li>Pick a shift \\(\\mu ^{\\left( k \\right)}\\);</li> <li>Do QR factorization: \\(\\boldsymbol{Q}^{\\left( k \\right)}\\boldsymbol{R}^{\\left( k \\right)}=\\boldsymbol{A}^{\\left( k \\right)}-\\mu ^{\\left( k \\right)}\\mathbf{I}\\);</li> <li>\\(\\boldsymbol{A}^{\\left( k \\right)}=\\boldsymbol{R}^{\\left( k \\right)}\\boldsymbol{Q}^{\\left( k \\right)}+\\mu ^{\\left( k \\right)}\\mathbf{I}\\);</li> <li>If any off-diagonal element \\({\\boldsymbol{A}_{j,j+1}}^{\\left( k \\right)}\\) is sufficiently small (close to zero is absolute value sense), we set \\({\\boldsymbol{A}_{j,j+1}}^{\\left( k \\right)}={\\boldsymbol{A}_{j+1,j}}^{\\left( k \\right)}=0\\) and obtain: \\(\\boldsymbol{A}^{\\left( k \\right)}=\\left[ \\begin{matrix}     \\boldsymbol{A}_1&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\boldsymbol{A}_2\\\\ \\end{matrix} \\right]\\) where \\(\\boldsymbol{A}_1\\) and \\(\\boldsymbol{A}_2\\) are tridiagonal matrices with smaller size;</li> <li>Apply QR algorithm to \\(\\boldsymbol{A}_1\\) and \\(\\boldsymbol{A}_2\\) in a recursive manner;</li> </ul> <p>End</p> <p>Two key modifications:</p> <ol> <li>Divide the big matrix into smaller matrices;</li> <li>Use the shift.</li> </ol>"},{"location":"Eigenvalue/QR_Algorithm/#about-shift","title":"About \"Shift\"","text":"<p>How can we pick the shift?</p>"},{"location":"Eigenvalue/QR_Algorithm/#method-one","title":"Method One","text":"<p>Method 1: we can pick \\(\\mu ^{\\left( k \\right)}={\\boldsymbol{A}_{m,m}}^{\\left( k \\right)}\\) as the last element of \\(\\boldsymbol{A}^k\\).</p> \\[ {\\boldsymbol{A}_{m,m}}^{\\left( k \\right)}={\\boldsymbol{e}_m}^T\\boldsymbol{A}^{\\left( k \\right)}\\boldsymbol{e}_m={\\boldsymbol{e}_m}^T\\left( \\boldsymbol{Q}^{\\left( k \\right)} \\right) ^T\\boldsymbol{AQ}^{\\left( k \\right)}\\boldsymbol{e}_m \\] \\[ =\\left( {\\boldsymbol{q}_m}^{\\left( k \\right)} \\right) ^T{\\boldsymbol{Aq}_m}^{\\left( k \\right)}=\\frac{\\left( {\\boldsymbol{q}_m}^{\\left( k \\right)} \\right) ^T{\\boldsymbol{Aq}_m}^{\\left( k \\right)}}{\\left( {\\boldsymbol{q}_m}^{\\left( k \\right)} \\right) ^T{\\boldsymbol{q}_m}^{\\left( k \\right)}} \\] <p>As \\(k\\rightarrow +\\infty\\), \\(\\boldsymbol{q}^{\\left( k \\right)}\\rightarrow \\boldsymbol{q}_m, \\mu ^{\\left( k \\right)}\\rightarrow \\lambda _m\\)</p> <p>This strategy may fail in some situations. For example, if \\(\\boldsymbol{A}=\\left[ \\begin{matrix}     0&amp;      1\\\\     1&amp;      0\\\\ \\end{matrix} \\right]\\), we get \\(\\lambda _{1,2}=\\pm 1\\). \\(\\boldsymbol{AI}=\\boldsymbol{A},\\boldsymbol{RQ}=\\boldsymbol{A}\\) remains unchanged. The shift is zero.</p>"},{"location":"Eigenvalue/QR_Algorithm/#wilkersons-shift","title":"Wilkerson's Shift","text":"<p>Method 2 (Wilkerson's Shift):</p> <p>Let:</p> \\[ \\boldsymbol{B}=\\left[ \\begin{matrix}     a_{m-1}&amp;        b_{m-1}\\\\     b_{m-1}&amp;        a_m\\\\ \\end{matrix} \\right]  \\] <p>as a lower-right 2 by 2 matrix. Pick the eigenvalue of \\(\\boldsymbol{B}\\), closer to \\(a_m\\), as the shift \\(\\mu ^{\\left( k \\right)}\\), In the case of a tie, just pick and arbitrary one:</p> \\[ \\mu ^{\\left( k \\right)}=a_m-\\mathrm{sign}\\left( \\delta \\right) \\frac{{b_{m-1}}^2}{\\left| \\delta \\right|+\\sqrt{\\delta ^2+{b_{m-1}}^2}} \\] <p>where \\(\\delta =\\frac{a_{m-1}-a_m}{2}\\). If \\(\\delta =0\\), set \\(\\delta =\\pm 1\\) arbitrarily.</p>"},{"location":"Eigenvalue/QR_Algorithm/#properties","title":"Properties","text":"<p><code>Claims</code>:</p> <ul> <li>QR algorithm with Wilkerson's Shift always converges in exact arithmetic.</li> <li>QR algorithm is backward stable.</li> <li>Overall cost of QR algorithm \\(O\\left( \\frac{4}{3}m^3 \\right)\\) flops (mainly the cost of phase 1).</li> </ul>"},{"location":"Eigenvalue/QR_Factorization/","title":"QR Factorization","text":"<p>Definition of QR Factorization:</p> \\[ \\boldsymbol{A}=\\boldsymbol{Q}\\cdot \\boldsymbol{R} \\] <p>\\(\\boldsymbol{Q}\\) is a unitary matrix and \\(\\boldsymbol{R}\\) is an upper triangular matrix.</p>"},{"location":"Eigenvalue/QR_Factorization/#gram-schmidt-algorithm","title":"Gram-Schmidt Algorithm","text":""},{"location":"Eigenvalue/QR_Factorization/#introduction-to-gram-schmidt-process","title":"Introduction to Gram-Schmidt Process","text":"<p>Gram-Schmidt is an algorithm to produce \\(\\boldsymbol{Q}\\) and \\(\\boldsymbol{R}\\).</p> <p>Given \\(\\left[ \\begin{matrix}     \\boldsymbol{a}_1&amp;       \\boldsymbol{a}_2&amp;       \\cdots&amp;     \\boldsymbol{a}_m\\\\ \\end{matrix} \\right]\\). Let \\(\\mathrm{span}\\left\\{ \\boldsymbol{a}_1, \\boldsymbol{a}_2, \\cdots , \\boldsymbol{a}_m \\right\\} \\triangleq \\left&lt; \\boldsymbol{a}_1, \\boldsymbol{a}_2, \\cdots , \\boldsymbol{a}_m \\right&gt;\\). We need to find \\(\\boldsymbol{q}_1, \\boldsymbol{q}_2, \\cdots , \\boldsymbol{q}_m\\) such that \\(\\left&lt; \\boldsymbol{q}_1, \\boldsymbol{q}_2, \\cdots , \\boldsymbol{q}_m \\right&gt; =\\left&lt; \\boldsymbol{a}_1, \\boldsymbol{a}_2, \\cdots , \\boldsymbol{a}_m \\right&gt;\\) and the inner products \\(\\left( \\boldsymbol{q}_i, \\boldsymbol{q}_j \\right) =0\\) if \\(i\\ne j\\), \\(\\left( \\boldsymbol{q}_i, \\boldsymbol{q}_i \\right) =1\\).</p> <p>Example steps (only 2-norm is used below):</p> \\[ \\boldsymbol{q}_1=\\frac{\\boldsymbol{a}_1}{\\left\\| \\boldsymbol{a}_1 \\right\\|}, r_{11}=\\left\\| \\boldsymbol{a}_1 \\right\\| \\Longrightarrow \\boldsymbol{q}_1=\\frac{\\boldsymbol{a}_1}{r_{11}} \\] \\[ r_{12}=\\left( \\boldsymbol{q}_1, \\boldsymbol{a}_2 \\right) , \\boldsymbol{v}_2=\\boldsymbol{a}_2-r_{12}\\cdot \\boldsymbol{q}_1, r_{22}=\\left\\| \\boldsymbol{v}_2 \\right\\|  \\\\ \\Longrightarrow \\boldsymbol{q}_2=\\frac{\\boldsymbol{v}_2}{r_{22}} \\] \\[ r_{13}=\\left( \\boldsymbol{q}_1, \\boldsymbol{a}_3 \\right) , r_{23}=\\left( \\boldsymbol{q}_2, \\boldsymbol{a}_3 \\right) , \\boldsymbol{v}_3=\\boldsymbol{a}_3-r_{13}\\cdot \\boldsymbol{q}_1-r_{23}\\cdot \\boldsymbol{q}_2, r_{33}=\\left\\| \\boldsymbol{v}_3 \\right\\|  \\\\ \\Longrightarrow \\boldsymbol{q}_3=\\frac{\\boldsymbol{v}_3}{r_{33}} \\]"},{"location":"Eigenvalue/QR_Factorization/#classical-gram-schmidt-algorithm","title":"Classical Gram-Schmidt Algorithm","text":"<p>Given \\(\\boldsymbol{A}=\\left[ \\begin{matrix}     \\boldsymbol{a}_1&amp;       \\cdots&amp;     \\boldsymbol{a}_j&amp;       \\cdots&amp;     \\boldsymbol{a}_m\\\\ \\end{matrix} \\right]\\);</p> <p>For \\(j=1:m\\):</p> <ul> <li>\\(\\boldsymbol{v}_j=\\boldsymbol{a}_j\\);</li> <li>For \\(i=1:(j-1)\\):<ul> <li>\\(r_{ij}=\\left( \\boldsymbol{q}_i, \\boldsymbol{a}_j \\right)\\);</li> <li>\\(\\boldsymbol{v}_j=\\boldsymbol{v}_j-r_{ij}\\cdot \\boldsymbol{q}_i\\);</li> </ul> </li> <li>End</li> <li>\\(r_{jj}=\\left\\| \\boldsymbol{v}_j \\right\\|\\);</li> <li>\\(\\boldsymbol{q}_j=\\frac{\\boldsymbol{v}_j}{r_{jj}}\\);</li> </ul> <p>End</p> <p>The outcome: \\(\\boldsymbol{Q}=\\left[ \\begin{matrix}     \\boldsymbol{q}_1&amp;       \\boldsymbol{q}_2&amp;       \\cdots&amp;     \\boldsymbol{q}_m\\\\ \\end{matrix} \\right]\\) unitary, and \\(\\boldsymbol{R}=\\left[ r_{ij} \\right]\\) upper triangular.</p> <p><code>Claim</code>: The classical Gram-Schmidt algorithm above is not stable.</p>"},{"location":"Eigenvalue/QR_Factorization/#modified-gram-schmidt-algorithm","title":"Modified Gram-Schmidt Algorithm","text":"<p>Given \\(\\boldsymbol{A}=\\left[ \\begin{matrix}     \\boldsymbol{a}_1&amp;       \\cdots&amp;     \\boldsymbol{a}_j&amp;       \\cdots&amp;     \\boldsymbol{a}_m\\\\ \\end{matrix} \\right]\\);</p> <p>For \\(i=1:m\\):</p> <ul> <li>\\(\\boldsymbol{v}_i=\\boldsymbol{a}_i\\);</li> </ul> <p>End</p> <p>For \\(i=1:m\\):</p> <ul> <li>\\(r_{ii}=\\left\\| \\boldsymbol{v}_i \\right\\|\\);</li> <li>\\(\\boldsymbol{q}_i=\\frac{\\boldsymbol{v}_i}{r_{ii}}\\);</li> <li>For \\(j=(i+1):m\\):<ul> <li>\\(r_{ij}=\\left( \\boldsymbol{q}_i, \\boldsymbol{v}_j \\right)\\);</li> <li>\\(\\boldsymbol{v}_j=\\boldsymbol{v}_j-r_{ij}\\cdot \\boldsymbol{q}_i\\);</li> </ul> </li> <li>End</li> </ul> <p>End</p> <p><code>Claim</code>: The modified Gram-Schmidt algorithm is stable.</p> <p>Questions:</p> <ol> <li>Why classical and modified Gram-Schmidt are equivalent?</li> <li>Intuitively, why is the classical one not stable, while the modified one stable?</li> </ol> <p>The cost of the modified Gram-Schmidt is \\(O(2mn^2)\\) for \\(\\boldsymbol{A}\\in \\mathbb{R} ^{m\\times n}\\).</p>"},{"location":"Eigenvalue/QR_Factorization/#discussion-on-qr-factorization","title":"Discussion on QR Factorization","text":"<p><code>Theorem</code>: Every matrix \\(\\boldsymbol{A}\\in \\mathbb{R} ^{m\\times n}\\,\\,\\left( m\\geqslant n \\right)\\) has a QR Factorization.</p> <p>If column vectors of \\(\\boldsymbol{A}\\) are linearly independent(full rank), then</p> \\[ \\left[ \\boldsymbol{A} \\right] _{m\\times n}=\\left[ \\boldsymbol{Q} \\right] _{m\\times n}\\cdot \\left[ \\boldsymbol{R} \\right] _{n\\times n} \\] <p>This is Reduced QR Factorization(default). The other one:</p> \\[ \\left[ \\boldsymbol{A} \\right] _{m\\times n}=\\underset{\\boldsymbol{Q}}{\\underbrace{\\left[ {\\boldsymbol{Q}_0}^{m\\times n}|\\cdots \\right] _{m\\times m}}}\\cdot \\underset{\\boldsymbol{R}}{\\underbrace{\\left[ \\begin{array}{c}     {\\boldsymbol{R}_0}^{n\\times n}\\\\     O\\\\ \\end{array} \\right] _{m\\times n}}} \\] <p>is called Full QR Factorization(rarely used).</p> <p><code>Theorem</code>: If \\(\\boldsymbol{A}\\in \\mathbb{R} ^{m\\times n}\\,\\,\\left( m\\geqslant n \\right)\\) is full rank, the Reduced QR Factorization is unique if we require \\(r_{ii}&gt;0, i=1,\\cdots ,n\\).</p> <p>We can use QR Factorization to solve linear systems (least square solution):</p> \\[ \\boldsymbol{A}=\\boldsymbol{QR} \\] \\[ \\boldsymbol{Ax}=\\boldsymbol{b}\\Leftrightarrow \\boldsymbol{QRx}=\\boldsymbol{b}\\Leftrightarrow \\boldsymbol{Rx}=\\boldsymbol{Q}^*\\boldsymbol{b} \\] <p>Matrix explanation of Gram-Schmidt process (Using upper triangular matrices to generate orthogonal matrix):</p> \\[ \\boldsymbol{A}\\cdot \\underset{\\boldsymbol{R}^{-1}}{\\underbrace{\\boldsymbol{R}_1\\boldsymbol{R}_2\\cdots \\boldsymbol{R}_n}}=\\boldsymbol{Q} \\] <p>Two types of orthogonal linear transformation:</p> <ol> <li>Rotation</li> <li>Reflection</li> </ol> <p>Actually, there are other ways to computer QR Factorization using unitary matrices to generate upper triangular matrix (Using unitary transformation):</p> \\[ \\underset{\\boldsymbol{Q}^{-1}}{\\underbrace{\\boldsymbol{Q}_n\\boldsymbol{Q}_{n-1}\\cdots \\boldsymbol{Q}_1}}\\cdot \\boldsymbol{A}=\\boldsymbol{R} \\] <p>where \\(\\boldsymbol{Q}_i\\) are unitary. There are two ways for this process: Householder transformation and Givens rotation.</p>"},{"location":"Eigenvalue/QR_Factorization/#householder-transformation","title":"Householder Transformation","text":""},{"location":"Eigenvalue/QR_Factorization/#basic-steps","title":"Basic Steps","text":"<p>The overall procedure (recursive):</p> \\[ \\boldsymbol{A}=\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\ \\end{matrix} \\right] \\xrightarrow{\\boldsymbol{Q}_1}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\ \\end{matrix} \\right] \\xrightarrow{\\boldsymbol{Q}_2} \\] \\[ \\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\ \\end{matrix} \\right] \\xrightarrow{\\boldsymbol{Q}_3}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\     0&amp;      0&amp;      0&amp;      \\times\\\\ \\end{matrix} \\right] =\\boldsymbol{R} \\] <p>Example step 1:</p> \\[ \\boldsymbol{x}_{\\left( \\mathrm{first} \\mathrm{column} \\right)}=\\left[ \\begin{array}{c}     \\times\\\\     \\times\\\\     \\times\\\\     \\times\\\\ \\end{array} \\right] \\xrightarrow{\\boldsymbol{Q}_1}\\left[ \\begin{array}{c}     \\times\\\\     0\\\\     0\\\\     0\\\\ \\end{array} \\right]  \\] <p>where \\(\\boldsymbol{Q}_1\\) is unitary.</p> <p></p> <p>We can get:</p> \\[ \\boldsymbol{v}=-\\left\\| \\boldsymbol{x} \\right\\| \\boldsymbol{e}_1-\\boldsymbol{x}, \\boldsymbol{v}\\bot \\mathbf{H} \\] \\[ \\boldsymbol{Q}_1=\\boldsymbol{I}-2\\cdot \\frac{\\boldsymbol{v}\\cdot \\boldsymbol{v}^*}{\\boldsymbol{v}^*\\boldsymbol{v}} \\] \\[ \\boldsymbol{Q}_1\\boldsymbol{x}=-\\left\\| \\boldsymbol{x} \\right\\| \\boldsymbol{e}_1 \\] <p><code>Question</code>: why not choose \\(\\mathbf{H}_2\\) and get \\(\\boldsymbol{v}=\\left\\| \\boldsymbol{x} \\right\\| \\boldsymbol{e}_1-\\boldsymbol{x}\\)?</p> <p>We have two choices: \\(\\boldsymbol{Q}_1\\boldsymbol{x}=-\\left\\| \\boldsymbol{x} \\right\\| \\boldsymbol{e}_1, \\boldsymbol{Q}_1\\boldsymbol{x}=\\left\\| \\boldsymbol{x} \\right\\| \\boldsymbol{e}_1\\). </p> <p>In practice, we select:</p> \\[ \\boldsymbol{v}=-\\mathrm{sign}\\left( \\boldsymbol{x}_1 \\right) \\cdot \\left\\| \\boldsymbol{x} \\right\\| \\boldsymbol{e}_1-\\boldsymbol{x} \\] <p>\\(\\boldsymbol{x}_1\\) is the first entry of \\(\\boldsymbol{x}\\). We choose it for the stability reason. This results in a larger \\(\\left\\| \\boldsymbol{v} \\right\\|\\).</p> <p>Example step 2:</p> \\[ \\boldsymbol{x}_{\\left( \\mathrm{second} \\mathrm{column} \\right)}=\\left[ \\begin{array}{c}     \\times\\\\     \\times\\\\     \\times\\\\     \\times\\\\ \\end{array} \\right] \\xrightarrow{\\boldsymbol{Q}_2}\\left[ \\begin{array}{c}     \\times\\\\     \\times\\\\     0\\\\     0\\\\ \\end{array} \\right]  \\] <p>We can get:</p> \\[ \\boldsymbol{Q}_2=\\left[ \\begin{matrix}     1&amp;      0\\\\     0&amp;      \\boldsymbol{F}\\\\ \\end{matrix} \\right]  \\]"},{"location":"Eigenvalue/QR_Factorization/#householder-qr-factorization","title":"Householder QR Factorization","text":"<p>Here is the algorithm for Householder QR Factorization for a matrix \\(\\boldsymbol{A}\\in \\mathbb{R} ^{m\\times n}\\) :</p> <p>For \\(k=1:m\\):</p> <ul> <li>\\(\\boldsymbol{x}=\\boldsymbol{A}_{k:m,k}\\);</li> <li>\\(\\boldsymbol{v}_k=-\\mathrm{sign}\\left( x_1 \\right) \\cdot \\left\\| \\boldsymbol{x} \\right\\| \\boldsymbol{e}_1-\\boldsymbol{x}\\);</li> <li>\\(\\boldsymbol{v}_k=\\frac{\\boldsymbol{v}_k}{\\left\\| \\boldsymbol{v}_k \\right\\|}\\);</li> <li>\\(\\boldsymbol{A}_{k:m,k:n}=\\boldsymbol{A}_{k:m,k:n}-2\\boldsymbol{v}_k\\left( {\\boldsymbol{v}_k}^T\\boldsymbol{A}_{k:m,k:n} \\right)\\);</li> </ul> <p>End</p> <p>Remarks:</p> <ol> <li>QR by Householder transformation is stable. In practice, it is used more frequently than modified Gram-Schmidt algorithm.</li> <li>The cost of this algorithm is \\(O\\left( 2mn^2-\\frac{2}{3}n^3 \\right)\\).</li> <li>\\(\\boldsymbol{Q}\\) has never been formed explicitly.</li> </ol>"},{"location":"Eigenvalue/QR_Factorization/#givens-rotation-as-qr-factorization","title":"Givens Rotation as QR Factorization","text":"\\[ \\boldsymbol{A}={\\boldsymbol{T}_{12}}^*\\cdots {\\boldsymbol{T}_{1n}}^*{\\boldsymbol{T}_{23}}^*\\cdots {\\boldsymbol{T}_{2n}}^*\\cdots {\\boldsymbol{T}_{n-1,n}}^*\\boldsymbol{R}=\\boldsymbol{QR} \\] <p>The core element of Givens Rotation:</p> \\[ \\boldsymbol{T}_{pq}=\\left[ \\begin{matrix}     1&amp;      &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       \\\\     &amp;       \\ddots&amp;     &amp;       \\vdots&amp;     &amp;       &amp;       &amp;       \\vdots&amp;     &amp;       &amp;       \\\\     &amp;       &amp;       1&amp;      &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       \\\\     &amp;       \\cdots&amp;     &amp;       \\bar{c}_{\\left( p,p \\right)}&amp;       &amp;       \\cdots&amp;     &amp;       \\bar{s}_{\\left( p,q \\right)}&amp;       &amp;       \\cdots&amp;     \\\\     &amp;       &amp;       &amp;       &amp;       1&amp;      &amp;       &amp;       &amp;       &amp;       &amp;       \\\\     &amp;       &amp;       &amp;       \\vdots&amp;     &amp;       \\ddots&amp;     &amp;       \\vdots&amp;     &amp;       &amp;       \\\\     &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       1&amp;      &amp;       &amp;       &amp;       \\\\     &amp;       \\cdots&amp;     &amp;       -s_{\\left( q,p \\right)}&amp;        &amp;       \\cdots&amp;     &amp;       c_{\\left( q,q \\right)}&amp;     &amp;       \\cdots&amp;     \\\\     &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       1&amp;      &amp;       \\\\     &amp;       &amp;       &amp;       \\vdots&amp;     &amp;       &amp;       &amp;       \\vdots&amp;     &amp;       \\ddots&amp;     \\\\     &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       1\\\\ \\end{matrix} \\right]  \\] \\[ c=\\cos \\theta ; \\\\ s=\\sin \\theta  \\] <p>Transformation pattern:</p> \\[ \\left[ \\begin{array}{c}     x_1\\\\     x_2\\\\     \\vdots\\\\     x_p\\\\     \\vdots\\\\     x_q\\\\     \\vdots\\\\     x_n\\\\ \\end{array} \\right] :\\left[ \\begin{array}{c}     x_p\\\\     x_q\\\\ \\end{array} \\right] \\rightarrow \\left[ \\begin{array}{c}     \\pm \\sqrt{{x_p}^2+{x_q}^2}\\\\     0\\\\ \\end{array} \\right]  \\]"},{"location":"Eigenvalue/SVD/","title":"Singular Value Decomposition (SVD)","text":""},{"location":"Eigenvalue/SVD/#svd-fundamentals","title":"SVD Fundamentals","text":""},{"location":"Eigenvalue/SVD/#unitary-matrix","title":"Unitary Matrix","text":"<p>Unitary matrix: \\(\\boldsymbol{Q}\\) is a unitary matrix if:</p> \\[ \\boldsymbol{QQ}^*=\\mathbf{I}, \\boldsymbol{Q}^*\\boldsymbol{Q}=\\mathbf{I} \\] <p>where \\(\\boldsymbol{Q}^*\\) is the conjugate transpose of \\(\\boldsymbol{Q}\\).</p> <p>A unitary matrix satisfies \\(\\left\\| \\boldsymbol{QA} \\right\\| _2=\\left\\| \\boldsymbol{A} \\right\\| _2\\), which means that a unitary transform does not change the 2-norm of a matrix. Similarly, it can also be verified that \\(\\left\\| \\boldsymbol{QA} \\right\\| _F=\\left\\| \\boldsymbol{A} \\right\\| _F\\), which means that a unitary transform does not change the Frobenius norm of a matrix.</p>"},{"location":"Eigenvalue/SVD/#introduction-to-svd","title":"Introduction to SVD","text":""},{"location":"Eigenvalue/SVD/#introductory-example","title":"Introductory Example","text":"<p><code>Example</code>: Let \\(\\boldsymbol{A}\\in \\mathbb{R} ^{2\\times 2}, \\boldsymbol{A}:\\mathbb{R} ^2\\mapsto \\mathbb{R} ^2\\). What is the image of the unit disk of \\(\\boldsymbol{A}\\) ?</p> <p></p> <p>We get:</p> \\[ \\boldsymbol{Av}_1=\\sigma _1\\boldsymbol{u}_1;\\boldsymbol{Av}_2=\\sigma _2\\boldsymbol{u}_2 \\] \\[ \\boldsymbol{A}\\left[ \\begin{matrix}     \\boldsymbol{v}_1&amp;       \\boldsymbol{v}_2\\\\ \\end{matrix} \\right] =\\left[ \\begin{matrix}     \\sigma _1\\boldsymbol{u}_1&amp;      \\sigma _2\\boldsymbol{u}_2\\\\ \\end{matrix} \\right] =\\left[ \\begin{matrix}     \\boldsymbol{u}_1&amp;       \\boldsymbol{u}_2\\\\ \\end{matrix} \\right] \\cdot \\left[ \\begin{matrix}     \\sigma _1&amp;      0\\\\     0&amp;      \\sigma _2\\\\ \\end{matrix} \\right]  \\] <p>Let:</p> \\[ \\boldsymbol{V}=\\left[ \\begin{matrix}     \\boldsymbol{v}_1&amp;       \\boldsymbol{v}_2\\\\ \\end{matrix} \\right] , \\boldsymbol{U}=\\left[ \\begin{matrix}     \\boldsymbol{u}_1&amp;       \\boldsymbol{u}_2\\\\ \\end{matrix} \\right] , \\mathbf{\\Sigma }=\\left[ \\begin{matrix}     \\sigma _1&amp;      0\\\\     0&amp;      \\sigma _2\\\\ \\end{matrix} \\right]  \\] <p>We can get:</p> \\[ \\boldsymbol{AV}=\\boldsymbol{U}\\mathbf{\\Sigma } \\] <p>where</p> \\[ \\boldsymbol{VV}^*=\\mathbf{I}, \\boldsymbol{UU}^*=\\mathbf{I} \\] <p>Then \\(\\boldsymbol{A}=\\boldsymbol{AVV}^*=\\boldsymbol{U}\\mathbf{\\Sigma }\\boldsymbol{V}^*\\).</p>"},{"location":"Eigenvalue/SVD/#definition-and-basic-theorem","title":"Definition and Basic Theorem","text":"<p>The Singular Value Decomposition (SVD) is:</p> \\[ \\boldsymbol{A}=\\boldsymbol{U}\\mathbf{\\Sigma }\\boldsymbol{V}^* \\] <p>where \\(\\boldsymbol{U}\\in \\mathbb{C} ^{m\\times m}\\) is a complex unitary matrix, \\(\\mathbf{\\Sigma }\\in \\mathbb{R} ^{m\\times n}\\) is a rectangular diagonal matrix with non-negative real numbers on the diagonal, and \\(\\boldsymbol{V}\\in \\mathbb{C} ^{n\\times n}\\) is a complex unitary matrix. (\\(\\boldsymbol{A}\\in \\mathbb{C} ^{m\\times n}\\) is a complex matrix)</p> <p>The diagonal entries \\(\\sigma _i=\\Sigma _{ii}\\) of \\(\\mathbf{\\Sigma }\\) are uniquely determined by \\(\\boldsymbol{A}\\) and are called singular values of \\(\\boldsymbol{A}\\).</p> <p><code>Theorem</code>(Existence and Uniqueness): Every matrix \\(\\boldsymbol{A}\\in \\mathbb{C} ^{m\\times n}\\) has a SVD. The singular values are uniquely determined. If \\(\\boldsymbol{A}\\) is square and \\(\\sigma _j\\) are distinct, the left and right singular vectors \\(\\boldsymbol{u}_i,\\boldsymbol{v}_i\\) are uniquely determined up to a complex sign (complex number with modular one).</p>"},{"location":"Eigenvalue/SVD/#properties-of-svd","title":"Properties of SVD","text":"<p>From:</p> \\[ \\boldsymbol{A}^*\\boldsymbol{A}=\\left( \\boldsymbol{U}\\mathbf{\\Sigma }\\boldsymbol{V}^* \\right) ^*\\cdot \\boldsymbol{U}\\mathbf{\\Sigma }\\boldsymbol{V}^* \\] \\[ =\\left( \\boldsymbol{V}\\mathbf{\\Sigma }^*\\boldsymbol{U}^* \\right) \\boldsymbol{U}\\mathbf{\\Sigma }\\boldsymbol{V}^*=\\boldsymbol{V}\\mathbf{\\Sigma }^2\\boldsymbol{V}^*=\\boldsymbol{V}\\mathbf{\\Sigma }^2\\boldsymbol{V}^{-1} \\] <p>We know that \\({\\sigma _i}^2\\) is the eigenvalue of \\(\\boldsymbol{A}^*\\boldsymbol{A}\\).</p> <p>We can also get \\(\\mathrm{rank}\\left( \\boldsymbol{A} \\right)\\) is the number of non-zero elements of singular values.</p> <p>Furthermore, \\(\\left\\| \\boldsymbol{A} \\right\\| _2=\\sigma _1\\) (largest singular value) if \\(\\sigma _1\\geqslant \\sigma _2\\geqslant \\cdots \\geqslant \\sigma _r\\).</p> <p>Also:</p> \\[ \\left\\| \\boldsymbol{A} \\right\\| _F=\\left( \\sum_{i=1}^r{{\\sigma _i}^2} \\right) ^{\\frac{1}{2}} \\] <p>If \\(\\boldsymbol{A}\\in \\mathbb{R} ^{m\\times m}\\), then:</p> \\[ \\left| \\det \\left( \\boldsymbol{A} \\right) \\right|=\\prod_{i=1}^m{\\sigma _i} \\]"},{"location":"Eigenvalue/SVD/#svd-computation","title":"SVD Computation","text":"<p>From:</p> \\[ \\boldsymbol{A}=\\boldsymbol{U}\\mathbf{\\Sigma }\\boldsymbol{V}^* \\] <p>We can get:</p> \\[ \\boldsymbol{A}^*\\boldsymbol{A}=\\boldsymbol{V}\\underset{\\boldsymbol{D}}{\\underbrace{\\mathbf{\\Sigma }^*\\mathbf{\\Sigma }}}\\boldsymbol{V}^* \\] <p>as the eigenvalue decomposition of \\(\\boldsymbol{A}^*\\boldsymbol{A}\\). \\(\\boldsymbol{A}^*\\boldsymbol{A}\\) is called normal matrix.</p> <p>Similar to eigenvalue decomposition, SVD computation has to be iterative!</p> <p>SVD is computed in two phases as well.</p>"},{"location":"Eigenvalue/SVD/#phase-one","title":"Phase One","text":"<p>Phase 1 (Different from eigenvalue decomposition, there is no similarity requirement here): We convert \\(\\boldsymbol{A}\\) into a bidiagonal matrix.</p> <p>The algorithm is called Golub-Kahan bidiagonalization.</p> <p><code>Example</code>:</p> \\[ \\boldsymbol{A}=\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\ \\end{matrix} \\right] \\xrightarrow{{\\boldsymbol{U}_1}^*}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\ \\end{matrix} \\right] \\xrightarrow[\\boldsymbol{V}_1]{}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     0&amp;      0\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\ \\end{matrix} \\right]  \\] \\[ \\xrightarrow{{\\boldsymbol{U}_2}^*}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     0&amp;      0\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\ \\end{matrix} \\right] \\xrightarrow[\\boldsymbol{V}_2]{}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     0&amp;      0\\\\     0&amp;      \\times&amp;     \\times&amp;     0\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\ \\end{matrix} \\right]  \\] \\[ \\xrightarrow{{\\boldsymbol{U}_3}^*}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     0&amp;      0\\\\     0&amp;      \\times&amp;     \\times&amp;     0\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\     0&amp;      0&amp;      0&amp;      \\times\\\\     0&amp;      0&amp;      0&amp;      \\times\\\\ \\end{matrix} \\right] \\xrightarrow[\\boldsymbol{V}_3]{}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     0&amp;      0\\\\     0&amp;      \\times&amp;     \\times&amp;     0\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\     0&amp;      0&amp;      0&amp;      \\times\\\\     0&amp;      0&amp;      0&amp;      \\times\\\\ \\end{matrix} \\right] \\xrightarrow{{\\boldsymbol{U}_4}^*}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     0&amp;      0\\\\     0&amp;      \\times&amp;     \\times&amp;     0\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\     0&amp;      0&amp;      0&amp;      \\times\\\\     0&amp;      0&amp;      0&amp;      0\\\\ \\end{matrix} \\right]  \\] <p>We can get the upper bidiagonal matrix. The work for Golub-Kahan bidiagonalization is \\(O\\left( 4mn^2-\\frac{4}{3}n^3 \\right)\\) flops.</p> <p>Another method is called Lawson-Hanson-Chan algorithm. This is more efficient when \\(m\\gg n\\).</p>"},{"location":"Eigenvalue/SVD/#phase-two","title":"Phase Two","text":"<p>Phase 2: Assume \\(\\boldsymbol{A}\\) is bidiagonal. Phase 2 must be an iterative procedure. Let \\(\\sigma _k=\\sqrt{\\lambda _k}\\), where \\(\\lambda _k\\) is an eigenvalue of \\(\\boldsymbol{A}^*\\boldsymbol{A}\\). </p>"},{"location":"Eigenvalue/SVD/#regular-method","title":"Regular Method","text":"<p>A natural strategy is:</p> <ol> <li>Form \\(\\boldsymbol{A}^*\\boldsymbol{A}\\);</li> <li>Compute \\(\\boldsymbol{A}^*\\boldsymbol{A}=\\boldsymbol{V}\\mathbf{\\Lambda }\\boldsymbol{V}^*\\);</li> <li>\\(\\sqrt{\\mathbf{\\Lambda }}=\\mathbf{\\Sigma }\\) (component-wise square root);</li> <li>Determine \\(\\boldsymbol{U}=\\boldsymbol{AV}\\mathbf{\\Sigma }^{-1}\\).</li> </ol> <p>This does work well in certain cases, but not so much in other cases!</p> <p>Denote \\(\\tilde{\\sigma}_k\\) as the computed singular value: \\(\\tilde{\\sigma}_k=\\sqrt{\\tilde{\\lambda}_k}\\).</p> <p><code>Claim</code>: \\(\\left| \\tilde{\\sigma}_k-\\sigma _k \\right|=O\\left( \\frac{\\left\\| \\boldsymbol{A} \\right\\| ^2}{\\sigma _k}\\varepsilon _{\\mathrm{machine}} \\right)\\).</p> <p><code>Proof</code>:</p> <p>Bauer\u2013Fike theorem states that:</p> \\[ \\left| \\lambda _k\\left( \\boldsymbol{A}^*\\boldsymbol{A}+\\delta \\boldsymbol{B} \\right) -\\lambda _k\\left( \\boldsymbol{A}^*\\boldsymbol{A} \\right) \\right|\\leqslant \\left\\| \\delta \\boldsymbol{B} \\right\\| _2 \\] <p>Therefore, eigenvalue is a continuous function. Also \\(\\left| \\sigma _k\\left( \\boldsymbol{A}+\\delta \\boldsymbol{A} \\right) -\\sigma _k\\boldsymbol{A} \\right|\\leqslant \\left\\| \\delta \\boldsymbol{A} \\right\\| _2\\).</p> <p>If \\(\\tilde{\\lambda}_k\\) is computed by a stable method, then:</p> \\[ \\left| \\tilde{\\lambda}_k-\\lambda _k \\right|\\leqslant \\left\\| \\delta \\boldsymbol{B} \\right\\| _2; \\] \\[ \\frac{\\left\\| \\delta \\boldsymbol{B} \\right\\| _2}{\\left\\| \\boldsymbol{A}^*\\boldsymbol{A} \\right\\| _2}\\leqslant O\\left( \\varepsilon _{\\mathrm{machine}} \\right)  \\] <p>Then:</p> \\[ \\left| \\tilde{\\lambda}_k-\\lambda _k \\right|\\leqslant O\\left( \\left\\| \\boldsymbol{A}^*\\boldsymbol{A} \\right\\| _2\\cdot \\varepsilon _{\\mathrm{machine}} \\right)  \\] <p>Take the square root:</p> \\[ \\left| \\tilde{\\sigma}_k-\\sigma _k \\right|=\\left| \\sqrt{\\tilde{\\lambda}_k}-\\sqrt{\\lambda _k} \\right|=\\left| \\frac{\\tilde{\\lambda}_k-\\lambda _k}{\\sqrt{\\tilde{\\lambda}_k}+\\sqrt{\\lambda _k}} \\right|=O\\left( \\frac{\\left| \\tilde{\\lambda}_k-\\lambda _k \\right|}{2\\sqrt{\\lambda _k}} \\right)  \\] \\[ \\leqslant O\\left( \\frac{\\left\\| \\boldsymbol{A}^*\\boldsymbol{A} \\right\\| _2\\varepsilon _{\\mathrm{machine}}}{2\\sigma _k} \\right) \\approx O\\left( \\frac{{\\left\\| \\boldsymbol{A} \\right\\| _2}^2}{\\sigma _k}\\varepsilon _{\\mathrm{machine}} \\right)  \\] <p>We end the proof.</p> <p>Note: If \\(\\sigma _k\\sim {\\left\\| \\boldsymbol{A} \\right\\| _2}^2\\), the computation is accurate. But if \\(\\sigma _k\\ll {\\left\\| \\boldsymbol{A} \\right\\| _2}^2\\), this is not a good strategy.</p>"},{"location":"Eigenvalue/SVD/#better-method","title":"Better Method","text":"<p>If \\(\\frac{{\\left\\| \\boldsymbol{A} \\right\\| _2}^2}{\\sigma _k}\\) is large, is there a better way to compute the SVD is phase 2? The answer is yes.</p> <p>Form a matrix:</p> \\[ \\boldsymbol{H}=\\left[ \\begin{matrix}     \\boldsymbol{O}&amp;     \\boldsymbol{A}^*\\\\     \\boldsymbol{A}&amp;     \\boldsymbol{O}\\\\ \\end{matrix} \\right]  \\] <p>Note that \\(\\boldsymbol{A}=\\boldsymbol{U}\\mathbf{\\Sigma }\\boldsymbol{V}^*, \\boldsymbol{A}^*=\\boldsymbol{V}\\mathbf{\\Sigma }^*\\boldsymbol{U}^*\\). We can verify:</p> \\[ \\left[ \\begin{matrix}     \\boldsymbol{O}&amp;     \\boldsymbol{A}^*\\\\     \\boldsymbol{A}&amp;     \\boldsymbol{O}\\\\ \\end{matrix} \\right] \\left[ \\begin{matrix}     \\boldsymbol{V}&amp;     \\boldsymbol{V}\\\\     \\boldsymbol{U}&amp;     -\\boldsymbol{U}\\\\ \\end{matrix} \\right] =\\left[ \\begin{matrix}     \\boldsymbol{V}&amp;     \\boldsymbol{V}\\\\     \\boldsymbol{U}&amp;     -\\boldsymbol{U}\\\\ \\end{matrix} \\right] \\left[ \\begin{matrix}     \\mathbf{\\Sigma }&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     -\\mathbf{\\Sigma }\\\\ \\end{matrix} \\right] ; \\] \\[ \\left[ \\begin{matrix}     \\boldsymbol{V}^*&amp;       \\boldsymbol{U}^*\\\\     \\boldsymbol{V}^*&amp;       -\\boldsymbol{U}^*\\\\ \\end{matrix} \\right] \\left[ \\begin{matrix}     \\boldsymbol{V}&amp;     \\boldsymbol{V}\\\\     \\boldsymbol{U}&amp;     -\\boldsymbol{U}\\\\ \\end{matrix} \\right] =\\left[ \\begin{matrix}     \\boldsymbol{V}^*\\boldsymbol{V}+\\boldsymbol{U}^*\\boldsymbol{U}&amp;      \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\boldsymbol{V}^*\\boldsymbol{V}+\\boldsymbol{U}^*\\boldsymbol{U}\\\\ \\end{matrix} \\right]  \\] <p>This helps with the eigenvalue decomposition of \\(\\boldsymbol{H}\\). The singular values of \\(\\boldsymbol{A}\\) are the absolute values of the eigenvalues of \\(\\boldsymbol{H}\\). Therefore, the eigenvalues of \\(\\boldsymbol{H}\\) give the singular values of \\(\\boldsymbol{A}\\).</p>"},{"location":"Gaussian/","title":"Direct Methods for Solving Linear Systems","text":"<ul> <li>Gaussian Elimination</li> <li>Variants of Gaussian Elimination</li> </ul>"},{"location":"Gaussian/Gaussian_Elimination/","title":"Gaussian Elimination","text":"<p>Lower\u2013Upper (LU) decomposition or factorization factors a matrix as the product of a lower triangular matrix and an upper triangular matrix. The product sometimes includes a permutation matrix as well. LU decomposition can be viewed as the matrix form of Gaussian Elimination.</p>"},{"location":"Gaussian/Gaussian_Elimination/#introduction-to-lu-factorization","title":"Introduction to LU Factorization","text":"<p>General steps of Gaussian Elimination:</p> \\[ \\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\ \\end{matrix} \\right] \\xrightarrow{\\boldsymbol{L}_1}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\ \\end{matrix} \\right]  \\] \\[ \\xrightarrow{\\boldsymbol{L}_2}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\ \\end{matrix} \\right] \\xrightarrow{\\boldsymbol{L}_3}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\     0&amp;      0&amp;      0&amp;      \\times\\\\ \\end{matrix} \\right] ; \\] \\[ \\Longrightarrow \\boldsymbol{L}_3\\boldsymbol{L}_2\\boldsymbol{L}_1\\boldsymbol{A}=\\boldsymbol{U} \\] <p><code>Question</code>: What is \\(\\boldsymbol{L} _i\\) ?</p> <p>In essence:</p> \\[ \\boldsymbol{x}_k=\\left[ \\begin{array}{c}     x_{1k}\\\\     \\vdots\\\\     x_{kk}\\\\     x_{k+1,k}\\\\     \\vdots\\\\     x_{mk}\\\\ \\end{array} \\right] \\xrightarrow{\\boldsymbol{L}_k}\\left[ \\begin{array}{c}     x_{1k}\\\\     \\vdots\\\\     x_{kk}\\\\     0\\\\     \\vdots\\\\     0\\\\ \\end{array} \\right]  \\] <p>Define \\(l_{jk}\\triangleq \\frac{x_{jk}}{x_{kk}}\\). Then we can do the \\(\\left( j \\right) -l_{jk}\\cdot \\left( k \\right)\\) row reduction. Also define:</p> \\[ \\boldsymbol{L}_k\\triangleq \\left[ \\begin{matrix}     1&amp;      &amp;       &amp;       &amp;       &amp;       \\\\     &amp;       \\ddots&amp;     &amp;       &amp;       &amp;       \\\\     &amp;       &amp;       1&amp;      &amp;       &amp;       \\\\     &amp;       &amp;       -l_{k+1,k}&amp;     \\ddots&amp;     &amp;       \\\\     &amp;       &amp;       \\vdots&amp;     &amp;       \\ddots&amp;     \\\\     &amp;       &amp;       -l_{m,k}&amp;       &amp;       &amp;       1\\\\ \\end{matrix} \\right]  \\] <p>And:</p> \\[ \\boldsymbol{L}_k\\cdot \\left[ \\begin{array}{c}     x_{1k}\\\\     \\vdots\\\\     x_{kk}\\\\     x_{k+1,k}\\\\     \\vdots\\\\     x_{mk}\\\\ \\end{array} \\right] \\rightarrow \\left[ \\begin{array}{c}     x_{1k}\\\\     \\vdots\\\\     x_{kk}\\\\     0\\\\     \\vdots\\\\     0\\\\ \\end{array} \\right] ; \\] \\[ \\boldsymbol{L}_k=\\mathbf{I}-\\boldsymbol{l}_k{\\boldsymbol{e}_k}^T;\\boldsymbol{l}_k=\\left[ \\begin{array}{c}     0\\\\     \\vdots\\\\     0\\\\     l_{k+1,k}\\\\     \\vdots\\\\     l_{mk}\\\\ \\end{array} \\right] ,\\boldsymbol{e}_k=\\left[ \\begin{array}{c}     0\\\\     \\vdots\\\\     0\\\\     1_{\\left( k+1\\ \\mathrm{position} \\right)}\\\\     0\\\\     \\vdots\\\\     0\\\\ \\end{array} \\right]  \\] <p>Then:</p> \\[ \\boldsymbol{L}_k=\\mathbf{I}-\\boldsymbol{l}_k{\\boldsymbol{e}_k}^T, \\] \\[ \\boldsymbol{L}_{m-1}\\cdots \\boldsymbol{L}_2\\boldsymbol{L}_1\\boldsymbol{A}=\\boldsymbol{U}\\Longrightarrow \\boldsymbol{A}={\\boldsymbol{L}_1}^{-1}{\\boldsymbol{L}_2}^{-1}\\cdots {\\boldsymbol{L}_{m-1}}^{-1}\\boldsymbol{U} \\] <p>Note that the inverse of a lower triangular matrix is also a lower triangular matrix. The product of lower triangular matrices is also a lower triangular matrix.</p> <p>Define \\(\\boldsymbol{L}\\triangleq {\\boldsymbol{L}_1}^{-1}{\\boldsymbol{L}_2}^{-1}\\cdots {\\boldsymbol{L}_{m-1}}^{-1}\\). Then:</p> \\[ \\boldsymbol{A}=\\boldsymbol{LU} \\] <p>is the LU factorization.</p> <p>Here are some useful conclusions:</p> \\[ {\\boldsymbol{L}_k}^{-1}=\\mathbf{I}+\\boldsymbol{l}_k{\\boldsymbol{e}_k}^T \\] \\[ \\Longleftarrow {\\boldsymbol{L}_k}^{-1}\\boldsymbol{L}_k=\\left( \\mathbf{I}+\\boldsymbol{l}_k{\\boldsymbol{e}_k}^T \\right) \\cdot \\left( \\mathbf{I}-\\boldsymbol{l}_k{\\boldsymbol{e}_k}^T \\right) =\\mathbf{I}-\\boldsymbol{l}_k{\\boldsymbol{e}_k}^T\\boldsymbol{l}_k{\\boldsymbol{e}_k}^T=\\mathbf{I} \\] <p>Also note:</p> \\[ {\\boldsymbol{L}_k}^{-1}{\\boldsymbol{L}_{k+1}}^{-1}=\\left( \\mathbf{I}+\\boldsymbol{l}_k{\\boldsymbol{e}_k}^T \\right) \\cdot \\left( \\mathbf{I}+\\boldsymbol{l}_{k+1}{\\boldsymbol{e}_{k+1}}^T \\right)  \\] \\[ =\\mathbf{I}+\\boldsymbol{l}_k{\\boldsymbol{e}_k}^T+\\boldsymbol{l}_{k+1}{\\boldsymbol{e}_{k+1}}^T+\\boldsymbol{l}_k{\\boldsymbol{e}_k}^T\\boldsymbol{l}_{k+1}{\\boldsymbol{e}_{k+1}}^T \\] \\[ =\\mathbf{I}+\\boldsymbol{l}_k{\\boldsymbol{e}_k}^T+\\boldsymbol{l}_{k+1}{\\boldsymbol{e}_{k+1}}^T \\] \\[ =\\left[ \\begin{matrix}     1&amp;      &amp;       &amp;       &amp;       &amp;       &amp;       \\\\     &amp;       \\ddots&amp;     &amp;       &amp;       &amp;       &amp;       \\\\     &amp;       &amp;       1&amp;      &amp;       &amp;       &amp;       \\\\     &amp;       &amp;       l_{k+1,k}&amp;      1&amp;      &amp;       &amp;       \\\\     &amp;       &amp;       \\vdots&amp;     l_{k+2,k+1}&amp;        \\ddots&amp;     &amp;       \\\\     &amp;       &amp;       \\vdots&amp;     \\vdots&amp;     &amp;       \\ddots&amp;     \\\\     &amp;       &amp;       l_{m,k}&amp;        l_{m,k+1}&amp;      &amp;       &amp;       1\\\\ \\end{matrix} \\right]  \\] <p>This is good for bookkeeping the solutions. Therefore:</p> \\[ \\boldsymbol{L}={\\boldsymbol{L}_1}^{-1}{\\boldsymbol{L}_2}^{-1}\\cdots {\\boldsymbol{L}_{m-1}}^{-1}=\\left[ \\begin{matrix}     1&amp;      &amp;       &amp;       &amp;       &amp;       \\\\     l_{21}&amp;     \\ddots&amp;     &amp;       &amp;       &amp;       \\\\     \\vdots&amp;     &amp;       1&amp;      &amp;       &amp;       \\\\     \\vdots&amp;     &amp;       l_{k+1,k}&amp;      \\ddots&amp;     &amp;       \\\\     \\vdots&amp;     &amp;       \\vdots&amp;     &amp;       \\ddots&amp;     \\\\     l_{m1}&amp;     &amp;       l_{m,k}&amp;        &amp;       &amp;       1\\\\ \\end{matrix} \\right] ; \\] \\[ \\boldsymbol{A}=\\boldsymbol{LU} \\] <p>To solve \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\), we may solve \\(\\boldsymbol{LUx}=\\boldsymbol{b}\\) instead. Define \\(\\boldsymbol{y}=\\boldsymbol{Ux}\\). We can solve two linear systems:</p> \\[ \\boldsymbol{Ly}=\\boldsymbol{b}, \\boldsymbol{Ux}=\\boldsymbol{y} \\]"},{"location":"Gaussian/Gaussian_Elimination/#steps-of-gaussian-elimination","title":"Steps of Gaussian Elimination","text":"<p><code>Algorithm</code> ( Simple Gaussian Elimination (GE) ):</p> <p>( Given \\(\\boldsymbol{A}\\), we output \\(\\boldsymbol{L}, \\boldsymbol{U}\\). )</p> <p>Let \\(\\boldsymbol{U}=\\boldsymbol{A},\\boldsymbol{L}=\\mathbf{I}\\) first;</p> <p>For \\(k=1,2,\\cdots ,m-1\\):</p> <ul> <li>For \\(j=k+1,k+2,\\cdots ,m\\):<ul> <li>\\(l_{j,k}=\\frac{u_{j,k}}{u_{k,k}}\\);</li> <li>\\(u_{j,k:m}=u_{j,k:m}-l_{j,k}\\cdot u_{k,k:m}\\);</li> </ul> </li> <li>End;</li> </ul> <p>End</p> <p>There are actually three loops for this algorithm.</p> <p><code>Question</code>: What is the cost of the algorithm?</p> <p>We measure the computational complexity by the number of flops (floating point operations):</p> \\[ \\sum_{k=1}^{m-1}{\\sum_{j=k+1}^m{2\\left( m-k+1 \\right) +1}}\\approx \\frac{2}{3}m^3+O\\left( m^2 \\right)  \\] <p>Therefore, the cost of LU factorization is \\(O\\left( m^3 \\right)\\).</p>"},{"location":"Gaussian/Gaussian_Elimination/#forward-and-backward-substitution","title":"Forward and Backward Substitution","text":"<p>Consider \\(\\boldsymbol{Ly}=\\boldsymbol{b}\\), we do Forward Substitution:</p> <p>For \\(j=1,2,\\cdots ,m\\):</p> \\[ y_j=b_j-\\sum_{k=1}^{j-1}{l_{kj}y_k} \\] <p>End. The cost for it is \\(O\\left( m^2 \\right)\\).</p> <p>Consider \\(\\boldsymbol{Ux}=\\boldsymbol{y}\\), we do Backward Substitution:</p> <p>For \\(j=m,m-1,\\cdots ,1\\):</p> \\[ x_j=\\frac{1}{r_{jj}}\\left( y_j-\\sum_{k=j+1}^m{x_kr_{jk}} \\right)  \\]"},{"location":"Gaussian/Gaussian_Elimination/#evaluation","title":"Evaluation","text":"<p>Gaussian-Elimination (Native) is not stable!</p> <p>e.g., Matrix \\(\\boldsymbol{A}=\\left[ \\begin{matrix}     0&amp;      1\\\\     1&amp;      1\\\\ \\end{matrix} \\right]\\) fails at the first step.</p> <p>e.g., For matrix</p> \\[ \\boldsymbol{A}=\\left[ \\begin{matrix}     10^{-20}&amp;       1\\\\     1&amp;      1\\\\ \\end{matrix} \\right]  \\] <p>Apply GE, we get:</p> \\[ \\boldsymbol{L}=\\left[ \\begin{matrix}     1&amp;      0\\\\     10^{20}&amp;        1\\\\ \\end{matrix} \\right] ,\\boldsymbol{U}=\\left[ \\begin{matrix}     10^{-20}&amp;       1\\\\     0&amp;      1-10^{20}\\\\ \\end{matrix} \\right]  \\] <p>However, in computer representation, the actual matrix might be like:</p> \\[ \\tilde{\\boldsymbol{L}}=\\left[ \\begin{matrix}     1&amp;      0\\\\     10^{20}&amp;        1\\\\ \\end{matrix} \\right] ,\\tilde{\\boldsymbol{U}}=\\left[ \\begin{matrix}     10^{-20}&amp;       1\\\\     0&amp;      -10^{20}\\\\ \\end{matrix} \\right]  \\] <p>Then:</p> \\[ \\tilde{\\boldsymbol{L}}\\tilde{\\boldsymbol{U}}=\\left[ \\begin{matrix}     10^{-20}&amp;       1\\\\     1&amp;      0\\\\ \\end{matrix} \\right] \\ne \\boldsymbol{A} \\] <p>A small mistake can lead to a large perturbation.</p> <p>Solution to this problem is partial pivoting.</p>"},{"location":"Gaussian/Gaussian_Elimination_2/","title":"Other Versions of Gaussian Elimination","text":""},{"location":"Gaussian/Gaussian_Elimination_2/#gaussian-elimination-with-partial-pivoting","title":"Gaussian Elimination with Partial Pivoting","text":"<p>Partial pivoting only rearranges the rows of \\(\\boldsymbol{A}\\) and leaves the columns fixed.</p> <p>Define the permutation matrix:</p> \\[ \\boldsymbol{P}\\triangleq \\left[ \\begin{matrix}     1&amp;      &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       \\\\     &amp;       \\ddots&amp;     &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       \\\\     &amp;       &amp;       1&amp;      &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       \\\\     &amp;       &amp;       &amp;       0_{\\left( \\mathrm{ith}\\ \\mathrm{row} \\right)}&amp;      &amp;       \\cdots&amp;     &amp;       1&amp;      &amp;       &amp;       \\\\     &amp;       &amp;       &amp;       &amp;       1&amp;      &amp;       &amp;       &amp;       &amp;       &amp;       \\\\     &amp;       &amp;       &amp;       \\vdots&amp;     &amp;       \\ddots&amp;     &amp;       \\vdots&amp;     &amp;       &amp;       \\\\     &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       1&amp;      &amp;       &amp;       &amp;       \\\\     &amp;       &amp;       &amp;       1&amp;      &amp;       \\cdots&amp;     &amp;       0_{\\left( \\mathrm{jth}\\ \\mathrm{col} \\right)}&amp;      &amp;       &amp;       \\\\     &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       1&amp;      &amp;       \\\\     &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       \\ddots&amp;     \\\\     &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       1\\\\ \\end{matrix} \\right]  \\] <p>\\(\\boldsymbol{PA}\\) switches the \\(i\\)-th and \\(j\\)-th rows in \\(\\boldsymbol{A}\\).</p> <p>We claim:</p> \\[ \\boldsymbol{L}_{m-1}\\boldsymbol{P}_{m-1}\\cdots \\boldsymbol{L}_2\\boldsymbol{P}_2\\boldsymbol{L}_1\\boldsymbol{P}_1\\boldsymbol{A}=\\boldsymbol{U} \\] \\[ \\Longleftrightarrow \\boldsymbol{PA}=\\boldsymbol{LU} \\] <p>Define:</p> \\[ \\boldsymbol{L}_{m-1}\\prime=\\boldsymbol{L}_{m-1}, \\] \\[ \\boldsymbol{L}_{m-2}\\prime=\\boldsymbol{P}_{m-1}\\boldsymbol{L}_{m-2}{\\boldsymbol{P}_{m-1}}^{-1}, \\] \\[ \\boldsymbol{L}_{m-3}\\prime=\\boldsymbol{P}_{m-1}\\boldsymbol{P}_{m-2}\\boldsymbol{L}_{m-3}{\\boldsymbol{P}_{m-2}}^{-1}{\\boldsymbol{P}_{m-1}}^{-1}, \\] \\[ \\vdots  \\] \\[ \\boldsymbol{L}_1\\prime=\\boldsymbol{P}_{m-1}\\boldsymbol{P}_{m-2}\\cdots \\boldsymbol{P}_2\\boldsymbol{L}_1{\\boldsymbol{P}_2}^{-1}\\cdots {\\boldsymbol{P}_{m-2}}^{-1}{\\boldsymbol{P}_{m-1}}^{-1} \\] <p>Then:</p> \\[ \\boldsymbol{L}_{m-1}\\boldsymbol{P}_{m-1}\\cdots \\boldsymbol{L}_2\\boldsymbol{P}_2\\boldsymbol{L}_1\\boldsymbol{P}_1=\\boldsymbol{A}, \\] \\[ \\boldsymbol{L}_{m-1}\\prime\\underset{\\boldsymbol{L}_{m-2}\\prime}{\\underbrace{\\boldsymbol{P}_{m-1}\\boldsymbol{L}_{m-2}{\\boldsymbol{P}_{m-1}}^{-1}}}\\boldsymbol{P}_{m-1}\\boldsymbol{P}_{m-2}\\boldsymbol{L}_{m-3}\\cdots \\boldsymbol{L}_2\\boldsymbol{P}_2\\boldsymbol{L}_1\\boldsymbol{P}_1=\\boldsymbol{A}, \\] \\[ \\underset{\\boldsymbol{L}^{-1}}{\\underbrace{\\boldsymbol{L}_{m-1}\\prime\\boldsymbol{L}_{m-2}\\prime\\cdots \\boldsymbol{L}_1}}\\prime\\underset{\\boldsymbol{P}}{\\underbrace{\\boldsymbol{P}_{m-1}\\cdots \\boldsymbol{P}_1}}\\boldsymbol{A}=\\boldsymbol{U}, \\] \\[ \\Longrightarrow \\boldsymbol{PA}=\\boldsymbol{LU} \\] <p><code>Questions</code>: How to write the algorithm? What is the cost?</p> <p>Note: Complete pivoting (Full pivoting) rearranges both rows and columns. Although it is more stable, this method is a lot more expensive than partial pivoting regarding cost, and is not commonly used in practice.</p>"},{"location":"Gaussian/Gaussian_Elimination_2/#cholesky-factorization","title":"Cholesky Factorization","text":"<p>If \\(\\boldsymbol{A}\\) is symmetric positive definite (SPD), then \\(\\boldsymbol{A}^T=\\boldsymbol{A}\\) and \\(\\boldsymbol{A}\\) has positive eigenvalues. We can also say that:</p> \\[ \\forall \\boldsymbol{x}\\in \\mathbb{R} ^m,\\boldsymbol{x}\\ne \\mathbf{0}:\\ \\boldsymbol{x}^T\\boldsymbol{Ax}&gt;0 \\] <p>By Applying LU factorization to SPD matrix \\(\\boldsymbol{A}\\), we can find that \\(\\boldsymbol{U}\\)'s diagonal parts are positive. Denote the diagonal part of \\(\\boldsymbol{U}\\) as \\(\\boldsymbol{D}=\\mathrm{diag}\\left( \\boldsymbol{U} \\right)\\). \\(\\bar{\\boldsymbol{U}}\\) is diagonal 1 upper triangular matrix.</p> \\[ \\boldsymbol{A}=\\boldsymbol{LU},\\boldsymbol{U}=\\boldsymbol{D}\\bar{\\boldsymbol{U}}, \\] \\[ \\boldsymbol{A}=\\boldsymbol{LU}=\\boldsymbol{LD}\\bar{\\boldsymbol{U}}, \\] \\[ \\boldsymbol{A}^T=\\left( \\boldsymbol{LD}\\bar{\\boldsymbol{U}} \\right) ^T=\\bar{\\boldsymbol{U}}^T\\boldsymbol{DL}^T=\\boldsymbol{A}=\\boldsymbol{LD}\\bar{\\boldsymbol{U}}, \\] \\[ \\Longrightarrow \\bar{\\boldsymbol{U}}^T=\\boldsymbol{L},\\boldsymbol{L}^T=\\bar{\\boldsymbol{U}}, \\] \\[ \\Longrightarrow \\boldsymbol{A}=\\boldsymbol{LDL}^T \\] <p>Define:</p> \\[ \\boldsymbol{D}^{\\frac{1}{2}}=\\left[ \\begin{matrix}     {d_{11}}^{\\frac{1}{2}}&amp;     &amp;       \\boldsymbol{O}\\\\     &amp;       \\ddots&amp;     \\\\     \\boldsymbol{O}&amp;     &amp;       {d_{mm}}^{\\frac{1}{2}}\\\\ \\end{matrix} \\right]  \\] <p>We get:</p> \\[ \\boldsymbol{A}=\\boldsymbol{LDL}^T=\\boldsymbol{LD}^{\\frac{1}{2}}\\boldsymbol{D}^{\\frac{1}{2}}\\boldsymbol{L}^T=\\left( \\boldsymbol{LD}^{\\frac{1}{2}} \\right) \\left( \\boldsymbol{LD}^{\\frac{1}{2}} \\right) ^T \\] <p>Rename \\(\\boldsymbol{LD}^{\\frac{1}{2}}=\\boldsymbol{R}\\), then:</p> \\[ \\boldsymbol{A}=\\boldsymbol{RR}^T \\] <p>This is Cholesky factorization.</p> <p>Actually, there is no need for partial pivoting here!</p> \\[ \\boldsymbol{A}=\\left[ \\begin{matrix}     a_{11}&amp;     \\boldsymbol{w}^T\\\\     \\boldsymbol{w}&amp;     \\boldsymbol{K}\\\\ \\end{matrix} \\right] ,a_{11}&gt;0,\\alpha =\\sqrt{a_{11}}; \\] \\[ \\boldsymbol{A}=\\underset{\\boldsymbol{R}_1}{\\underbrace{\\left[ \\begin{matrix}     \\alpha&amp;     0\\\\     \\frac{\\boldsymbol{w}}{\\alpha}&amp;      \\mathbf{I}\\\\ \\end{matrix} \\right] }}\\underset{\\boldsymbol{A}_1}{\\underbrace{\\left[ \\begin{matrix}     1&amp;      0\\\\     0&amp;      \\boldsymbol{K}-\\frac{\\boldsymbol{ww}^T}{a_{11}}\\\\ \\end{matrix} \\right] }}\\underset{{\\boldsymbol{R}_1}^T}{\\underbrace{\\left[ \\begin{matrix}     \\alpha&amp;     \\frac{\\boldsymbol{w}^T}{\\alpha}\\\\     0&amp;      \\mathbf{I}\\\\ \\end{matrix} \\right] }} \\] \\[ =\\boldsymbol{R}_1\\boldsymbol{A}_1{\\boldsymbol{R}_1}^T \\] <p>We can continue this step:</p> \\[ \\boldsymbol{A}=\\boldsymbol{R}_1\\boldsymbol{A}_1{\\boldsymbol{R}_1}^T=\\cdots =\\underset{\\boldsymbol{R}}{\\underbrace{\\boldsymbol{R}_1\\cdots \\boldsymbol{R}_m}}\\mathbf{I}\\underset{\\boldsymbol{R}^T}{\\underbrace{{\\boldsymbol{R}_m}^T\\cdots {\\boldsymbol{R}_1}^T}} \\] <p><code>Algorithm</code> ( Cholesky Factorization ):</p> <p>Let initial \\(\\boldsymbol{R}=\\boldsymbol{A}\\);</p> <p>For \\(k=1,2,\\cdots ,m\\):</p> <ul> <li>For \\(j=k+1,\\cdots ,m\\):<ul> <li>\\(R_{j,j:m}=R_{j,j:m}-R_{j,j:m}\\frac{R_{kj}}{R_{kk}}\\);</li> </ul> </li> <li>End;</li> <li>\\(R_{k,k:m}=\\frac{R_{k,k:m}}{\\sqrt{R_{kk}}}\\);</li> </ul> <p>End</p> <p>The cost is \\(O\\left( \\frac{1}{3}m^3 \\right) \\approx O\\left( m^3 \\right)\\).</p>"},{"location":"Iterative%20Methods/","title":"Iterative Methods","text":""},{"location":"Iterative%20Methods/#general-introduction","title":"General Introduction","text":"<p>We introduce some iterative methods for solving the linear system \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) in this chapter.</p> <p>Why do we need iterative methods?</p> <ul> <li>Reduce the cost of direct methods: \\(O\\left( m^3 \\right) \\rightarrow O\\left( m^2 \\right) \\xrightarrow{\\mathrm{sometimes}}O\\left( m \\right)\\);</li> <li>Take advantage of matrix-vector multiplications:</li> <li>Matrix-vector multiplications may be inexpensive if the matrix is structured (e.g., banned, sparse);</li> <li>Iterative methods are especially useful if \\(\\boldsymbol{A}\\) is not known explicitly (e.g., black box, PDE).</li> </ul>"},{"location":"Iterative%20Methods/#contents-in-this-chapter","title":"Contents in This Chapter","text":"<ul> <li>Classical Methods</li> <li>General Strategy of Iteration Methods</li> <li>Projection Method</li> <li>Steepest Descent</li> <li>Conjugate Gradient 1</li> <li>Conjugate Gradient 2</li> <li>GMRES</li> <li>Preconditioning</li> </ul>"},{"location":"Iterative%20Methods/Classical_Iteration/","title":"Classical Iterative Methods","text":"<p>We cover three strategies of Classical Iterations: Jacobi, Gauss-Seidel and SOR(Successive Over-Relaxation) method.</p> <p><code>Task</code>: Solve linear system \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\).</p> <p><code>Strategy</code>: Iterate \\(\\boldsymbol{x}^{\\left( 0 \\right)}\\rightarrow \\boldsymbol{x}^{\\left( 1 \\right)}\\rightarrow \\boldsymbol{x}^{\\left( 2 \\right)}\\rightarrow \\cdots \\rightarrow \\boldsymbol{x}^{\\left( k \\right)}\\rightarrow \\boldsymbol{x}^{\\left( k+1 \\right)}\\rightarrow \\cdots\\)</p>"},{"location":"Iterative%20Methods/Classical_Iteration/#jacobi-iteration","title":"Jacobi Iteration","text":""},{"location":"Iterative%20Methods/Classical_Iteration/#component-wise-viewpoint","title":"Component-wise Viewpoint","text":"<p>Let us discuss the component-wise viewpoint (good for coding) of Jacobi Iteration.</p> <p>We examine \\(i\\) -th equation in \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\):</p> \\[ a_{i1}x_1+a_{i2}x_2+\\cdots +a_{ii}x_i+\\cdots +a_{im}x_m=b_i, \\] \\[ a_{ii}x_i+\\left( \\sum_{j=1}^{i-1}{a_{ij}x_j}+\\sum_{j=i+1}^m{a_{ij}x_j} \\right) =b_i \\] <p>Assume \\(x_j\\ (j\\ne i)\\) are known (as the current approximation), we have:</p> \\[ a_{ii}{x_i}^{\\left( k+1 \\right)}=b_i-\\left( \\sum_{j=1}^{i-1}{a_{ij}{x_j}^{\\left( k \\right)}}+\\sum_{j=i+1}^m{a_{ij}{x_j}^{\\left( k \\right)}} \\right)  \\] <p>If \\(a_{ii} \\ne 0\\), then:</p> \\[ {x_i}^{\\left( k+1 \\right)}=\\frac{1}{a_{ii}}\\left[ b_i-\\sum_{j\\ne i}{a_{ij}{x_j}^{\\left( k \\right)}} \\right] ; i=1,2,\\cdots ,m \\] <p>This is the formula for Jacobi Iteration.</p>"},{"location":"Iterative%20Methods/Classical_Iteration/#matrix-viewpoint","title":"Matrix Viewpoint","text":"<p>Let us now discuss the matrix viewpoint (good for analysis) of Jacobi Iteration.</p> <p>We split \\(\\boldsymbol{A}\\) as:</p> \\[ \\boldsymbol{A}=\\left[ \\begin{matrix}     \\ddots&amp;     &amp;       -\\boldsymbol{F}\\\\     &amp;       \\boldsymbol{D}&amp;     \\\\     -\\boldsymbol{E}&amp;        &amp;       \\ddots\\\\ \\end{matrix} \\right] =\\boldsymbol{D}-\\boldsymbol{E}-\\boldsymbol{F} \\] <p>where \\(\\boldsymbol{D}\\) is the diagonal part of \\(\\boldsymbol{A}\\), and \\(\\boldsymbol{E}\\) and \\(\\boldsymbol{F}\\) are the lower triangular part and upper triangular part of negative \\(\\boldsymbol{A}\\). Then:</p> \\[ \\boldsymbol{Ax}=\\boldsymbol{b}\\Longleftrightarrow \\left( \\boldsymbol{D}-\\boldsymbol{E}-\\boldsymbol{F} \\right) \\boldsymbol{x}=\\boldsymbol{b}; \\] \\[ \\boldsymbol{Dx}=\\boldsymbol{b}+\\left( \\boldsymbol{E}+\\boldsymbol{F} \\right) \\boldsymbol{x} \\] <p>We get:</p> \\[ \\boldsymbol{Dx}^{\\left( k+1 \\right)}=\\boldsymbol{b}+\\left( \\boldsymbol{E}+\\boldsymbol{F} \\right) \\boldsymbol{x}^{\\left( k \\right)} \\] <p>If \\(\\boldsymbol{D}\\) is invertible, we have:</p> \\[ \\boldsymbol{x}^{\\left( k+1 \\right)}=\\boldsymbol{D}^{-1}\\left[ \\boldsymbol{b}+\\left( \\boldsymbol{E}+\\boldsymbol{F} \\right) \\boldsymbol{x}^{\\left( k \\right)} \\right]  \\] <p>This is the matrix representation for Jacobi Iteration. The iterative matrix is \\(\\boldsymbol{M}_{\\mathrm{jacobi}}=\\boldsymbol{D}^{-1}\\left( \\boldsymbol{E}+\\boldsymbol{F} \\right)\\).</p>"},{"location":"Iterative%20Methods/Classical_Iteration/#gauss-seidel-iteration","title":"Gauss-Seidel Iteration","text":"<p>Using the most updated values for \\({x_j}^{\\left( k \\right)}\\), we have the algorithm below:</p> <p>For \\(i=1,2 \\cdots , m\\):</p> \\[ {x_i}^{\\left( k+1 \\right)}=\\frac{1}{a_{ii}}\\left[ b_i-\\left( \\sum_{j=1}^{i-1}{a_{ij}{x_j}^{\\left( k+1 \\right)}}+\\sum_{j=i+1}^m{a_{ij}{x_j}^{\\left( k \\right)}} \\right) \\right]  \\] <p>End</p> <p>This is Forward Gauss-Seidel Iteration.</p> <p>From:</p> \\[ \\boldsymbol{Ax}=\\boldsymbol{b}\\Longleftrightarrow \\left( \\boldsymbol{D}-\\boldsymbol{E} \\right) \\boldsymbol{x}=\\boldsymbol{b}+\\boldsymbol{Fx}; \\] \\[ \\boldsymbol{x}=\\left( \\boldsymbol{D}-\\boldsymbol{E} \\right) ^{-1}\\left( \\boldsymbol{b}+\\boldsymbol{Fx} \\right)  \\] <p>We get:</p> \\[ \\boldsymbol{x}^{\\left( k+1 \\right)}=\\left( \\boldsymbol{D}-\\boldsymbol{E} \\right) ^{-1}\\left( \\boldsymbol{b}+\\boldsymbol{Fx}^{\\left( k \\right)} \\right)  \\] <p>This is the matrix expression for Gauss-Seidel Iteration. The iterative matrix is \\(\\boldsymbol{M}_{\\mathrm{FGS}}=\\left( \\boldsymbol{D}-\\boldsymbol{E} \\right) ^{-1}\\boldsymbol{F}\\).</p> <p>We have another option (Backward Gauss-Seidel Iteration):</p> \\[ \\boldsymbol{x}^{\\left( k+1 \\right)}=\\left( \\boldsymbol{D}-\\boldsymbol{F} \\right) ^{-1}\\left( \\boldsymbol{b}+\\boldsymbol{Ex}^{\\left( k \\right)} \\right)  \\] <p>The iterative matrix for it is \\(\\boldsymbol{M}_{\\mathrm{BGS}}=\\left( \\boldsymbol{D}-\\boldsymbol{F} \\right) ^{-1}\\boldsymbol{E}\\). What is the component-wise formula for this option?</p> \\[ {x_i}^{\\left( k+1 \\right)}=\\frac{1}{a_{ii}}\\left[ b_i-\\left( \\sum_{j=1}^{i-1}{a_{ij}{x_j}^{\\left( k \\right)}}+\\sum_{j=i+1}^m{a_{ij}{x_j}^{\\left( k+1 \\right)}} \\right) \\right]  \\] <p><code>Note</code>: Symmetric Gauss-Seidel Iteration iterates \\(i=1,2,\\cdots ,m-1,m\\) , followed by \\(i=m,m-1,\\cdots ,2,1\\). A symmetric Gauss-Seidel iteration is a forward Gauss-Seidel Iteration followed by a backward Gauss-Seidel Iteration. The iterative matrix is \\(\\boldsymbol{M}_{\\mathrm{SGS}}=\\boldsymbol{M}_{\\mathrm{BGS}}\\cdot \\boldsymbol{M}_{\\mathrm{FGS}}=\\left( \\boldsymbol{D}-\\boldsymbol{F} \\right) ^{-1}\\boldsymbol{E}\\left( \\boldsymbol{D}-\\boldsymbol{E} \\right) ^{-1}\\boldsymbol{F}\\).</p>"},{"location":"Iterative%20Methods/Classical_Iteration/#successive-over-relaxation-sor","title":"Successive Over-Relaxation (SOR)","text":""},{"location":"Iterative%20Methods/Classical_Iteration/#introduction-to-sor","title":"Introduction to SOR","text":"<p>Pick \\(\\omega \\ne 0\\). For \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\), we have:</p> \\[ \\omega \\boldsymbol{Ax}=\\omega \\boldsymbol{b}; \\] \\[ \\omega \\boldsymbol{A}=\\left( \\boldsymbol{D}-\\omega \\boldsymbol{E} \\right) -\\left[ \\omega \\boldsymbol{F}+\\left( 1-\\omega \\right) \\boldsymbol{D} \\right] ; \\] \\[ \\boldsymbol{x}=\\left( \\boldsymbol{D}-\\omega \\boldsymbol{E} \\right) ^{-1}\\cdot \\left[ w\\boldsymbol{b}+\\left[ \\omega \\boldsymbol{F}+\\left( 1-\\omega \\right) \\boldsymbol{D} \\right] \\cdot \\boldsymbol{x} \\right]  \\] <p>Thus:</p> \\[ \\boldsymbol{x}^{\\left( k+1 \\right)}=\\left( \\boldsymbol{D}-\\omega \\boldsymbol{E} \\right) ^{-1}\\cdot \\left[ w\\boldsymbol{b}+\\left[ \\omega \\boldsymbol{F}+\\left( 1-\\omega \\right) \\boldsymbol{D} \\right] \\cdot \\boldsymbol{x}^{\\left( k \\right)} \\right]  \\] <p>This is the matrix representation of SOR iteration.</p> <p>What is the component-wise formula for SOR?</p> \\[ {x_i}^{\\left( k+1 \\right)}=\\left( 1-\\omega \\right) {x_i}^{\\left( k \\right)}+\\frac{\\omega}{a_{ii}}\\left( b_i-\\sum_{j&lt;i}{a_{ij}{x_j}^{\\left( k+1 \\right)}}-\\sum_{j&gt;i}{a_{ij}{x_j}^{\\left( k \\right)}} \\right) ; i=1,2,\\cdots ,m \\]"},{"location":"Iterative%20Methods/Classical_Iteration/#discussion-on-sor","title":"Discussion on SOR","text":"<p>Geometric explanation of SOR:</p> <p></p> \\[ \\boldsymbol{x}^{\\left( k+1 \\right)}=\\omega \\boldsymbol{z}^{\\left( k+1 \\right)}+\\left( 1-\\omega \\right) \\boldsymbol{x}^{\\left( k \\right)} \\] <p>When \\(\\omega \\in \\left( 1,2 \\right)\\), it is called Over-Relaxation.</p> <p>When \\(\\omega\\) is picked as the so-called optimal relaxation parameter, SOR outperforms Jacobi or Gauss-Seidel significantly.</p> \\[ \\omega _{\\mathrm{optimal}}=\\frac{2}{1+\\sqrt{1-\\rho ^2\\left( \\boldsymbol{G} \\right)}} \\] <p>where \\(\\boldsymbol{G}\\) is the iterative matrix of Jacobi Iteration, and \\(\\rho \\left( \\boldsymbol{G} \\right)\\) is the spectral radius of \\(\\boldsymbol{G}\\)</p> <p>This formula is rarely used. In practice, we use trial and error to find \\(\\omega\\).</p>"},{"location":"Iterative%20Methods/Conjugate_Gradient/","title":"Conjugate Gradient Method (CG)","text":""},{"location":"Iterative%20Methods/Conjugate_Gradient/#introduction-to-conjugate-gradient-algorithm","title":"Introduction to Conjugate Gradient Algorithm","text":"<p><code>Algorithm</code> ( Conjugate Gradient Method ):</p> <p>Compute \\(\\boldsymbol{r}^{\\left( 0 \\right)}=\\boldsymbol{b}-\\boldsymbol{Ax}^{\\left( 0 \\right)}\\), and set \\(\\boldsymbol{p}^{\\left( 0 \\right)}=\\boldsymbol{r}^{\\left( 0 \\right)}\\);</p> <p>For \\(j=0,1,\\cdots\\) until convergence, do:</p> <ul> <li>Step length: \\(\\alpha _j=\\frac{\\left&lt; \\boldsymbol{r}^{\\left( j \\right)},\\boldsymbol{r}^{\\left( j \\right)} \\right&gt;}{\\left&lt; \\boldsymbol{p}^{\\left( j \\right)},\\boldsymbol{Ap}^{\\left( j \\right)} \\right&gt;}\\);</li> <li>Approximate solution: \\(\\boldsymbol{x}^{\\left( j+1 \\right)}=\\boldsymbol{x}^{\\left( j \\right)}+\\alpha _j\\boldsymbol{p}^{\\left( j \\right)}\\);</li> <li>Residual: \\(\\boldsymbol{r}^{\\left( j+1 \\right)}=\\boldsymbol{r}^{\\left( j \\right)}-\\alpha _j\\boldsymbol{Ap}^{\\left( j \\right)}\\);</li> <li>Improvement this step: \\(\\tau _j=\\frac{\\left&lt; \\boldsymbol{r}^{\\left( j+1 \\right)},\\boldsymbol{r}^{\\left( j+1 \\right)} \\right&gt;}{\\left&lt; \\boldsymbol{r}^{\\left( j \\right)},\\boldsymbol{r}^{\\left( j \\right)} \\right&gt;}\\);</li> <li>Search direction: \\(\\boldsymbol{p}^{\\left( j+1 \\right)}=\\boldsymbol{r}^{\\left( j+1 \\right)}+\\tau _j\\boldsymbol{p}^{\\left( j \\right)}\\);</li> </ul> <p>End</p> <p>The most expensive part for computation of this algorithm is computing \\(\\boldsymbol{Ap}\\), whose cost is \\(O(m^2)\\). However, It is needed for only once per iteration!</p> <p><code>Question</code>: How can we derive the CG method?</p>"},{"location":"Iterative%20Methods/Conjugate_Gradient/#derivation-from-projection-method","title":"Derivation from Projection Method","text":"<p>Perform the projection method with \\(\\mathbf{K}=\\mathbf{L}=\\mathrm{span}\\left\\{ \\boldsymbol{r}^{\\left( j \\right)},\\boldsymbol{p}^{\\left( j-1 \\right)} \\right\\}\\):</p> \\[ \\boldsymbol{x}^{\\left( j+1 \\right)}=\\boldsymbol{x}^{\\left( j \\right)}+\\boldsymbol{\\delta }^{\\left( j \\right)} \\] <p>where \\(\\boldsymbol{\\delta }^{\\left( j \\right)}\\in \\mathbf{K}\\). Then:</p> \\[ \\boldsymbol{r}^{\\left( j+1 \\right)}=\\boldsymbol{b}-\\boldsymbol{Ax}^{\\left( j+1 \\right)} \\] <p>Based on the requirement of Projection Method:</p> \\[ \\boldsymbol{r}^{\\left( j+1 \\right)}\\bot \\mathbf{L}=\\mathrm{span}\\left\\{ \\boldsymbol{r}^{\\left( j \\right)},\\boldsymbol{p}^{\\left( j-1 \\right)} \\right\\} \\] \\[ \\Longrightarrow \\left&lt; \\boldsymbol{r}^{\\left( j+1 \\right)},\\boldsymbol{r}^{\\left( j \\right)} \\right&gt; =0, \\left&lt; \\boldsymbol{r}^{\\left( j+1 \\right)},\\boldsymbol{p}^{\\left( j-1 \\right)} \\right&gt; =0 \\] <p>Since \\(\\boldsymbol{\\delta }^{\\left( j \\right)}\\in \\mathbf{K}\\), we can write (in a strange way) \\(\\boldsymbol{\\delta }^{\\left( j \\right)}=\\alpha _j\\left( \\boldsymbol{r}^{\\left( j \\right)}+\\tau _{j-1}\\boldsymbol{p}^{\\left( j-1 \\right)} \\right)\\), where \\(\\boldsymbol{r}^{\\left( j \\right)}+\\tau _{j-1}\\boldsymbol{p}^{\\left( j-1 \\right)}\\) is the new search direction.</p> \\[ \\boldsymbol{p}^{\\left( j \\right)}=\\boldsymbol{r}^{\\left( j \\right)}+\\tau _{j-1}\\boldsymbol{p}^{\\left( j-1 \\right)} \\\\ \\Rightarrow \\boldsymbol{r}^{\\left( j \\right)}=\\boldsymbol{p}^{\\left( j \\right)}-\\tau _{j-1}\\boldsymbol{p}^{\\left( j-1 \\right)} \\] <p>where \\(\\alpha _j,\\tau _{j-1}\\) are to be determined.</p> <p><code>Claim</code>: We can find (How to prove?):</p> \\[ \\boldsymbol{r}^{\\left( j+1 \\right)}=\\boldsymbol{r}^{\\left( j \\right)}-\\alpha _j\\boldsymbol{Ap}^{\\left( j \\right)} \\] <p><code>Proof</code>:</p> \\[ \\boldsymbol{p}^{\\left( j \\right)}=\\boldsymbol{r}^{\\left( j \\right)}+\\tau _{j-1}\\boldsymbol{p}^{\\left( j-1 \\right)},\\Leftrightarrow \\boldsymbol{r}^{\\left( j \\right)}=\\boldsymbol{p}^{\\left( j \\right)}-\\tau _{j-1}\\boldsymbol{p}^{\\left( j-1 \\right)}; \\] <p>Then:</p> \\[ \\boldsymbol{r}^{\\left( j+1 \\right)}=\\boldsymbol{b}-\\boldsymbol{Ax}^{\\left( j+1 \\right)}=\\boldsymbol{b}-\\boldsymbol{A}\\left( \\boldsymbol{x}^{\\left( j \\right)}+\\boldsymbol{\\delta }^{\\left( j \\right)} \\right)  \\] \\[ =\\left( \\boldsymbol{b}-\\boldsymbol{Ax}^{\\left( j \\right)} \\right) -\\boldsymbol{A\\delta }^{\\left( j \\right)}=\\boldsymbol{r}^{\\left( j \\right)}-\\boldsymbol{A}\\delta ^{\\left( j \\right)} \\] \\[ =\\boldsymbol{r}^{\\left( j \\right)}-\\boldsymbol{A}\\alpha _j\\left( \\boldsymbol{r}^{\\left( j \\right)}+\\tau _{j-1}\\boldsymbol{p}^{\\left( j-1 \\right)} \\right)  \\] \\[ =\\boldsymbol{r}^{\\left( j \\right)}-\\alpha _j\\boldsymbol{A}\\left( \\boldsymbol{p}^{\\left( j \\right)}-\\tau _{j-1}\\boldsymbol{p}^{\\left( j-1 \\right)} \\right) -\\alpha _j\\tau _{j-1}\\boldsymbol{Ap}^{\\left( j-1 \\right)} \\] \\[ =\\boldsymbol{r}^{\\left( j \\right)}-\\alpha _j\\boldsymbol{Ap}^{\\left( j \\right)} \\] <p>End of proof.</p> <p>Because:</p> \\[ 0=\\left&lt; \\boldsymbol{r}^{\\left( j+1 \\right)},\\boldsymbol{r}^{\\left( j \\right)} \\right&gt; =\\left&lt; \\boldsymbol{r}^{\\left( j \\right)}-\\alpha _j\\boldsymbol{Ap}^{\\left( j \\right)},\\boldsymbol{r}^{\\left( j \\right)} \\right&gt; =\\left&lt; \\boldsymbol{r}^{\\left( j \\right)},\\boldsymbol{r}^{\\left( j \\right)} \\right&gt; -\\alpha _j\\left&lt; \\boldsymbol{r}^{\\left( j \\right)},\\boldsymbol{Ap}^{\\left( j \\right)} \\right&gt;  \\] <p>We have:</p> \\[ \\alpha _j=\\frac{\\left&lt; \\boldsymbol{r}^{\\left( j \\right)},\\boldsymbol{r}^{\\left( j \\right)} \\right&gt;}{\\left&lt; \\boldsymbol{r}^{\\left( j \\right)},\\boldsymbol{Ap}^{\\left( j \\right)} \\right&gt;} \\] <p>Also:</p> \\[ 0=\\left&lt; \\boldsymbol{r}^{\\left( j+1 \\right)},\\boldsymbol{p}^{\\left( j-1 \\right)} \\right&gt; \\Rightarrow \\tau _{j-1}=\\frac{\\left&lt; \\boldsymbol{r}^{\\left( j \\right)},\\boldsymbol{r}^{\\left( j \\right)} \\right&gt;}{\\left&lt; \\boldsymbol{r}^{\\left( j-1 \\right)},\\boldsymbol{r}^{\\left( j-1 \\right)} \\right&gt;} \\]"},{"location":"Iterative%20Methods/Conjugate_Gradient/#properties-of-conjugate-gradient-method","title":"Properties of Conjugate Gradient Method","text":"<p>It can be proved that CG satisfies the following properties:</p> <ul> <li>\\(\\left&lt; \\boldsymbol{r}^{\\left( j \\right)},\\boldsymbol{r}^{\\left( i \\right)} \\right&gt; =0, i\\ne j;\\)</li> <li>\\(\\left&lt; \\boldsymbol{Ap}^{\\left( j \\right)},\\boldsymbol{p}^{\\left( i \\right)} \\right&gt; =0,i\\ne j\\).</li> </ul> <p>Note that for \\(\\alpha _j\\), the formulas just derived here are slightly different from those described in the algorithm, but they are in fact equivalent. We can show:</p> \\[ \\alpha _j=\\frac{\\left&lt; \\boldsymbol{r}^{\\left( j \\right)},\\boldsymbol{r}^{\\left( j \\right)} \\right&gt;}{\\left&lt; \\boldsymbol{r}^{\\left( j \\right)},\\boldsymbol{Ap}^{\\left( j \\right)} \\right&gt;}=\\frac{\\left&lt; \\boldsymbol{r}^{\\left( j \\right)},\\boldsymbol{r}^{\\left( j \\right)} \\right&gt;}{\\left&lt; \\boldsymbol{p}^{\\left( j \\right)},\\boldsymbol{Ap}^{\\left( j \\right)} \\right&gt;}, \\] \\[ \\Longleftarrow \\left&lt; \\boldsymbol{r}^{\\left( j \\right)},\\boldsymbol{Ap}^{\\left( j \\right)} \\right&gt; =\\left&lt; \\boldsymbol{p}^{\\left( j \\right)},\\boldsymbol{Ap}^{\\left( j \\right)} \\right&gt; ; \\] \\[ \\left&lt; \\boldsymbol{r}^{\\left( j \\right)},\\boldsymbol{Ap}^{\\left( j \\right)} \\right&gt; =\\left&lt; \\boldsymbol{p}^{\\left( j \\right)}-\\tau _{j-1}\\boldsymbol{p}^{\\left( j-1 \\right)},\\boldsymbol{Ap}^{\\left( j \\right)} \\right&gt;  \\] \\[ =\\left&lt; \\boldsymbol{p}^{\\left( j \\right)},\\boldsymbol{Ap}^{\\left( j \\right)} \\right&gt; -\\tau _{j-1}\\left&lt; \\boldsymbol{p}^{\\left( j-1 \\right)},\\boldsymbol{Ap}^{\\left( j \\right)} \\right&gt;  \\] \\[ =\\left&lt; \\boldsymbol{p}^{\\left( j \\right)},\\boldsymbol{Ap}^{\\left( j \\right)} \\right&gt;  \\] <p>In this way we prove the equivalence of formulas.</p>"},{"location":"Iterative%20Methods/Conjugate_Gradient_2/","title":"Conjugate Gradient Method (Another Perspective)","text":""},{"location":"Iterative%20Methods/Conjugate_Gradient_2/#krylov-subspace","title":"Krylov Subspace","text":"<p>Let us introduce a different way to derive the CG method.</p> <p>Krylov Subspace: Given \\(\\boldsymbol{r}_0\\), let</p> \\[ \\mathbf{K}_n=\\mathrm{span}\\left\\{ \\boldsymbol{r}_0,\\boldsymbol{Ar}_0,\\boldsymbol{A}^2\\boldsymbol{r}_0,\\cdots ,\\boldsymbol{A}^{n-1}\\boldsymbol{r}_0 \\right\\} \\triangleq \\mathbf{K}_n\\left( \\boldsymbol{A},\\boldsymbol{r}_0 \\right)  \\] <p><code>Claim</code>: If we take \\(\\mathbf{L}_n=\\mathbf{K}_n\\), when \\(\\boldsymbol{A}\\) is SPD, by applying projection method, we obtain the CG algorithm.</p> <p>We need to have a basis for \\(\\mathbf{L}_n=\\mathbf{K}_n\\).</p> <p><code>Goal</code>: Construct an orthonormal basis for \\(\\mathbf{K}_n\\), denoted by \\(\\boldsymbol{V}_n\\).</p> <p>We can apply Modified Gram-Schmidt algorithm to find the orthonormal basis for the Krylov Subspace \\(\\mathbf{K}_n\\). This resulting algorithm is called Lancozs algorithm ( \\(\\boldsymbol{A}\\) is  Hermitian matrix ).</p> <p><code>Algorithm</code> ( Lancozs ):</p> <p>Choose an initial vector \\(\\boldsymbol{v}_1\\) with \\(\\left\\| \\boldsymbol{v}_1 \\right\\| =1\\);</p> <p>Set \\(\\beta _1=0\\);</p> <p>For \\(j=1,2,\\cdots ,n\\), do:</p> <ul> <li>\\(\\boldsymbol{w}_j=\\boldsymbol{Av}_j-\\beta _j\\boldsymbol{v}_{j-1}\\);</li> <li>\\(\\alpha _j=\\left&lt; \\boldsymbol{w}_j,\\boldsymbol{v}_j \\right&gt;\\);</li> <li>\\(\\boldsymbol{w}_j=\\boldsymbol{w}_j-\\alpha _j\\boldsymbol{v}_j\\);</li> <li>\\(\\beta _{j+1}=\\left\\| \\boldsymbol{w}_j \\right\\|\\); If \\(\\beta _{j+1}=0\\), stop;</li> <li>\\(\\boldsymbol{v}_{j+1}=\\frac{\\boldsymbol{w}_j}{\\beta _{j+1}}\\);</li> </ul> <p>End</p> <p>\\(\\left\\{ \\boldsymbol{v}_1,\\cdots ,\\boldsymbol{v}_n \\right\\}\\) can form an orthonormal basis for \\(\\mathbf{K}_n\\). Also:</p> \\[ \\beta _{j+1}\\boldsymbol{v}_{j+1}=\\boldsymbol{w}_j=\\boldsymbol{Av}_j-\\alpha _j\\boldsymbol{v}_j-\\beta _j\\boldsymbol{v}_{j-1}; \\] \\[ \\boldsymbol{Av}_j=\\beta _{j+1}\\boldsymbol{v}_{j+1}+\\alpha _j\\boldsymbol{v}_j+\\beta _j\\boldsymbol{v}_{j-1} \\] <p>Note that:</p> \\[ \\boldsymbol{V}_n=\\left[ \\begin{matrix}     \\boldsymbol{v}_1&amp;       \\cdots&amp;     \\boldsymbol{v}_n\\\\ \\end{matrix} \\right] , \\boldsymbol{AV}_n=\\boldsymbol{V}_n\\boldsymbol{T}_n; \\] <p>where:</p> \\[ \\boldsymbol{T}_n=\\left[ \\begin{matrix}     \\alpha _1&amp;      \\beta _2&amp;       &amp;       &amp;       &amp;       \\boldsymbol{O}\\\\     \\beta _2&amp;       \\alpha _2&amp;      \\beta _3&amp;       &amp;       &amp;       \\\\     &amp;       \\beta _3&amp;       \\alpha _3&amp;      \\beta _4&amp;       &amp;       \\\\     &amp;       &amp;       \\ddots&amp;     \\ddots&amp;     \\ddots&amp;     \\\\     &amp;       &amp;       &amp;       \\ddots&amp;     \\ddots&amp;     \\beta _n\\\\     \\boldsymbol{O}&amp;     &amp;       &amp;       &amp;       \\beta _n&amp;       \\alpha _n\\\\ \\end{matrix} \\right]  \\] <p>We know that:</p> \\[ {\\boldsymbol{V}_n}^T\\boldsymbol{AV}_n=\\boldsymbol{T}_n \\] <p>Apply Projection Method:</p> \\[ \\begin{cases}     \\boldsymbol{x}^{\\left( n \\right)}=\\boldsymbol{x}^{\\left( 0 \\right)}+\\boldsymbol{V}_n\\boldsymbol{y}^{\\left( n \\right)}\\\\     \\boldsymbol{y}^{\\left( n \\right)}=\\left( {\\boldsymbol{V}_n}^T\\boldsymbol{AV}_n \\right) ^{-1}{\\boldsymbol{V}_n}^T\\boldsymbol{r}^{\\left( 0 \\right)}\\\\ \\end{cases}; \\] <p>Then:</p> \\[ \\boldsymbol{y}^{\\left( n \\right)}=\\left( \\boldsymbol{T}_n \\right) ^{-1}\\beta _1\\boldsymbol{e}_1, \\beta _1=\\left\\| \\boldsymbol{r}^{\\left( 0 \\right)} \\right\\|  \\] <p>We now apply LU factorization to find \\(\\left( \\boldsymbol{T}_n \\right) ^{-1}\\):</p> \\[ \\boldsymbol{T}_n=\\boldsymbol{L}_n\\boldsymbol{U}_n,  \\] \\[ \\boldsymbol{L}_n=\\left[ \\begin{matrix}     1&amp;      &amp;       &amp;       \\boldsymbol{O}\\\\     \\lambda _2&amp;     \\ddots&amp;     &amp;       \\\\     &amp;       \\ddots&amp;     \\ddots&amp;     \\\\     \\boldsymbol{O}&amp;     &amp;       \\lambda _n&amp;     1\\\\ \\end{matrix} \\right] , \\boldsymbol{U}_n=\\left[ \\begin{matrix}     \\eta _1&amp;        \\beta _2&amp;       &amp;       \\boldsymbol{O}\\\\     &amp;       \\ddots&amp;     \\ddots&amp;     \\\\     &amp;       &amp;       \\ddots&amp;     \\beta _n\\\\     \\boldsymbol{O}&amp;     &amp;       &amp;       \\eta _n\\\\ \\end{matrix} \\right] ; \\] \\[ \\boldsymbol{x}^{\\left( n \\right)}=\\boldsymbol{x}^{\\left( 0 \\right)}+\\boldsymbol{V}_n\\boldsymbol{y}^{\\left( n \\right)}=\\boldsymbol{x}^{\\left( 0 \\right)}+\\boldsymbol{V}_n\\left( \\boldsymbol{T}_n \\right) ^{-1}\\beta _1\\boldsymbol{e}_1 \\] \\[ =\\boldsymbol{x}^{\\left( 0 \\right)}+\\underset{\\boldsymbol{P}_n}{\\underbrace{\\boldsymbol{V}_n\\left( \\boldsymbol{U}_n \\right) ^{-1}}}\\cdot \\underset{\\boldsymbol{z}_n}{\\underbrace{\\left( \\boldsymbol{L}_n \\right) ^{-1}\\beta _1\\boldsymbol{e}_1}} \\] \\[ =\\boldsymbol{x}^{\\left( 0 \\right)}+\\boldsymbol{P}_n\\boldsymbol{z}_n=\\boldsymbol{x}^{\\left( 0 \\right)}+\\boldsymbol{P}_{n-1}\\boldsymbol{z}_{n-1}+\\xi _n\\boldsymbol{p}_n \\] \\[ =\\boldsymbol{x}^{\\left( n-1 \\right)}+\\xi _n\\boldsymbol{p}_n \\] <p>We need to determine \\(\\xi _n\\boldsymbol{p}_n\\) such that \\(\\boldsymbol{x}^{\\left( 0 \\right)}\\) is the solution of Projection Method.</p> <p>Summarize what we have got so far:</p> \\[ \\boldsymbol{x}^{\\left( n \\right)}=\\boldsymbol{x}^{\\left( 0 \\right)}+\\boldsymbol{P}_n\\boldsymbol{z}_n, \\] \\[ \\boldsymbol{P}_n=\\boldsymbol{V}_n{\\boldsymbol{U}_n}^{-1}, \\boldsymbol{z}_n={\\boldsymbol{L}_n}^{-1}\\beta _1\\boldsymbol{e}_1; \\] \\[ \\boldsymbol{P}_n=\\left[ \\begin{matrix}     \\boldsymbol{p}_1&amp;       \\boldsymbol{p}_2&amp;       \\cdots&amp;     \\boldsymbol{p}_n\\\\ \\end{matrix} \\right] , \\] \\[ \\boldsymbol{x}_n=\\boldsymbol{x}^{\\left( 0 \\right)}+\\boldsymbol{P}_{n-1}\\boldsymbol{z}_{n-1}+\\xi _n\\boldsymbol{p}_n=\\boldsymbol{x}^{\\left( n-1 \\right)}+\\xi _n\\boldsymbol{p}_n \\] <p>Note that:</p> \\[ \\boldsymbol{P}_n=\\boldsymbol{V}_n{\\boldsymbol{U}_n}^{-1}\\Leftrightarrow \\boldsymbol{P}_n\\boldsymbol{U}_n=\\boldsymbol{V}_n, \\] \\[ \\left[ \\begin{matrix}     \\boldsymbol{p}_1&amp;       \\boldsymbol{p}_2&amp;       \\cdots&amp;     \\boldsymbol{p}_n\\\\ \\end{matrix} \\right] \\cdot \\left[ \\begin{matrix}     \\eta _1&amp;        \\beta _2&amp;       0&amp;      \\cdots&amp;     \\boldsymbol{O}\\\\     0&amp;      \\eta _2&amp;        \\beta _3&amp;       \\ddots&amp;     \\vdots\\\\     &amp;       \\ddots&amp;     \\ddots&amp;     \\ddots&amp;     0\\\\     &amp;       &amp;       \\ddots&amp;     \\ddots&amp;     \\beta _n\\\\     \\boldsymbol{O}&amp;     &amp;       &amp;       0&amp;      \\eta _n\\\\ \\end{matrix} \\right] =\\left[ \\begin{matrix}     \\boldsymbol{v}_1&amp;       \\boldsymbol{v}_2&amp;       \\cdots&amp;     \\boldsymbol{v}_n\\\\ \\end{matrix} \\right] , \\] \\[ \\boldsymbol{v}_n=\\beta _n\\boldsymbol{p}_{n-1}+\\eta _n\\boldsymbol{p}_n \\] \\[ \\Rightarrow \\boldsymbol{p}_n=\\frac{1}{\\eta _n}\\left( \\boldsymbol{v}_n-\\beta _n\\boldsymbol{p}_{n-1} \\right)  \\] <p><code>Claim</code>: \\(\\boldsymbol{v}_n\\parallel \\boldsymbol{r}_n\\). (Why?  Prove by induction)</p> <p>Then:</p> \\[ \\boldsymbol{p}_n=\\frac{1}{\\eta _n}\\left( \\gamma _n\\boldsymbol{r}_n-\\beta _n\\boldsymbol{p}_{n-1} \\right)  \\] <p>At this point, this projection method becomes CG algorithm.</p>"},{"location":"Iterative%20Methods/Conjugate_Gradient_2/#properties-of-conjugate-gradient-method","title":"Properties of Conjugate Gradient Method","text":""},{"location":"Iterative%20Methods/Conjugate_Gradient_2/#basic-property","title":"Basic Property","text":"<p><code>Theorem</code>: In CG method,</p> <ul> <li>\\(\\left&lt; \\boldsymbol{r}^{\\left( j \\right)},\\boldsymbol{r}^{\\left( i \\right)} \\right&gt; =0, i\\ne j\\);</li> <li>\\(\\left&lt; \\boldsymbol{Ap}^{\\left( j \\right)},\\boldsymbol{p}^{\\left( i \\right)} \\right&gt; =0,i\\ne j\\).</li> </ul> <p><code>Proof</code>: Note that:</p> \\[ \\boldsymbol{P}_n=\\boldsymbol{V}_n{\\boldsymbol{U}_n}^{-1}, \\] \\[ {\\boldsymbol{P}_n}^T\\boldsymbol{AP}_n=\\left( {\\boldsymbol{U}_n}^{-1} \\right) ^T{\\boldsymbol{V}_n}^T\\boldsymbol{AV}_n{\\boldsymbol{U}_n}^{-1} \\] \\[ =\\left( {\\boldsymbol{U}_n}^{-1} \\right) ^T\\boldsymbol{T}_n{\\boldsymbol{U}_n}^{-1}=\\left( {\\boldsymbol{U}_n}^{-1} \\right) ^T\\boldsymbol{L}_n\\boldsymbol{U}_n{\\boldsymbol{U}_n}^{-1}=\\left( {\\boldsymbol{U}_n}^{-1} \\right) ^T\\boldsymbol{L}_n \\] <p>Since \\(\\left( {\\boldsymbol{U}_n}^{-1} \\right) ^T\\boldsymbol{L}_n\\) is lower triangular, and \\({\\boldsymbol{P}_n}^T\\boldsymbol{AP}_n\\) is symmetric, then we know that \\({\\boldsymbol{P}_n}^T\\boldsymbol{AP}_n\\) is diagonal. Therefore,</p> \\[ {\\boldsymbol{p}_i}^T\\boldsymbol{Ap}_j=0, i\\ne j\\Longleftrightarrow \\left&lt; \\boldsymbol{Ap}^{\\left( j \\right)},\\boldsymbol{p}^{\\left( i \\right)} \\right&gt; =0,i\\ne j \\] <p>End of proof.</p> <p>We call \\(\\boldsymbol{p}^{\\left( j \\right)},\\boldsymbol{p}^{\\left( i \\right)}\\) conjugate to each other.</p> <p><code>Theorem</code>: Assume \\(\\boldsymbol{A}\\) is SPD, \\(\\mathbf{L}=\\mathbf{K}\\), then a vector \\(\\boldsymbol{x}\\) is the result of a projection method onto \\(\\mathbf{K}\\), orthogonal to \\(\\mathbf{L}\\) with initial guess \\(\\boldsymbol{x}^{(0)}\\) if and only if \\(\\boldsymbol{x}\\) minimizes the \\(\\boldsymbol{A}\\)-norm of the error over \\(\\boldsymbol{x}^{\\left( 0 \\right)}+\\mathbf{K}\\), i.e.,</p> \\[ E\\left( \\boldsymbol{y} \\right) =\\left\\| \\boldsymbol{x}^*-\\boldsymbol{y} \\right\\| _{\\boldsymbol{A}}^{2}=\\left( \\boldsymbol{x}^*-\\boldsymbol{y} \\right) ^T\\boldsymbol{A}\\left( \\boldsymbol{x}^*-\\boldsymbol{y} \\right)  \\] <p>where \\(\\boldsymbol{x}^*\\) is the true solution of \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\), and:</p> \\[ \\boldsymbol{x}=\\mathrm{arg}\\min E\\left( \\boldsymbol{y} \\right) , \\boldsymbol{y}\\in \\boldsymbol{x}^{\\left( 0 \\right)}+\\mathbf{K} \\] <p><code>Proof</code> (idea): To show that</p> \\[ \\frac{\\partial E}{\\partial \\boldsymbol{y}}\\mid_{\\boldsymbol{y}=\\boldsymbol{x}}^{}=\\boldsymbol{v}^T\\boldsymbol{r}=0,\\forall \\boldsymbol{v}\\in \\mathbf{K}=\\mathbf{L}, \\boldsymbol{r}=\\boldsymbol{b}-\\boldsymbol{Ax} \\]"},{"location":"Iterative%20Methods/Conjugate_Gradient_2/#convergence-rate","title":"Convergence Rate","text":"<p><code>Theorem</code>: In exact mathematics, CG method converges in at most \\(m\\) steps if \\(\\boldsymbol{A}\\) is SPD in \\(\\mathbb{R} ^{m\\times m}\\).</p> <p><code>Theorem</code>: In general, CG method converges according to the following estimate ( \\(\\boldsymbol{A}\\) is SPD):</p> \\[ \\left\\| \\boldsymbol{x}^{\\left( n \\right)}-\\boldsymbol{x}^{\\left( * \\right)} \\right\\| _2\\leqslant \\left( \\frac{\\sqrt{\\kappa \\left( \\boldsymbol{A} \\right)}-1}{\\sqrt{\\kappa \\left( \\boldsymbol{A} \\right)}+1} \\right) ^n\\cdot \\left\\| \\boldsymbol{x}^{\\left( 0 \\right)}-\\boldsymbol{x}^{\\left( * \\right)} \\right\\| _2 \\] <p><code>Theorem</code>: The Steepest Descent Method converges with the following estimate ( \\(\\boldsymbol{A}\\) is SPD):</p> \\[ \\left\\| \\boldsymbol{x}^{\\left( n \\right)}-\\boldsymbol{x}^{\\left( * \\right)} \\right\\| _{\\boldsymbol{A}}\\leqslant \\left( \\frac{\\kappa \\left( \\boldsymbol{A} \\right) -1}{\\kappa \\left( \\boldsymbol{A} \\right) +1} \\right) ^n\\cdot \\left\\| \\boldsymbol{x}^{\\left( 0 \\right)}-\\boldsymbol{x}^{\\left( * \\right)} \\right\\| _{\\boldsymbol{A}} \\] <p>The difference between CG and Steepest Descent Method is significant.</p> <p><code>Example</code>: If \\(\\kappa \\left( \\boldsymbol{A} \\right) =999\\), then:</p> \\[ \\frac{\\kappa \\left( \\boldsymbol{A} \\right) -1}{\\kappa \\left( \\boldsymbol{A} \\right) +1}=0.998, \\] \\[ n=100: \\left( 0.998 \\right) ^{100}\\approx 1-100\\times 0.002=0.8; \\] \\[ \\frac{\\sqrt{\\kappa \\left( \\boldsymbol{A} \\right)}-1}{\\sqrt{\\kappa \\left( \\boldsymbol{A} \\right)}+1}=0.93, \\] \\[ n=0.8100: \\left( 0.93 \\right) ^{100}\\approx 7\\times 10^{-4} \\] <p>Preconditioning Techniques: Reduce the condition number of a linear system, so that the convergence is faster.</p>"},{"location":"Iterative%20Methods/GMRES/","title":"Generalized Minimal Residual Method (GMRES)","text":"<p>The Generalized Minimal Residual Method (GMRES) is an iterative method for the numerical solution of an indefinite non-symmetric system of linear equations.</p>"},{"location":"Iterative%20Methods/GMRES/#arnoldi-method","title":"Arnoldi Method","text":"<p>We apply Krylov Subspace based Projection Method to non-symmetric matrices. Pick:</p> \\[ \\boldsymbol{A}\\mathbf{K}=\\mathbf{L} \\] <p>where \\(\\mathbf{K}=\\mathbf{K}_n\\) is Krylov Subspace:</p> \\[ \\mathbf{K}_n=\\mathrm{span}\\left\\{ \\boldsymbol{r}^{\\left( 0 \\right)},\\boldsymbol{Ar}^{\\left( 0 \\right)},\\boldsymbol{A}^2\\boldsymbol{r}^{\\left( 0 \\right)},\\cdots ,\\boldsymbol{A}^{n-1}\\boldsymbol{r}^{\\left( 0 \\right)} \\right\\}  \\] <p>and:</p> \\[ \\mathbf{L}=\\boldsymbol{A}\\mathbf{K}_n=\\mathrm{span}\\left\\{ \\boldsymbol{Ar}^{\\left( 0 \\right)},\\boldsymbol{A}^2\\boldsymbol{r}^{\\left( 0 \\right)},\\boldsymbol{A}^3\\boldsymbol{r}^{\\left( 0 \\right)},\\cdots ,\\boldsymbol{A}^n\\boldsymbol{r}^{\\left( 0 \\right)} \\right\\}  \\] <p>Now apply modified Gram-Schmidt to Krylov Subspace. If \\(\\boldsymbol{A}\\) is non-symmetric, this process is called Arnoldi Procedure. If \\(\\boldsymbol{A}\\) is SPD, this process is called Lancozs Method.</p> <p><code>Algorithm</code> ( Arnoldi Procedure ):</p> <p>\\(\\boldsymbol{r}^{\\left( 0 \\right)}\\ne 0\\) is an arbitrary vector;</p> <p>\\(\\boldsymbol{q}_1=\\frac{\\boldsymbol{r}^{\\left( 0 \\right)}}{\\left\\| \\boldsymbol{r}^{\\left( 0 \\right)} \\right\\| _2}\\);</p> <p>For \\(n=1,2,3,\\cdots\\):</p> <ul> <li>\\(\\boldsymbol{v}=\\boldsymbol{Aq}_n\\);</li> <li>For \\(j=1,2,\\cdots ,n\\):<ul> <li>\\(h_{jn}=\\left&lt; \\boldsymbol{q}_j,\\boldsymbol{v} \\right&gt;\\);</li> <li>\\(\\boldsymbol{v}=\\boldsymbol{v}-h_{jn}\\boldsymbol{q}_j\\);</li> </ul> </li> <li>End;</li> <li>\\(h_{n+1,n}=\\left\\| \\boldsymbol{v} \\right\\|\\);</li> <li>\\(\\boldsymbol{q}_{n+1}=\\frac{\\boldsymbol{v}}{h_{n+1,n}}\\);</li> </ul> <p>End</p> <p>Arnoldi Procedure creates:</p> \\[ \\boldsymbol{Q}_n=\\left[ \\begin{matrix}     \\boldsymbol{q}_1&amp;       \\boldsymbol{q}_2&amp;       \\cdots&amp;     \\boldsymbol{q}_n\\\\ \\end{matrix} \\right] , \\boldsymbol{H}_n=\\left[ h_{ij} \\right] ; \\] \\[ \\Longrightarrow {\\boldsymbol{Q}_n}^T\\boldsymbol{AQ}_n=\\boldsymbol{H}_n \\] <p>Also denote:</p> \\[ \\overline{\\boldsymbol{H}}_n=\\left[ \\begin{matrix}     \\boldsymbol{H}_n&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     h_{n+1,n}\\\\ \\end{matrix} \\right]  \\] <p>Then we can verify:</p> \\[ \\boldsymbol{AQ}_n=\\boldsymbol{Q}_{n+1}\\overline{\\boldsymbol{H}}_n \\]"},{"location":"Iterative%20Methods/GMRES/#introduction-to-gmres","title":"Introduction to GMRES","text":""},{"location":"Iterative%20Methods/GMRES/#derivation","title":"Derivation","text":"<p><code>Theorem</code>: Let \\(\\boldsymbol{A}\\) be a square nonsingular matrix. \\(\\mathbf{L}=\\boldsymbol{A}\\mathbf{K}\\). The vector \\(\\boldsymbol{x}\\prime\\) is the result of the Projection Method onto \\(\\mathbf{K}\\), orthogonal to \\(\\mathbf{L}\\) with the starting point \\(\\boldsymbol{x}^{\\left( 0 \\right)}\\) if and only if \\(\\boldsymbol{x}\\prime\\) minimizes the 2-norm of the residual vector over \\(\\boldsymbol{x}\\in \\boldsymbol{x}^{\\left( 0 \\right)}+\\mathbf{K}\\). i.e., define:</p> \\[ R\\left( \\boldsymbol{x} \\right) =\\left\\| \\boldsymbol{b}-\\boldsymbol{Ax} \\right\\| _2 \\] \\[ \\Rightarrow \\boldsymbol{x}\\prime=\\mathrm{arg}\\min R\\left( \\boldsymbol{x} \\right) , \\boldsymbol{x}\\in \\boldsymbol{x}^{\\left( 0 \\right)}+\\mathbf{K} \\] <p>Based on the argument above, we can do a quick derivation of GMRES. Now select:</p> \\[ \\mathbf{K}=\\mathbf{K}_n,\\mathbf{L}=\\boldsymbol{A}\\mathbf{K}_n \\] <p>And \\(\\left\\{ \\boldsymbol{q}_1,\\cdots ,\\boldsymbol{q}_n \\right\\}\\) is an orthonormal basis for \\(\\mathbf{K}\\). Also \\(\\boldsymbol{r}^{\\left( n \\right)}=\\boldsymbol{b}-\\boldsymbol{Ax}^{\\left( n \\right)}\\).</p> <p>The solution of Projection Method:</p> \\[ \\boldsymbol{x}^{\\left( n \\right)}=\\mathrm{arg}\\min_{\\boldsymbol{x}\\in \\boldsymbol{x}^{\\left( 0 \\right)}+\\mathbf{K}} \\left\\| \\boldsymbol{b}-\\boldsymbol{Ax} \\right\\| _2 \\] \\[ \\boldsymbol{r}^{\\left( n \\right)}\\bot \\mathbf{L}, \\] \\[ \\boldsymbol{x}^{\\left( n \\right)}\\in \\boldsymbol{x}^{\\left( 0 \\right)}+\\mathbf{K}\\Longrightarrow \\boldsymbol{x}^{\\left( n \\right)}=\\boldsymbol{x}^{\\left( 0 \\right)}+\\boldsymbol{Q}_n\\boldsymbol{y},\\ \\boldsymbol{y}\\in \\mathbb{R} ^n \\] <p>Then:</p> \\[ \\left\\| \\boldsymbol{b}-\\boldsymbol{Ax} \\right\\| _2=\\left\\| \\boldsymbol{b}-\\boldsymbol{A}\\left( \\boldsymbol{x}^{\\left( 0 \\right)}+\\boldsymbol{Q}_n\\boldsymbol{y} \\right) \\right\\| _2 \\] \\[ =\\left\\| \\left( \\boldsymbol{b}-\\boldsymbol{Ax}^{\\left( 0 \\right)} \\right) -\\boldsymbol{AQ}_n\\boldsymbol{y} \\right\\| _2=\\left\\| \\boldsymbol{r}^{\\left( 0 \\right)}-\\boldsymbol{AQ}_n\\boldsymbol{y} \\right\\| _2 \\] \\[ =\\left\\| \\beta \\boldsymbol{q}_1-\\boldsymbol{Q}_{n+1}\\overline{\\boldsymbol{H}}_n\\boldsymbol{y} \\right\\| _2 \\] \\[ =\\left\\| \\boldsymbol{Q}_{n+1}\\left( \\beta \\boldsymbol{e}_1-\\overline{\\boldsymbol{H}}_n\\boldsymbol{y} \\right) \\right\\| _2=\\left\\| \\beta \\boldsymbol{e}_1-\\overline{\\boldsymbol{H}}_n\\boldsymbol{y} \\right\\| _2 \\] <p>We can find:</p> \\[ \\min_{\\boldsymbol{x}\\in \\boldsymbol{x}^{\\left( 0 \\right)}+\\mathbf{K}} \\left\\| \\boldsymbol{b}-\\boldsymbol{Ax} \\right\\| _2=\\min_{\\boldsymbol{y}\\in \\mathbb{R} ^n} \\left\\| \\beta \\boldsymbol{e}_1-\\overline{\\boldsymbol{H}}_n\\boldsymbol{y} \\right\\| _2 \\] <p>Therefore, the idea of GMRES algorithm is to find the least square solution to:</p> \\[ \\min_{\\boldsymbol{y}\\in \\mathbb{R} ^n} \\left\\| \\beta \\boldsymbol{e}_1-\\overline{\\boldsymbol{H}}_n\\boldsymbol{y} \\right\\| _2 \\] <p>for \\(\\boldsymbol{y}\\), and update:</p> \\[ \\boldsymbol{x}^{\\left( n \\right)}=\\boldsymbol{x}^{\\left( 0 \\right)}+\\boldsymbol{Q}_n\\boldsymbol{y} \\]"},{"location":"Iterative%20Methods/GMRES/#algorithm-steps","title":"Algorithm Steps","text":"<p><code>Algorithm</code> ( GMRES ):</p> <p>Compute \\(\\boldsymbol{r}^{\\left( 0 \\right)}=\\boldsymbol{b}-\\boldsymbol{Ax}^{\\left( 0 \\right)}, \\beta =\\left\\| \\boldsymbol{r}^{\\left( 0 \\right)} \\right\\| , \\boldsymbol{q}_1=\\frac{\\boldsymbol{r}^{\\left( 0 \\right)}}{\\beta}\\);</p> <p>Define a \\((n+1)\\times n\\) matrix \\(\\overline{\\boldsymbol{H}}_n=\\left\\{ h_{ij} \\right\\} _{\\left( n+1 \\right) \\times n}\\), and set \\(\\overline{\\boldsymbol{H}}_n=\\boldsymbol{O}\\);</p> <p>For \\(j=1,2,\\cdots ,n\\):</p> <ul> <li>\\(\\boldsymbol{w}_j=\\boldsymbol{Aq}_j\\);</li> <li>For \\(i=1,2,\\cdots ,j\\):<ul> <li>\\(h_{ij}=\\left&lt; \\boldsymbol{w}_j,\\boldsymbol{q}_i \\right&gt;\\);</li> <li>\\(\\boldsymbol{w}_j=\\boldsymbol{w}_j-h_{ij}\\boldsymbol{q}_i\\);</li> </ul> </li> <li>End;</li> <li>\\(h_{j+1,j}=\\left\\| \\boldsymbol{w}_j \\right\\| _2\\);</li> <li>\\(\\boldsymbol{q}_{j+1}=\\frac{\\boldsymbol{w}_j}{h_{j+1,j}}\\);</li> </ul> <p>End;</p> <p>Compute \\(\\boldsymbol{y}_n\\) to minimize \\(\\left\\| \\beta \\boldsymbol{e}_1-\\overline{\\boldsymbol{H}}_n\\boldsymbol{y} \\right\\| _2\\);</p> <p>Set \\(\\boldsymbol{x}^{\\left( n \\right)}=\\boldsymbol{x}^{\\left( 0 \\right)}+\\boldsymbol{Q}_n\\boldsymbol{y}_n\\);</p> <p>End of procedure.</p> <p><code>Note</code>: If \\(h_{j+1,j} =0\\), break! (This is a lucky breakdown.) In this case, the solution is in \\(\\boldsymbol{x}^{\\left( 0 \\right)}+\\mathbf{K}\\).</p>"},{"location":"Iterative%20Methods/GMRES/#further-discussion","title":"Further Discussion","text":"<p>In practice, restarting GMRES can avoid large \\(n\\).</p> <p>Theoretically, GMRES converges quickly if \\(\\kappa \\left( \\boldsymbol{V} \\right)\\) is not loo large, where \\(\\boldsymbol{V}\\) is the eigenmatrix of \\(\\boldsymbol{A}\\).</p> <p>Bi-CG, CGN are other similar iterative methods.</p>"},{"location":"Iterative%20Methods/Introduction_to_Iteration/","title":"General Strategy of Iteration Methods","text":"<p>We will discuss the general Splitting Strategy.</p>"},{"location":"Iterative%20Methods/Introduction_to_Iteration/#general-splitting-strategy","title":"General Splitting Strategy","text":"<p>For linear system \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\), let \\(\\boldsymbol{A}=\\boldsymbol{M}-\\boldsymbol{N}\\). Then:</p> \\[ \\boldsymbol{Ax}=\\boldsymbol{b}\\,\\,\\Longleftrightarrow \\left( \\boldsymbol{M}-\\boldsymbol{N} \\right) \\boldsymbol{x}=\\boldsymbol{b}\\,\\,\\Longleftrightarrow \\boldsymbol{Mx}=\\boldsymbol{b}+\\boldsymbol{Nx} \\] <p>We can have the iteration:</p> \\[ \\boldsymbol{Mx}^{\\left( k+1 \\right)}=\\boldsymbol{Nx}^{\\left( k \\right)}+\\boldsymbol{b} \\] <p>If \\(\\boldsymbol{M}\\) is invertible, we have:</p> \\[ \\boldsymbol{x}^{\\left( k+1 \\right)}=\\boldsymbol{M}^{-1}\\boldsymbol{Nx}^{\\left( k \\right)}+\\boldsymbol{M}^{-1}\\boldsymbol{b} \\] <p>Denote \\(\\boldsymbol{M}^{-1}\\boldsymbol{b}=\\boldsymbol{f}, \\boldsymbol{M}^{-1}\\boldsymbol{N}=\\boldsymbol{G}\\), we get:</p> \\[ \\boldsymbol{x}^{\\left( k+1 \\right)}=\\boldsymbol{Gx}^{\\left( k \\right)}+\\boldsymbol{f} \\] <p>where \\(\\boldsymbol{G}\\) is called iterative matrix.</p> <p>For example, Jacobi method: \\(\\boldsymbol{M}=\\boldsymbol{D}, \\boldsymbol{N}=\\boldsymbol{E}+\\boldsymbol{F}\\).</p>"},{"location":"Iterative%20Methods/Introduction_to_Iteration/#speed-of-convergence","title":"Speed of Convergence","text":"<p><code>Question</code>: Is it convergent? If so, how fast?</p> <p>If \\(\\boldsymbol{x}^*\\) is a solution of \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\), then:</p> \\[ \\boldsymbol{Ax}^*=\\boldsymbol{b}\\,\\,\\Longleftrightarrow \\left( \\boldsymbol{M}-\\boldsymbol{N} \\right) \\boldsymbol{x}^*=\\boldsymbol{b}\\Longleftrightarrow \\boldsymbol{Mx}^*=\\boldsymbol{Nx}^*+\\boldsymbol{b}; \\] \\[ \\boldsymbol{x}^*=\\boldsymbol{M}^{-1}\\boldsymbol{Nx}^*+\\boldsymbol{M}^{-1}\\boldsymbol{b}=\\boldsymbol{Gx}^*+\\boldsymbol{f}; \\] \\[ \\boldsymbol{x}^*=\\boldsymbol{Gx}^*+\\boldsymbol{f} \\] <p>Therefore, \\(\\boldsymbol{x}^*\\) is a fixed point of the iteration.</p> <p>Note that \\(\\boldsymbol{x}^{\\left( k+1 \\right)}=\\boldsymbol{Gx}^{\\left( k \\right)}+\\boldsymbol{f}\\), then:</p> \\[ \\boldsymbol{x}^{\\left( k+1 \\right)}-\\boldsymbol{x}^*=\\boldsymbol{G}\\left( \\boldsymbol{x}^{\\left( k \\right)}-\\boldsymbol{x}^* \\right)  \\] <p>Define \\(\\boldsymbol{e}^{\\left( k \\right)}=\\boldsymbol{x}^{\\left( k+1 \\right)}-\\boldsymbol{x}^*\\). We get:</p> \\[ \\boldsymbol{e}^{\\left( k+1 \\right)}=\\boldsymbol{Ge}^{\\left( k \\right)} \\] <p>This is the error equation.</p>"},{"location":"Iterative%20Methods/Introduction_to_Iteration/#method-1","title":"Method 1","text":"<p><code>Question</code>: Is \\(\\boldsymbol{e}^{\\left( k \\right)}\\rightarrow 0\\) as \\(k\\rightarrow +\\infty\\) ? If so, how fast?</p> <p><code>Theorem</code>: If the spectral radius \\(\\rho \\left( \\boldsymbol{G} \\right) &lt;1\\), it implies that \\((\\mathbf{I}-\\boldsymbol{G})\\) is invertible and \\(\\boldsymbol{x}^{\\left( k \\right)}\\rightarrow \\boldsymbol{x}^*\\).</p> <p>The inverse is also true: If \\(\\boldsymbol{x}^{\\left( k \\right)}\\) converges for any \\(\\boldsymbol{f}\\) and \\(\\boldsymbol{x}^{(0)}\\), then \\(\\rho \\left( \\boldsymbol{G} \\right) &lt;1\\). (How to prove?)</p> <p><code>Remarks</code>: For an iterative method, if it is convergent:</p> <ol> <li>\\(\\boldsymbol{b}\\) can be arbitrary;</li> <li>\\(\\boldsymbol{x}^{(0)}\\) can be arbitrary;</li> <li>The method always converges to \\(\\boldsymbol{x}^*\\).</li> </ol> <p><code>Question</code>: If \\(\\rho \\left( \\boldsymbol{G} \\right) &gt;1\\), there exists an initial guess that \\(\\boldsymbol{x}^{(k)}\\) does not converge. If \\(\\rho \\left( \\boldsymbol{G} \\right) =1\\), what happens?</p> <p><code>Theorem</code>: If \\(\\left\\| \\boldsymbol{G} \\right\\| &lt;1\\) (any matrix norm is fine), then \\(\\rho \\left( \\boldsymbol{G} \\right) &lt;1\\).</p>"},{"location":"Iterative%20Methods/Introduction_to_Iteration/#method-2","title":"Method 2","text":"<p><code>Definition</code>: A matrix \\(\\boldsymbol{A}\\in \\mathbb{R} ^{m\\times m}\\) is called (weakly) diagonally dominant if:</p> \\[ \\left| a_{ii} \\right|\\geqslant \\sum_{\\begin{array}{c}     j=1\\\\     j\\ne i\\\\ \\end{array}}^m{\\left| a_{ij} \\right|}; i=1,2,\\cdots ,m \\] <p>and is called (strongly) diagonally dominant if:</p> \\[ \\left| a_{ii} \\right|&gt;\\sum_{\\begin{array}{c}     j=1\\\\     j\\ne i\\\\ \\end{array}}^m{\\left| a_{ij} \\right|}; i=1,2,\\cdots ,m \\] <p><code>Theorem</code>: If \\(\\boldsymbol{A}\\) is a strongly diagonally dominant matrix, then the associate Jacobi, Gauss-Seidel iterations converge for any \\(\\boldsymbol{x}^{(0)}\\).</p> <p><code>Theorem</code>: If \\(\\boldsymbol{A}\\) is symmetric with positive diagonal elements, and \\(\\omega \\in \\left( 0,2 \\right)\\), then SOR converges for any \\(\\boldsymbol{x}^{(0)}\\) if and only if \\(\\boldsymbol{A}\\) is positive definite.</p> <p><code>Theorem</code>(Gershgorin Theroem): Given \\(\\boldsymbol{A}\\in \\mathbb{R} ^{m\\times m}\\), the eigenvalues of \\(\\boldsymbol{A}\\) are contained in the union of the following disks:</p> \\[ D_i=\\left\\{ z\\in \\mathbb{C} \\middle| \\left| z-a_i \\right|\\leqslant \\sum_{\\begin{array}{c}     j=1\\\\     j\\ne i\\\\ \\end{array}}^m{\\left| a_{ij} \\right|} \\right\\} ; i=1,2,\\cdots ,m \\] <p>Let:</p> \\[ r_i=\\sum_{\\begin{array}{c}     j=1\\\\     j\\ne i\\\\ \\end{array}}^m{\\left| a_{ij} \\right|} \\] <p>We have:</p> \\[ D_i=\\left\\{ z\\in \\mathbb{C} \\middle| \\left| z-a_i \\right|\\leqslant r_i \\right\\} ; i=1,2,\\cdots ,m \\]"},{"location":"Iterative%20Methods/Preconditioning/","title":"Preconditioning","text":""},{"location":"Iterative%20Methods/Preconditioning/#general-introduction","title":"General Introduction","text":"<p>Idea: Perform Preconditioning on Conjugate Gradient method.</p> <p>Preconditioning is to reduce the condition number of the system so that it can converge faster. We want to solve \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) where \\(\\kappa \\left( \\boldsymbol{A} \\right) \\gg 1\\).</p> <p>If \\(\\boldsymbol{M}\\) is invertible, then \\(\\boldsymbol{M}^{-1}\\boldsymbol{Ax}=\\boldsymbol{M}^{-1}\\boldsymbol{b}\\). We need to select a good \\(\\boldsymbol{M}\\) so that \\(\\kappa \\left( \\boldsymbol{M}^{-1}\\boldsymbol{A} \\right) \\ll \\kappa \\left( \\boldsymbol{A} \\right)\\). \\(\\boldsymbol{M}\\) is called preconditioner.</p> <p>Here are two criteria that we choose for a good \\(\\boldsymbol{M}\\):</p> <ol> <li>\\(\\kappa \\left( \\boldsymbol{M}^{-1}\\boldsymbol{A} \\right) \\ll \\kappa \\left( \\boldsymbol{A} \\right)\\);</li> <li>\\(\\boldsymbol{M}^{-1}\\) is easy to find ( \\(\\boldsymbol{Mz}=\\boldsymbol{r}\\) is easy to solve).</li> </ol> <p><code>Question</code>: How to apply CG to the preconditioned system \\(\\boldsymbol{M}^{-1}\\boldsymbol{Ax}=\\boldsymbol{M}^{-1}\\boldsymbol{b}\\) ? Note that \\(\\boldsymbol{M}^{-1}\\boldsymbol{A}\\) is no longer guaranteed to be a symmetric matrix.</p> <p>Assume \\(\\boldsymbol{A}\\) is SPD.</p>"},{"location":"Iterative%20Methods/Preconditioning/#option-1-split-preconditioner","title":"Option 1: Split Preconditioner","text":"<p>Pick a SPD matrix \\(\\boldsymbol{M}\\). Using Cholesky factorization: \\(\\boldsymbol{M}=\\boldsymbol{LL}^T\\). Then we have:</p> \\[ \\underbrace{\\boldsymbol{L}^{-1}\\boldsymbol{A}\\left( \\boldsymbol{L}^{-1} \\right) ^T}\\cdot \\underset{\\boldsymbol{u}}{\\underbrace{\\boldsymbol{L}^T\\boldsymbol{x}}}=\\boldsymbol{L}^{-1}\\boldsymbol{b} \\] \\[ \\Longrightarrow \\begin{cases}     \\boldsymbol{L}^T\\boldsymbol{x}=\\boldsymbol{u}\\\\     \\underset{\\mathrm{SPD}}{\\underbrace{\\boldsymbol{L}^{-1}\\boldsymbol{A}\\left( \\boldsymbol{L}^{-1} \\right) ^T}}\\cdot \\boldsymbol{u}=\\boldsymbol{L}^{-1}\\boldsymbol{b}\\\\ \\end{cases} \\] <p>How to pick \\(\\boldsymbol{L}\\) such that \\(\\kappa \\left( \\boldsymbol{L}^{-1}\\boldsymbol{A}\\left( \\boldsymbol{L}^{-1} \\right) ^T \\right) \\ll \\kappa \\left( \\boldsymbol{A} \\right)\\) ?</p> <p><code>Strategy</code>: \\(\\boldsymbol{L}\\) is constructed directly from \\(\\boldsymbol{A}\\).</p> <p><code>Task</code> (HW question): Write an algorithm to use CG for \\(\\boldsymbol{L}^{-1}\\boldsymbol{A}\\left( \\boldsymbol{L}^{-1} \\right) ^T\\boldsymbol{u}=\\boldsymbol{L}^{-1}\\boldsymbol{b}\\).</p>"},{"location":"Iterative%20Methods/Preconditioning/#option-2","title":"Option 2","text":""},{"location":"Iterative%20Methods/Preconditioning/#some-concepts","title":"Some Concepts","text":"<p>Assume \\(\\boldsymbol{A}\\) is SPD. \\(\\boldsymbol{A}^T=\\boldsymbol{A}\\).</p> <p>We claim that \\(\\forall \\boldsymbol{x},\\boldsymbol{y}\\in \\mathbb{R} ^n\\), \\(\\boldsymbol{A}\\) is symmetric if \\(\\left&lt; \\boldsymbol{x},\\boldsymbol{Ay} \\right&gt; =\\left&lt; \\boldsymbol{Ax},\\boldsymbol{y} \\right&gt;\\).</p> <p>Note that if \\(\\boldsymbol{A}\\) is symmetric,</p> \\[ \\left&lt; \\boldsymbol{x},\\boldsymbol{Ay} \\right&gt; =\\boldsymbol{x}^T\\left( \\boldsymbol{Ay} \\right) =\\boldsymbol{x}^T\\boldsymbol{A}^T\\boldsymbol{y}=\\left( \\boldsymbol{Ax} \\right) ^T\\boldsymbol{y}=\\left&lt; \\boldsymbol{Ax},\\boldsymbol{y} \\right&gt;  \\] <p>If \\(\\boldsymbol{M}\\) is SPD, we introduce \\(\\boldsymbol{M}\\) - inner product: \\(\\forall \\boldsymbol{x},\\boldsymbol{y}\\in \\mathbb{R} ^n\\), define:</p> \\[ \\left&lt; \\boldsymbol{x},\\boldsymbol{y} \\right&gt; _{\\boldsymbol{M}}\\triangleq \\left&lt; \\boldsymbol{Mx},\\boldsymbol{y} \\right&gt; =\\left&lt; \\boldsymbol{x},\\boldsymbol{My} \\right&gt;  \\] <p><code>Claim</code>: \\(\\boldsymbol{M}^{-1}\\boldsymbol{A}\\) is symmetric with respect to \\(\\boldsymbol{M}\\) -inner product.</p> <p><code>Verification</code>: Beacuse \\(\\boldsymbol{A}\\) is symmetric,</p> \\[ \\left&lt; \\left( \\boldsymbol{M}^{-1}\\boldsymbol{A} \\right) \\boldsymbol{x}, \\boldsymbol{y} \\right&gt; _{\\boldsymbol{M}}=\\left&lt; \\boldsymbol{M}\\left( \\boldsymbol{M}^{-1}\\boldsymbol{A} \\right) \\boldsymbol{x}, \\boldsymbol{y} \\right&gt; =\\left&lt; \\boldsymbol{Ax},\\boldsymbol{y} \\right&gt;  \\] \\[ =\\left&lt; \\boldsymbol{x},\\boldsymbol{Ay} \\right&gt; =\\left&lt; \\boldsymbol{x},\\boldsymbol{M}\\left( \\boldsymbol{M}^{-1}\\boldsymbol{A} \\right) \\boldsymbol{y} \\right&gt; =\\left&lt; \\boldsymbol{x},\\left( \\boldsymbol{M}^{-1}\\boldsymbol{A} \\right) \\boldsymbol{y} \\right&gt; _{\\boldsymbol{M}} \\]"},{"location":"Iterative%20Methods/Preconditioning/#preconditioned-conjugate-gradients-pcg","title":"Preconditioned Conjugate Gradients (PCG)","text":"<p>The idea is to replace the standard inner product in CG by the \\(\\boldsymbol{M}\\) -inner product for the preconditioned system to PCG.</p> CG PCG \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) \\(\\boldsymbol{M}^{-1}\\boldsymbol{Ax}=\\boldsymbol{M}^{-1}\\boldsymbol{b}\\) \\(\\boldsymbol{r}_j=\\boldsymbol{b}-\\boldsymbol{Ax}_j\\) \\(\\boldsymbol{z}_j=\\boldsymbol{M}^{-1}\\left( \\boldsymbol{b}-\\boldsymbol{Ax}_j \\right) =\\boldsymbol{M}^{-1}\\boldsymbol{r}_j,\\left( \\boldsymbol{Mz}_j=\\boldsymbol{r}_j \\right)\\) \\(\\alpha _j=\\frac{\\left&lt; \\boldsymbol{r}_j,\\boldsymbol{r}_j \\right&gt;}{\\left&lt; \\boldsymbol{p}_j,\\boldsymbol{Ap}_j \\right&gt;}\\) \\(\\alpha _j=\\frac{\\left&lt; \\boldsymbol{z}_j,\\boldsymbol{z}_j \\right&gt; _{\\boldsymbol{M}}}{\\left&lt; \\boldsymbol{p}_j,\\boldsymbol{M}^{-1}\\boldsymbol{Ap}_j \\right&gt; _{\\boldsymbol{M}}}\\) \\(\\tau _j=\\frac{\\left&lt; \\boldsymbol{r}_{j+1},\\boldsymbol{r}_{j+1} \\right&gt;}{\\left&lt; \\boldsymbol{r}_j,\\boldsymbol{r}_j \\right&gt;}\\) \\(\\tau _j=\\frac{\\left&lt; \\boldsymbol{z}_{j+1},\\boldsymbol{z}_{j+1} \\right&gt; _{\\boldsymbol{M}}}{\\left&lt; \\boldsymbol{z}_j,\\boldsymbol{z}_j \\right&gt; _{\\boldsymbol{M}}}\\) <p>Also note:</p> \\[ \\tau _j=\\frac{\\left&lt; \\boldsymbol{z}_{j+1},\\boldsymbol{z}_{j+1} \\right&gt; _{\\boldsymbol{M}}}{\\left&lt; \\boldsymbol{z}_j,\\boldsymbol{z}_j \\right&gt; _{\\boldsymbol{M}}}=\\frac{\\left&lt; \\boldsymbol{Mz}_{j+1},\\boldsymbol{z}_{j+1} \\right&gt;}{\\left&lt; \\boldsymbol{Mz}_j,\\boldsymbol{z}_j \\right&gt;}=\\frac{\\left&lt; \\boldsymbol{r}_{j+1},\\boldsymbol{z}_{j+1} \\right&gt;}{\\left&lt; \\boldsymbol{r}_j,\\boldsymbol{z}_j \\right&gt;}, \\] \\[ \\alpha _j=\\frac{\\left&lt; \\boldsymbol{z}_j,\\boldsymbol{z}_j \\right&gt; _{\\boldsymbol{M}}}{\\left&lt; \\boldsymbol{p}_j,\\boldsymbol{M}^{-1}\\boldsymbol{Ap}_j \\right&gt; _{\\boldsymbol{M}}}=\\frac{\\left&lt; \\boldsymbol{Mz}_j,\\boldsymbol{z}_j \\right&gt;}{\\left&lt; \\boldsymbol{p}_j,\\boldsymbol{M}\\left( \\boldsymbol{M}^{-1}\\boldsymbol{A} \\right) \\boldsymbol{p}_j \\right&gt;}=\\frac{\\left&lt; \\boldsymbol{r}_j,\\boldsymbol{z}_j \\right&gt;}{\\left&lt; \\boldsymbol{p}_j,\\boldsymbol{Ap}_j \\right&gt;} \\] <p><code>Algorithm</code> ( PCG ):</p> <p>Compute \\(\\boldsymbol{r}_0=\\boldsymbol{b}-\\boldsymbol{Ax}^{\\left( 0 \\right)}\\), and set \\(\\boldsymbol{p}_0=\\boldsymbol{r}_0\\);</p> <p>Solve \\(\\boldsymbol{Mz}_0=\\boldsymbol{r}_0\\);</p> <p>For \\(j=0,1,2,\\cdots\\) until convergence, do:</p> <ul> <li>\\(\\alpha _j=\\frac{\\left&lt; \\boldsymbol{r}_j,\\boldsymbol{z}_j \\right&gt;}{\\left&lt; \\boldsymbol{p}_j,\\boldsymbol{Ap}_j \\right&gt;}\\);</li> <li>\\(\\boldsymbol{x}^{\\left( j+1 \\right)}=\\boldsymbol{x}^{\\left( j \\right)}+\\alpha _j\\boldsymbol{p}_j\\);</li> <li>\\(\\boldsymbol{r}_{j+1}=\\boldsymbol{r}_j-\\alpha _j\\boldsymbol{Ap}_j\\);</li> <li>( Critical change ) Solve \\(\\boldsymbol{Mz}_{j+1}=\\boldsymbol{r}_{j+1}\\);</li> <li>\\(\\tau _j=\\frac{\\left&lt; \\boldsymbol{r}_{j+1},\\boldsymbol{z}_{j+1} \\right&gt;}{\\left&lt; \\boldsymbol{r}_j,\\boldsymbol{z}_j \\right&gt;}\\);</li> <li>\\(\\boldsymbol{p}_{j+1}=\\boldsymbol{z}_{j+1}+\\tau _j\\boldsymbol{p}_j\\);</li> </ul> <p>End</p> <p><code>Question</code>: How to pick \\(\\boldsymbol{M}\\) ?</p> <ul> <li>Multigrid;</li> <li>Domain Decomposition.</li> </ul>"},{"location":"Iterative%20Methods/Projection_Method/","title":"Projection Methods","text":""},{"location":"Iterative%20Methods/Projection_Method/#general-framework-for-projection-methods","title":"General framework for Projection Methods","text":"<p>Given two subspaces \\(\\mathbf{L},\\mathbf{K}\\), and an initial guess for the solution \\(\\boldsymbol{x}^{\\left( 0 \\right)}\\), we want to find \\(\\boldsymbol{\\delta }\\in \\mathbf{K}\\) such that \\(\\boldsymbol{x}=\\boldsymbol{x}^{\\left( 0 \\right)}+\\boldsymbol{\\delta }\\) produces a residual \\(\\boldsymbol{r}=\\boldsymbol{b}-\\boldsymbol{Ax}\\) that is orthogonal to \\(\\mathbf{L}\\).</p> \\[ \\boldsymbol{r}=\\boldsymbol{b}-\\boldsymbol{Ax}=\\boldsymbol{b}-\\boldsymbol{A}\\left( \\boldsymbol{x}^{\\left( 0 \\right)}+\\boldsymbol{\\delta } \\right)  \\] \\[ =\\left( \\boldsymbol{b}-\\boldsymbol{Ax}^{\\left( 0 \\right)} \\right) -\\boldsymbol{A\\delta }=\\boldsymbol{r}^{\\left( 0 \\right)}-\\boldsymbol{A\\delta }; \\] <p>Note that:</p> \\[ \\boldsymbol{r}\\bot \\mathbf{L} \\] <p></p> <p>Assume \\(\\boldsymbol{x}\\in \\mathbb{R} ^m\\left( n\\leqslant m \\right)\\). Let \\(\\left\\{ \\boldsymbol{v}_i \\right\\} _{i=1}^{n}\\) be a basis of \\(\\mathbf{K}\\), and let \\(\\left\\{ \\boldsymbol{w}_i \\right\\} _{i=1}^{n}\\) be a basis of \\(\\mathbf{L}\\). Note that we assume \\(\\mathrm{dim}\\left( \\mathbf{K} \\right) =\\mathrm{dim}\\left( \\mathbf{L} \\right)\\) here. Then:</p> \\[ \\boldsymbol{r}\\bot \\mathbf{L}\\ \\ \\Leftrightarrow \\ \\ \\boldsymbol{r}\\bot \\boldsymbol{w}_i,\\ \\left( i=1,2,\\cdots ,n \\right) ; \\] \\[ \\left&lt; \\boldsymbol{w}_i,\\ \\boldsymbol{r} \\right&gt; =0,\\ \\left( i=1,2,\\cdots ,n \\right)  \\] <p>If we denote:</p> \\[ \\boldsymbol{W}=\\left[ \\begin{matrix}     \\boldsymbol{w}_1&amp;       \\cdots&amp;     \\boldsymbol{w}_n\\\\ \\end{matrix} \\right]; \\] \\[ \\boldsymbol{V}=\\left[ \\begin{matrix}     \\boldsymbol{v}_1&amp;       \\cdots&amp;     \\boldsymbol{v}_n\\\\ \\end{matrix} \\right]  \\] <p>Then:</p> \\[ \\boldsymbol{W}^T\\boldsymbol{r}=\\mathbf{0} \\] <p>For \\(\\boldsymbol{\\delta }\\in \\mathbf{K}\\), we have:</p> \\[ \\boldsymbol{\\delta }=\\sum_{i=1}^n{y_i\\boldsymbol{v}_i}=\\boldsymbol{Vy};\\ \\boldsymbol{y}=\\left[ \\begin{array}{c}     y_1\\\\     \\vdots\\\\     y_n\\\\ \\end{array} \\right]  \\] <p>We get:</p> \\[ \\boldsymbol{x}=\\boldsymbol{x}^{\\left( 0 \\right)}+\\boldsymbol{\\delta }=\\boldsymbol{x}^{\\left( 0 \\right)}+\\boldsymbol{Vy}; \\] \\[ \\boldsymbol{r}=\\boldsymbol{b}-\\boldsymbol{Ax}=\\boldsymbol{r}^{\\left( 0 \\right)}-\\boldsymbol{A\\delta }=\\boldsymbol{r}^{\\left( 0 \\right)}-\\boldsymbol{AVy}; \\] \\[ \\mathbf{0}=\\boldsymbol{W}^T\\boldsymbol{r}=\\boldsymbol{W}^T\\left( \\boldsymbol{r}^{\\left( 0 \\right)}-\\boldsymbol{AVy} \\right) =\\boldsymbol{W}^T\\boldsymbol{r}^{\\left( 0 \\right)}-\\boldsymbol{W}^T\\boldsymbol{AVy}; \\] <p>Then:</p> \\[ \\boldsymbol{W}^T\\boldsymbol{AVy}=\\boldsymbol{W}^T\\boldsymbol{r}^{\\left( 0 \\right)} \\] <p>Therefore, if \\(\\boldsymbol{W}^T\\boldsymbol{AV}\\) is invertible (note that we have assumed the dimensions of \\(\\mathbf{K}\\) and \\(\\mathbf{L}\\) are the same before), we get:</p> \\[ \\boldsymbol{y}=\\left( \\boldsymbol{W}^T\\boldsymbol{AV} \\right) ^{-1}\\boldsymbol{W}^T\\boldsymbol{r}^{\\left( 0 \\right)} \\] <p>where \\(\\boldsymbol{W}^T\\boldsymbol{AV}\\) is a \\(n\\times n\\) matrix (in many cases, we select \\(n\\) as a small number; sometimes even \\(n=1\\)). Then:</p> \\[ \\boldsymbol{x}=\\boldsymbol{x}^{\\left( 0 \\right)}+\\boldsymbol{\\delta }=\\boldsymbol{x}^{\\left( 0 \\right)}+\\boldsymbol{Vy} \\] \\[ =\\boldsymbol{x}^{\\left( 0 \\right)}+\\boldsymbol{V}\\left( \\boldsymbol{W}^T\\boldsymbol{AV} \\right) ^{-1}\\boldsymbol{W}^T\\boldsymbol{r}^{\\left( 0 \\right)} \\] <p><code>Algorithm</code>( Projection Algorithm ):</p> <p>Until convergence, do:</p> <ul> <li>Select a pair of subspaces \\(\\mathbf{K},\\mathbf{L}\\);</li> <li>Choose bases \\(\\boldsymbol{V}=\\left[ \\begin{matrix}     \\boldsymbol{v}_1&amp;       \\cdots&amp;     \\boldsymbol{v}_n\\\\ \\end{matrix} \\right] , \\boldsymbol{W}=\\left[ \\begin{matrix}     \\boldsymbol{w}_1&amp;       \\cdots&amp;     \\boldsymbol{w}_n\\\\ \\end{matrix} \\right]\\);</li> <li>\\(\\boldsymbol{r}=\\boldsymbol{b}-\\boldsymbol{Ax}\\);</li> <li>\\(\\boldsymbol{y}=\\left( \\boldsymbol{W}^T\\boldsymbol{AV} \\right) ^{-1}\\boldsymbol{W}^T\\boldsymbol{r}\\);</li> <li>\\(\\boldsymbol{x}=\\boldsymbol{x}+\\boldsymbol{Vy}\\);</li> </ul> <p>End</p>"},{"location":"Iterative%20Methods/Projection_Method/#example-gauss-seidel-method","title":"Example: Gauss-Seidel Method","text":"<p><code>Example</code>: Gauss-Seidel is a Projection Method by taking \\(\\mathbf{K}=\\mathbf{L}=\\mathrm{span}\\left\\{ \\boldsymbol{e}_i \\right\\}\\) (standard basis). In this case:</p> \\[ \\boldsymbol{\\delta }=\\left[ \\begin{array}{c}     0\\\\     \\vdots\\\\     0\\\\     \\delta _i\\\\     0\\\\     \\vdots\\\\     0\\\\ \\end{array} \\right] , \\boldsymbol{x}=\\boldsymbol{x}^{\\left( 0 \\right)}+\\boldsymbol{\\delta }=\\left[ \\begin{array}{c}     {\\boldsymbol{x}_1}^{\\left( 0 \\right)}\\\\     \\vdots\\\\     {\\boldsymbol{x}_i}^{\\left( 0 \\right)}\\\\     \\vdots\\\\     {\\boldsymbol{x}_n}^{\\left( 0 \\right)}\\\\ \\end{array} \\right] +\\left[ \\begin{array}{c}     0\\\\     \\vdots\\\\     0\\\\     \\delta _i\\\\     0\\\\     \\vdots\\\\     0\\\\ \\end{array} \\right] ; \\] <p>And:</p> \\[ \\boldsymbol{r}=\\boldsymbol{b}-\\boldsymbol{Ax}=\\boldsymbol{b}-\\boldsymbol{Ax}^{\\left( 0 \\right)}-\\boldsymbol{A\\delta }=\\boldsymbol{r}^{\\left( 0 \\right)}-\\boldsymbol{A\\delta }; \\] <p>Note that we require:</p> \\[ \\boldsymbol{r}\\bot \\mathbf{L}\\Leftrightarrow \\boldsymbol{r}\\bot \\boldsymbol{e}_i; \\] \\[ 0={\\boldsymbol{e}_i}^T\\boldsymbol{r}=\\left( \\boldsymbol{b}-\\boldsymbol{Ax}^{\\left( 0 \\right)}-\\boldsymbol{A\\delta } \\right) _i=0; \\] \\[ b_i-\\sum_{j=1}^m{a_{ij}{x_j}^{\\left( 0 \\right)}}-a_{ii}\\delta _i=0, \\] \\[ \\delta _i=\\frac{1}{a_{ii}}\\left( b_i-\\sum_{j=1}^m{a_{ij}{x_j}^{\\left( 0 \\right)}} \\right)  \\] <p>We end up with Gauss-Seidel iteration.</p>"},{"location":"Iterative%20Methods/Projection_Method/#discussion-on-projection-method","title":"Discussion on Projection Method","text":"<p>Now, consider the projection method again:</p> \\[ \\begin{cases}     \\boldsymbol{r}=\\boldsymbol{b}-\\boldsymbol{Ax}\\\\     \\boldsymbol{y}=\\left( \\boldsymbol{W}^T\\boldsymbol{AV} \\right) ^{-1}\\boldsymbol{W}^T\\boldsymbol{r}\\\\     \\boldsymbol{x}=\\boldsymbol{x}+\\boldsymbol{Vy}\\\\ \\end{cases} \\] <p>The algorithm can be continued (does not necessarily mean convergence) if \\(\\boldsymbol{W}^T\\boldsymbol{AV}\\) is invertible.</p> <p><code>Question</code>: How do we ensure that \\(\\boldsymbol{W}^T\\boldsymbol{AV}\\) is invertible?</p> <p>If \\(\\boldsymbol{A}\\) is invertible, it is not necessarily true that \\(\\boldsymbol{W}^T\\boldsymbol{AV}\\) is also invertible. For example, consider:</p> \\[ \\boldsymbol{A}=\\left[ \\begin{matrix}     \\boldsymbol{O}&amp;     \\mathbf{I}_{\\left( \\mathrm{r}\\ \\mathrm{rows} \\right)}\\\\     \\mathbf{I}_{\\left( \\mathrm{r}\\ \\mathrm{cols} \\right)}&amp;      \\mathbf{I}\\\\ \\end{matrix} \\right] , \\boldsymbol{V}=\\boldsymbol{W}=\\left[ \\begin{array}{c}     \\mathbf{I}\\\\     \\boldsymbol{O}\\\\ \\end{array} \\right] ; \\] \\[ \\Rightarrow \\boldsymbol{W}^T\\boldsymbol{AV}=\\boldsymbol{O} \\] <p><code>Theorem</code>: Let \\(\\boldsymbol{A},\\mathbf{L},\\mathbf{K}\\) satisfy either one of the following conditions:</p> <ol> <li>\\(\\boldsymbol{A}\\) is SPD and \\(\\mathbf{L}=\\mathbf{K}\\);</li> <li>\\(\\boldsymbol{A}\\) is invertible and \\(\\mathbf{L}=\\boldsymbol{A} \\mathbf{K}\\).</li> </ol> <p>Then the matrix \\(\\boldsymbol{W}^T\\boldsymbol{AV}\\) is nonsingular for any \\(\\boldsymbol{V}, \\boldsymbol{W}\\) of \\(\\mathbf{K},\\mathbf{L}\\) respectively.</p> <p><code>Proof</code>: \\(\\mathbf{L}=\\boldsymbol{A} \\mathbf{K}\\) means that \\(\\forall \\boldsymbol{v}\\in \\mathbf{K}, \\boldsymbol{Av}\\in \\mathbf{L}\\), and \\(\\forall \\boldsymbol{u}\\in \\mathbf{L}\\), there exists \\(\\boldsymbol{v}\\in \\mathbf{K}\\) such that \\(\\boldsymbol{u}=\\boldsymbol{Av}\\).</p> <p>First situation: Since \\(\\mathbf{L}=\\mathbf{K}\\), and \\(\\boldsymbol{V},\\boldsymbol{W}\\) are the two bases, then \\(\\boldsymbol{W}=\\boldsymbol{VG}\\), where \\(\\boldsymbol{G}\\) is the change of basis matrix ( \\(\\boldsymbol{G}\\) is invertible). We can find out that:</p> \\[ \\boldsymbol{W}^T\\boldsymbol{AV}=\\left( \\boldsymbol{VG} \\right) ^T\\boldsymbol{AV}=\\underset{\\mathrm{invertible}}{\\underbrace{\\boldsymbol{G}^T}}\\cdot \\underset{\\mathrm{invertible}}{\\underbrace{\\boldsymbol{V}^T\\boldsymbol{AV}}} \\] <p>Then \\(\\boldsymbol{W}^T\\boldsymbol{AV}\\) is invertible.</p> <p>Second situation: Pick \\(\\mathbf{L}=\\boldsymbol{A} \\mathbf{K}\\), then \\(\\boldsymbol{AV}\\) is a basis for \\(\\mathbf{L}\\). Since \\(\\boldsymbol{W}\\) is also a basis for \\(\\mathbf{L}\\), there exists a change of basis matrix \\(\\boldsymbol{G}\\) (invertible) such that \\(\\boldsymbol{AVG}=\\boldsymbol{W}\\). Then:</p> \\[ \\boldsymbol{W}^T\\boldsymbol{AV}=\\boldsymbol{G}^T\\left( \\boldsymbol{AV} \\right) ^T\\boldsymbol{AV}=\\boldsymbol{G}^T\\boldsymbol{V}^T\\boldsymbol{A}^T\\boldsymbol{AV} \\] <p>\\(\\boldsymbol{A}^T\\boldsymbol{A}\\) is SPD because \\(\\boldsymbol{A}\\) is nonsingular. Since \\(\\boldsymbol{V}^T\\boldsymbol{A}^T\\boldsymbol{AV}\\) and \\(\\boldsymbol{G}^T\\) are invertible, we can find that \\(\\boldsymbol{W}^T\\boldsymbol{AV}\\) is invertible.</p> <p>Therefore, the projection method can be continued under one of these two conditions. End of proof.</p> <p><code>Example</code>: Assume \\(\\boldsymbol{A}\\) is SPD, pick \\(\\mathbf{L}=\\mathbf{K}\\). Let \\(\\mathbf{K}=\\mathbf{L}=\\mathrm{span}\\left\\{ \\boldsymbol{r}^{\\left( j \\right)},\\boldsymbol{p}^{\\left( j-1 \\right)} \\right\\}\\) where \\(\\boldsymbol{p}^{\\left( j-1 \\right)}\\) is the search direction in the previous step and \\(\\boldsymbol{r}^{\\left( j \\right)}\\) is the current residual. This is how Conjugate Gradient Method (CG) is formed.</p>"},{"location":"Iterative%20Methods/Steepest_Descent/","title":"Steepest Descent Method","text":""},{"location":"Iterative%20Methods/Steepest_Descent/#introduction-to-steepest-descent","title":"Introduction to Steepest Descent","text":"<p><code>Example</code>: Assume \\(\\boldsymbol{A}\\) is SPD, and \\(\\boldsymbol{r}\\) is the current residual. We pick \\(\\mathbf{L}=\\mathbf{K}=\\mathrm{span}\\left\\{ \\boldsymbol{r} \\right\\}\\) if \\(\\boldsymbol{r} \\ne 0\\). Then:</p> \\[ \\boldsymbol{\\delta }=\\alpha \\boldsymbol{r}; \\boldsymbol{\\delta }\\in \\mathbf{L}, \\alpha \\in \\mathbb{R}  \\] <p>Also:</p> \\[ \\boldsymbol{x}^{\\mathrm{new}}=\\boldsymbol{x}+\\alpha \\boldsymbol{r}, \\] \\[ \\boldsymbol{r}^{\\mathrm{new}}=\\boldsymbol{b}-\\boldsymbol{Ax}^{\\mathrm{new}}=\\boldsymbol{r}-\\alpha \\boldsymbol{Ar} \\] <p>Note that:</p> \\[ \\boldsymbol{r}^{\\mathrm{new}}\\bot \\boldsymbol{r}\\Leftrightarrow \\boldsymbol{r}^{\\mathrm{new}}\\bot \\mathbf{L}, \\] \\[ \\left( \\boldsymbol{r}^{\\mathrm{new}} \\right) ^T\\boldsymbol{r}=0,\\left( \\boldsymbol{r}-\\alpha \\boldsymbol{Ar} \\right) ^T\\boldsymbol{r}=0, \\] \\[ \\Longrightarrow \\boldsymbol{r}^T\\boldsymbol{r}-\\alpha \\boldsymbol{r}^T\\boldsymbol{Ar}=0 \\] <p>We get:</p> \\[ \\alpha =\\frac{\\boldsymbol{r}^T\\boldsymbol{r}}{\\boldsymbol{r}^T\\boldsymbol{Ar}}, \\boldsymbol{x}^{\\mathrm{new}}=\\boldsymbol{x}+\\alpha \\boldsymbol{r} \\] <p><code>Algorithm</code>( Steepest Descent Method ):</p> <p>For \\(k=1,2,\\cdots\\):</p> <ul> <li>\\(\\boldsymbol{r}^{\\left( k \\right)}=\\boldsymbol{b}^{\\left( k \\right)}-\\boldsymbol{Ax}^{\\left( k \\right)}\\);</li> <li>\\(\\alpha ^{\\left( k \\right)}=\\frac{\\left( \\boldsymbol{r}^{\\left( k \\right)} \\right) ^T\\boldsymbol{r}^{\\left( k \\right)}}{\\left( \\boldsymbol{r}^{\\left( k \\right)} \\right) ^T\\boldsymbol{Ar}^{\\left( k \\right)}}\\);</li> <li>\\(\\boldsymbol{x}^{\\left( k+1 \\right)}=\\boldsymbol{x}^{\\left( k \\right)}+\\alpha ^{\\left( k \\right)}\\boldsymbol{r}^{\\left( k \\right)}\\);</li> </ul> <p>End</p>"},{"location":"Iterative%20Methods/Steepest_Descent/#discussion-on-steepest-descent","title":"Discussion on Steepest Descent","text":"<p><code>Question</code>: Why is it called \"Steepest Descent\"? Why is \\(\\alpha\\) optimal?</p>"},{"location":"Iterative%20Methods/Steepest_Descent/#first-perspective-from-error-viewpoint","title":"First Perspective (From Error Viewpoint)","text":"<p>Assume \\(\\boldsymbol{x}^*\\) is the true solution that we want to compute. We define the error as \\(\\boldsymbol{e}=\\boldsymbol{x}-\\boldsymbol{x}^*\\). Also define \\(f\\left( \\boldsymbol{x} \\right) =\\boldsymbol{e}^T\\boldsymbol{Ae}\\triangleq \\left\\| \\boldsymbol{e} \\right\\| _{\\boldsymbol{A}}^{2}\\) as the \\(\\boldsymbol{A}\\)-norm ( \\(\\boldsymbol{A}\\) is SPD here).</p> <p>Note that since \\(f\\left( \\boldsymbol{x} \\right)\\) is convex (quadratic function), there is a unique minimizer, \\(\\boldsymbol{x}^*\\) satisfying:</p> \\[ \\mathrm{arg}\\min_{\\boldsymbol{x}} f\\left( \\boldsymbol{x} \\right) =\\boldsymbol{x}^* \\] <p>We use gradient descent to find the minimizing search along the direction of negative gradient:</p> \\[ -\\nabla f\\left( \\boldsymbol{x} \\right) =-2\\boldsymbol{A}\\left( \\boldsymbol{x}-\\boldsymbol{x}^* \\right)  \\] \\[ =-2\\left( \\boldsymbol{Ax}-\\boldsymbol{Ax}^* \\right) =-2\\left( \\boldsymbol{Ax}-\\boldsymbol{b} \\right)  \\] \\[ =2\\left( \\boldsymbol{b}-\\boldsymbol{Ax} \\right) =2\\boldsymbol{r} \\] <p>This means that the residual and negative gradient have the same direction.</p> \\[ \\boldsymbol{x}^{\\mathrm{new}}=\\boldsymbol{x}+\\underset{\\mathrm{step}\\ \\mathrm{size}}{\\underbrace{\\alpha }}\\cdot \\boldsymbol{r} \\] <p>What is the best step size? Actually we would like:</p> \\[ \\min_{\\alpha} f\\left( \\boldsymbol{x}+\\alpha \\boldsymbol{r} \\right)  \\] <p>Therefore, we want to find \\(\\alpha\\) such that \\(\\frac{\\mathrm{d}}{\\mathrm{d}\\alpha}\\left( f\\left( \\boldsymbol{x}+\\alpha \\boldsymbol{r} \\right) \\right) =0\\). It turns out that (how to prove?):</p> \\[ \\alpha =\\frac{\\boldsymbol{r}^T\\boldsymbol{r}}{\\boldsymbol{r}^T\\boldsymbol{Ar}} \\] <p>This is the optimal solution for \\(\\alpha\\).</p>"},{"location":"Iterative%20Methods/Steepest_Descent/#second-perspective-quadratic-optimization","title":"Second Perspective (Quadratic Optimization)","text":"<p>We define:</p> \\[ g\\left( \\boldsymbol{x} \\right) =\\frac{1}{2}\\boldsymbol{x}^T\\boldsymbol{Ax}-\\boldsymbol{b}^T\\boldsymbol{x} \\] <p>Note that:</p> \\[ \\boldsymbol{x}^*=\\mathrm{arg}\\min_{\\boldsymbol{x}} g\\left( \\boldsymbol{x} \\right)  \\] <p>It turns out that:</p> \\[ -\\nabla g\\left( \\boldsymbol{x} \\right) =-\\left( \\boldsymbol{Ax}-\\boldsymbol{b} \\right) =\\boldsymbol{r} \\] <p>Similarly, we get:</p> \\[ \\alpha =\\frac{\\boldsymbol{r}^T\\boldsymbol{r}}{\\boldsymbol{r}^T\\boldsymbol{Ar}} \\] <p>is the optimal choice along \\(\\boldsymbol{r}\\).</p> <p>We can examine the level set of \\(f\\left( \\boldsymbol{x} \\right)\\) or \\(g\\left( \\boldsymbol{x} \\right)\\) to get geometric properties.</p>"},{"location":"Iterative%20Methods/Steepest_Descent/#further-remarks-on-steepest-descent","title":"Further Remarks on Steepest Descent","text":"<p>Consider two cases for \\(\\boldsymbol{A}\\):</p> <ul> <li><code>Case 1</code>: Eigenvalues \\(\\frac{\\lambda _{\\max}}{\\lambda _{\\min}}\\sim O\\left( 1 \\right)\\) (nearly a circle, fast convergence);</li> <li><code>Case 2</code>: Eigenvalues \\(\\frac{\\lambda _{\\max}}{\\lambda _{\\min}}\\gg 1\\) (nearly a very flat oval, slow convergence).</li> </ul> <p><code>Conclusion</code>: The condition number of \\(\\boldsymbol{A}\\) determines the speed of convergence:</p> <ul> <li>If \\(\\kappa \\left( \\boldsymbol{A} \\right) \\gg 1\\), we call \\(\\boldsymbol{A}\\) ill-conditioned;</li> <li>Otherwise, we call \\(\\boldsymbol{A}\\) well-conditioned.</li> </ul>"},{"location":"Miscellaneous/","title":"Miscellaneous Topics","text":"<p>The Least Square Problem</p>"},{"location":"Miscellaneous/Least_Square/","title":"Least Square Problems","text":"<p>Given \\(\\boldsymbol{A}\\in \\mathbb{R} ^{m\\times n}, m\\geqslant n; \\boldsymbol{b}\\in \\mathbb{R} ^m\\), the Least Square problem is defined as: Find \\(\\boldsymbol{x}^*\\in \\mathbb{R} ^n\\), such that:</p> \\[ \\boldsymbol{x}^*=\\mathrm{arg}\\min_{\\boldsymbol{x}} \\left\\| \\boldsymbol{b}-\\boldsymbol{Ax} \\right\\| _2 \\] <p>where \\(\\boldsymbol{r}=\\boldsymbol{b}-\\boldsymbol{Ax}\\) is called residual. Residual \\(\\boldsymbol{r}\\ne \\mathbf{0}\\).</p> <p>We will introduce three different methods to solve the Least Square problem.</p>"},{"location":"Miscellaneous/Least_Square/#normal-equation","title":"Normal Equation","text":""},{"location":"Miscellaneous/Least_Square/#traditional-method","title":"Traditional Method","text":"<p>We solve \\(\\boldsymbol{A}^*\\boldsymbol{Ax}=\\boldsymbol{A}^*\\boldsymbol{b}\\), where \\(\\boldsymbol{A}^*\\boldsymbol{A}\\in \\mathbb{R} ^{n\\times n}\\). If \\(\\boldsymbol{A}\\) is full rank, then \\(\\boldsymbol{A}^*\\boldsymbol{A}\\) is invertible.</p> <p>The Least Square solution is:</p> \\[ \\boldsymbol{x}^*=\\left( \\boldsymbol{A}^*\\boldsymbol{A} \\right) ^{-1}\\boldsymbol{A}^*\\boldsymbol{b} \\] <p><code>Question</code>: How about \\(\\boldsymbol{A}\\) is not full rank?</p> <p><code>Comment</code>: This method is not recommended is \\(\\kappa \\left( \\boldsymbol{A} \\right)\\) is large.</p> <p>Note: \\(\\kappa \\left( \\boldsymbol{A}^*\\boldsymbol{A} \\right) =\\left( \\kappa \\left( \\boldsymbol{A} \\right) \\right) ^2\\)</p> <p>Usually in practice, the number of matrix rows are way larger than that of columns.</p>"},{"location":"Miscellaneous/Least_Square/#two-recommended-methods","title":"Two Recommended Methods","text":""},{"location":"Miscellaneous/Least_Square/#qr-factorization","title":"QR Factorization","text":"\\[ \\boldsymbol{A}=\\boldsymbol{QR}\\,\\,\\Longleftrightarrow \\boldsymbol{Q}^*\\boldsymbol{A}=\\boldsymbol{R} \\] \\[ \\left\\| \\boldsymbol{Ax}-\\boldsymbol{b} \\right\\| _2=\\left\\| \\boldsymbol{Q}^*\\left( \\boldsymbol{Ax}-\\boldsymbol{b} \\right) \\right\\| _2 \\] \\[ =\\left\\| \\boldsymbol{Q}^*\\boldsymbol{Ax}-\\boldsymbol{Q}^*\\boldsymbol{b} \\right\\| _2=\\left\\| \\boldsymbol{Rx}-\\boldsymbol{Q}^*\\boldsymbol{b} \\right\\| _2 \\] <p>Let \\(\\boldsymbol{c}=\\boldsymbol{Q}^*\\boldsymbol{b}\\), then:</p> \\[ \\boldsymbol{x}^*=\\boldsymbol{R}^{-1}\\boldsymbol{c}=\\boldsymbol{R}^{-1}\\boldsymbol{Q}^*\\boldsymbol{b} \\]"},{"location":"Miscellaneous/Least_Square/#svd-method-recommended","title":"SVD Method (Recommended)","text":"<p>Assume \\(\\boldsymbol{A}=\\boldsymbol{U}\\mathbf{\\Sigma }\\boldsymbol{V}^*\\) as full SVD. We can get:</p> \\[ \\boldsymbol{A}=\\boldsymbol{U}\\mathbf{\\Sigma }\\boldsymbol{V}^*\\Leftrightarrow \\boldsymbol{U}^*\\boldsymbol{AV}=\\mathbf{\\Sigma }; \\] \\[ \\left\\| \\boldsymbol{Ax}-\\boldsymbol{b} \\right\\| _2=\\left\\| \\boldsymbol{U}^*\\boldsymbol{AVV}^*\\boldsymbol{x}-\\boldsymbol{U}^*\\boldsymbol{b} \\right\\| _2=\\left\\| \\mathbf{\\Sigma }\\boldsymbol{V}^*\\boldsymbol{x}-\\boldsymbol{U}^*\\boldsymbol{b} \\right\\| _2 \\] <p>Define \\(\\boldsymbol{V}^*\\boldsymbol{x}=\\boldsymbol{y},\\ \\boldsymbol{w}=\\boldsymbol{U}^*\\boldsymbol{b}\\), then:</p> \\[ \\left\\| \\boldsymbol{Ax}-\\boldsymbol{b} \\right\\| _2=\\left\\| \\mathbf{\\Sigma }\\boldsymbol{y}-\\boldsymbol{w} \\right\\| _2; \\] \\[ \\mathrm{arg}\\min_{\\boldsymbol{y}} \\left\\| \\mathbf{\\Sigma }\\boldsymbol{y}-\\boldsymbol{w} \\right\\| _2 \\] \\[ \\Longrightarrow \\boldsymbol{y}^*=\\mathbf{\\Sigma }^{-1}\\boldsymbol{w}=\\mathbf{\\Sigma }^{-1}\\boldsymbol{U}^*\\boldsymbol{b}, \\] \\[ \\boldsymbol{x}^*=\\boldsymbol{Vy}^*=\\boldsymbol{V}\\mathbf{\\Sigma }^{-1}\\boldsymbol{U}^*\\boldsymbol{b} \\] <p>We get the formula:</p> \\[ \\boldsymbol{x}^*=\\boldsymbol{V}\\mathbf{\\Sigma }^{-1}\\boldsymbol{U}^*\\boldsymbol{b} \\] <p>However,  \\(\\mathbf{\\Sigma }^{-1}\\) may not exist! Assume:</p> \\[ \\mathbf{\\Sigma }=\\left[ \\begin{matrix}     \\sigma _1&amp;      &amp;       &amp;       &amp;       &amp;       \\\\     &amp;       \\ddots&amp;     &amp;       &amp;       &amp;       \\\\     &amp;       &amp;       \\sigma _r&amp;      &amp;       &amp;       \\\\     &amp;       &amp;       &amp;       0&amp;      &amp;       \\\\     &amp;       &amp;       &amp;       &amp;       \\ddots&amp;     \\\\     &amp;       &amp;       &amp;       &amp;       &amp;       0\\\\ \\end{matrix} \\right] , \\sigma _1\\geqslant \\sigma _2\\geqslant \\cdots \\geqslant \\sigma _r&gt;\\sigma _{r+1}=\\cdots =\\sigma _m=0 \\] <p>The pseudo inverse \\(\\mathbf{\\Sigma }^{-1}\\) is defined as:</p> \\[ \\mathbf{\\Sigma }^{-1}=\\left[ \\begin{matrix}     \\frac{1}{\\sigma _1}&amp;        &amp;       &amp;       &amp;       &amp;       \\\\     &amp;       \\ddots&amp;     &amp;       &amp;       &amp;       \\\\     &amp;       &amp;       \\frac{1}{\\sigma _r}&amp;        &amp;       &amp;       \\\\     &amp;       &amp;       &amp;       0&amp;      &amp;       \\\\     &amp;       &amp;       &amp;       &amp;       \\ddots&amp;     \\\\     &amp;       &amp;       &amp;       &amp;       &amp;       0\\\\ \\end{matrix} \\right]  \\] <p>Then \\(\\boldsymbol{x}^*=\\boldsymbol{V}\\mathbf{\\Sigma }^{-1}\\boldsymbol{U}^*\\boldsymbol{b}\\) can be applied to the reduced SVD.</p> <p>We can rewrite another formula of the result:</p> \\[ \\boldsymbol{x}^*=\\sum_{i=1}^r{\\frac{{\\boldsymbol{u}_i}^T\\boldsymbol{b}}{\\sigma _i}\\boldsymbol{v}_i} \\] <p>where \\(\\boldsymbol{u}_i, \\boldsymbol{v}_i\\) are columns of \\(\\boldsymbol{U}, \\boldsymbol{V}\\) respectively.</p> <p><code>Comments</code>: If \\(\\sigma _i\\) is small, the error is enlarged by \\(\\sigma _i\\).</p> <p>We can compute the approximate value (\"the best approximation of \\(\\boldsymbol{x}^*\\) in \\(\\mathbb{R} ^k\\) \"):</p> \\[ \\boldsymbol{y}^*=\\sum_{i=1}^k{\\frac{{\\boldsymbol{u}_i}^T\\boldsymbol{b}}{\\sigma _i}\\boldsymbol{v}_i}, k&lt;r \\] <p>This is the idea behind Principal Component Analysis (where to cut off?).</p>"}]}